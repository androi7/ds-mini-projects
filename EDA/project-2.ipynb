{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 15px; height: 80px\">\n",
    "\n",
    "# Project 2\n",
    "\n",
    "### Exploratory Data Analysis (EDA)\n",
    "\n",
    "---\n",
    "\n",
    "Your hometown mayor just created a new data analysis team to give policy advice, and the administration recruited _you_ via LinkedIn to join it. Unfortunately, due to budget constraints, for now the \"team\" is just you...\n",
    "\n",
    "The mayor wants to start a new initiative to move the needle on one of two separate issues: high school education outcomes, or drug abuse in the community.\n",
    "\n",
    "Also unfortunately, that is the entirety of what you've been told. And the mayor just went on a lobbyist-funded fact-finding trip in the Bahamas. In the meantime, you got your hands on two national datasets: one on SAT scores by state, and one on drug use by age. Start exploring these to look for useful patterns and possible hypotheses!\n",
    "\n",
    "---\n",
    "\n",
    "This project is focused on exploratory data analysis, aka \"EDA\". EDA is an essential part of the data science analysis pipeline. Failure to perform EDA before modeling is almost guaranteed to lead to bad models and faulty conclusions. What you do in this project are good practices for all projects going forward, especially those after this course!\n",
    "\n",
    "This lab includes a variety of plotting problems. Much of the plotting code will be left up to you to find either in the lesson notes, or if not there, online. There are massive amounts of code snippets either in documentation or sites like [Stack Overflow](https://stackoverflow.com/search?q=%5Bpython%5D+seaborn) that have almost certainly done what you are trying to do.\n",
    "\n",
    "**Get used to googling for code!** You will use it every single day as a data scientist, especially for visualization and plotting.\n",
    "\n",
    "#### Package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import csv\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['font.size'] = 14\n",
    "\n",
    "# this line tells jupyter notebook to put the plots in the notebook rather than saving them to file\n",
    "%matplotlib inline\n",
    "\n",
    "# this line makes plots prettier on mac retina screens\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "## 1. Load the `sat_scores.csv` dataset and describe it\n",
    "\n",
    "---\n",
    "\n",
    "You should replace the placeholder path to the `sat_scores.csv` dataset below with your specific path to the file.\n",
    "\n",
    "### 1.1 Load the file with the `csv` module and put it in a Python dictionary\n",
    "\n",
    "The dictionary format for data will be the column names as key, and the data under each column as the values.\n",
    "\n",
    "Toy example:\n",
    "```python\n",
    "data = {\n",
    "    'column1':[0,1,2,3],\n",
    "    'column2':['a','b','c','d']\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(x):\n",
    "    if x.isnumeric():\n",
    "        return int(x)\n",
    "    else:\n",
    "        return str(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "data = {}\n",
    "with open('sat_scores.csv', newline='') as sat_file:\n",
    "    reader = csv.DictReader(sat_file)\n",
    "    for i, row in enumerate(reader):\n",
    "        for col in row.keys():\n",
    "            if i == 0:\n",
    "                data[col] = [convert(row[col])]\n",
    "            else:\n",
    "                data[col].append(convert(row[col]))\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print first 5 entries of each key\n",
    "pprint({k: data[k][:5] for k in data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Make a pandas DataFrame object with the SAT dictionary, and another with the pandas `.read_csv()` function\n",
    "\n",
    "Compare the DataFrames using the `.dtypes` attribute in the DataFrame objects. What is the difference between loading from file and inputting this dictionary (if any)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_scores_from_dict = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_scores = pd.read_csv('sat_scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sat_scores_from_dict.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sat_scores.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did not convert the string column values to float in your dictionary, the columns in the DataFrame are of type `object` (which are string values, essentially). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Look at the first ten rows of the DataFrame: what does our data describe?\n",
    "\n",
    "From now on, use the DataFrame loaded from the file using the `.read_csv()` function.\n",
    "\n",
    "Use the `.head(num)` built-in DataFrame function, where `num` is the number of rows to print out.\n",
    "\n",
    "You are not given a \"codebook\" with this data, so you will have to make some (very minor) inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_scores.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "## 2. Create a \"data dictionary\" based on the data\n",
    "\n",
    "---\n",
    "\n",
    "A data dictionary is an object that describes your data. This should contain the name of each variable (column), the type of the variable, your description of what the variable is, and the shape (rows and columns) of the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = list(sat_scores.columns)\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = [list(sat_scores[col]) for col in sat_scores.columns]\n",
    "data = np.array(sat_scores)\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = \"\"\"SAT Scores (standardized college admissions test):\n",
    "\n",
    ":Description: The score range for the SAT is 400-1600. It's the sum on the \n",
    "              Evidence-Based Reading and Writing (ERW) section and the Math section, \n",
    "              which each have a score range of 200-800.\n",
    ":Number of Instances: {instance}\n",
    ":Number of Attributes: {attr}\n",
    ":Attribute Information (in order):\n",
    "    - State     US states (abbreviations)\n",
    "    - Rate      Rate in previous school (1 is the best)\n",
    "    - Verbal    Verbal score\n",
    "    - Math      Math score\n",
    "\"\"\".format(instance=len(data[0]), attr=len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# filename = os.path.dirname(os.getcwd()) + '/EDA/sat_scores.csv'\n",
    "filename = 'https://github.com/androi7/ds-mini-projects/tree/main/EDA/sat_scores.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_scores = {'data': data, 'description': description, 'feature_names': feature_names, 'filename': filename}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print only the first five rows of the data key\n",
    "pprint({k: sat_scores[k][:5] if k == 'data' else sat_scores[k] for k in sat_scores})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "## 3. Plot the data using seaborn\n",
    "\n",
    "---\n",
    "\n",
    "### 3.1 Using seaborn's `distplot`, plot the distributions for each of `Rate`, `Math`, and `Verbal`\n",
    "\n",
    "Set the keyword argument `kde=False`. This way you can actually see the counts within bins. You can adjust the number of bins to your liking. \n",
    "\n",
    "[Please read over the `distplot` documentation to learn about the arguments and fine-tune your chart if you want.](https://stanford.edu/~mwaskom/software/seaborn/generated/seaborn.distplot.html#seaborn.distplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_scores = pd.DataFrame(data=sat_scores['data'], columns=sat_scores['feature_names'])\n",
    "sat_scores.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_scores.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert(x, dtype):\n",
    "#     try:\n",
    "#         return dtype(x)\n",
    "#     except:\n",
    "#         return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_scores = sat_scores.astype({'Rate': 'int32', 'Verbal': 'int32', 'Math': 'int32'})\n",
    "sat_scores.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(font_scale=1.5)\n",
    "plt.style.use('seaborn')\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(12, 4))\n",
    "\n",
    "# distplot is deprecated soon\n",
    "g = sns.histplot(sat_scores.Rate, kde=False, bins=10, ax=ax[0])\n",
    "ax[0].set_title('Rate distribution\\n', fontsize=18)\n",
    "ax[0].set_ylabel('Counts')\n",
    "sns.histplot(sat_scores.Verbal, kde=False, bins=10, ax=ax[1])\n",
    "ax[1].set_title('Verbal distribution\\n', fontsize=18)\n",
    "ax[1].set_ylabel('Counts')\n",
    "sns.histplot(sat_scores.Math, kde=False, bins=10, ax=ax[2])\n",
    "ax[2].set_title('Math distribution\\n', fontsize=18)\n",
    "ax[2].set_ylabel('Counts')\n",
    "fig.tight_layout(pad=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Using seaborn's `pairplot`, show the joint distributions for each of `Rate`, `Math`, and `Verbal`\n",
    "\n",
    "Explain what the visualization tells you about your data.\n",
    "\n",
    "[Please read over the `pairplot` documentation to fine-tune your chart.](https://stanford.edu/~mwaskom/software/seaborn/generated/seaborn.pairplot.html#seaborn.pairplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.pairplot(sat_scores, corner=True, diag_kws={'bins': 10, 'color': 'dodgerblue'})\n",
    "g.fig.suptitle('Pairplot of the SAT results', y=1.08)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pairplot depicts a histogram plot on the diagonal (univariate distribution) where the variables on the x- and y-axis are the same and it shows a scatter plot where the variables are different.\n",
    "\n",
    "Furthermore, the *Verbal* assignment is strongly negative correlated to the SAT *Rate*, because the *Verbal* is also strongly positive correlated to the *Math* score. Which means that students who gain good results in the reading and writing section also have a well understanding in math. The same relation applies to the under end of the score range and everything in between."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "## 4. Plot the data using built-in pandas functions.\n",
    "\n",
    "---\n",
    "\n",
    "Pandas is very powerful and contains a variety of nice, built-in plotting functions for your data. Read the documentation here to understand the capabilities:\n",
    "\n",
    "http://pandas.pydata.org/pandas-docs/stable/visualization.html\n",
    "\n",
    "### 4.1 Plot a stacked histogram with `Verbal` and `Math` using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "colors = ['dodgerblue', 'orange']\n",
    "\n",
    "sat_scores[['Math', 'Verbal']].plot.hist(stacked=True, bins=10, colormap=ListedColormap(colors))\n",
    "plt.title('Histogram of Verbal and Math\\n', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Plot `Verbal` and `Math` on the same chart using boxplots\n",
    "\n",
    "What are the benefits of using a boxplot as compared to a scatterplot or a histogram?\n",
    "\n",
    "What's wrong with plotting a box-plot of `Rate` on the same chart as `Math` and `Verbal`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sat_scores.boxplot(column=['Math', 'Verbal'], figsize=(6,4), color={'boxes':'Black', 'medians': 'Red'}, \n",
    "                        medianprops={'linestyle': '--', 'linewidth': 2}, return_type='dict', \n",
    "                        patch_artist=True, fontsize=14)\n",
    "\n",
    "for i, box in enumerate(ax['boxes']):\n",
    "    box.set_facecolor(colors[i])\n",
    "plt.title('Boxplot of Math and Verbal\\n', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boxplots are primarly used to compare several distributions against each other. They show easily the five number summary (Min, Q1, Median, Q3, Max). Compared to histograms you can extract the exact positions of the outliers and you see the median and the interquartile range by default. And it's easier to determine the skewness. \n",
    "When using the scatterplot instead, you would neither detect the median, IQR, nor the outliers.\n",
    "\n",
    "The Rate column has different dimensions and it would squeeze the plot if the data aren't normalized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/xDpSobf.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "### 4.3 Plot `Verbal`, `Math`, and `Rate` appropriately on the same boxplot chart\n",
    "\n",
    "Think about how you might change the variables so that they would make sense on the same chart. Explain your rationale for the choices on the chart. You should strive to make the chart as intuitive as possible. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standarized(x):\n",
    "    \"Returns the z-score\"\n",
    "    return (x-x.mean())/x.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_scores_normalized = sat_scores[['Rate', 'Verbal', 'Math']].apply(standarized)\n",
    "ax = sat_scores_normalized.boxplot(column=['Math', 'Verbal', 'Rate'], \n",
    "                                   figsize=(6,4), \n",
    "                                   color={'boxes':'Black', 'medians': 'Red'}, \n",
    "                                   medianprops={'linestyle': '--', 'linewidth': 2},\n",
    "                                   return_type='dict',\n",
    "                                   patch_artist=True,\n",
    "                                   fontsize=14)\n",
    "\n",
    "colors.append('turquoise')\n",
    "for i, box in enumerate(ax['boxes']):\n",
    "    box.set_facecolor(colors[i])\n",
    "    \n",
    "plt.title('Boxplot of standardized Math, Verbal and Rate\\n', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each column of the dataset was normalised by calculating the z-scores. While each column has now a distribution with the mean of null and a standard deviation of one, they can be used for comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "## 5. Create and examine subsets of the data\n",
    "\n",
    "---\n",
    "\n",
    "For these questions you will practice **masking** in pandas. Masking uses conditional statements to select portions of your DataFrame (through boolean operations under the hood.)\n",
    "\n",
    "Remember the distinction between DataFrame indexing functions in pandas:\n",
    "\n",
    "    .iloc[row, col] : row and column are specified by index, which are integers\n",
    "    .loc[row, col]  : row and column are specified by string \"labels\" (boolean arrays are allowed; useful for rows)\n",
    "    .ix[row, col]   : row and column indexers can be a mix of labels and integer indices\n",
    "    \n",
    "For detailed reference and tutorial make sure to read over the pandas documentation:\n",
    "\n",
    "http://pandas.pydata.org/pandas-docs/stable/indexing.html\n",
    "\n",
    "\n",
    "\n",
    "### 5.1 Find the list of states that have `Verbal` scores greater than the average of `Verbal` scores across states\n",
    "\n",
    "How many states are above the mean? What does this tell you about the distribution of `Verbal` scores?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbal_mean = sat_scores['Verbal'].mean()\n",
    "print('Verbal mean:', round(verbal_mean))\n",
    "mask = (sat_scores['Verbal'] >  verbal_mean)\n",
    "# alphabetically ordered\n",
    "sat_scores_states_above_verbal_mean = sat_scores[mask][['State', 'Verbal']].sort_values(by=['State'])\n",
    "sat_scores_states_above_verbal_mean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"%s states have a higher Verbal score than the mean.\" % sat_scores_states_above_verbal_mean.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Counts of states:', sat_scores.shape[0])\n",
    "verbal_median = sat_scores['Verbal'].median()\n",
    "print('Verbal median:', verbal_median)\n",
    "print('Verbal distribution skewness:', round(sat_scores[['Verbal']].skew()[0], 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_scores['Verbal'].hist(bins=15)\n",
    "plt.title('Histogram of Verbal\\n', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On half of the states is above and other half is under the mean of the Verbal score. The distribution is positive skewed and multimodal (probably bimodal based on the given plot with 15 bins). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Find the list of states that have `Verbal` scores greater than the median of `Verbal` scores across states\n",
    "\n",
    "How does this compare to the list of states greater than the mean of `Verbal` scores? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (sat_scores['Verbal'] > verbal_median)\n",
    "sat_scores_states_above_verbal_median = sat_scores[mask][['State', 'Verbal']].sort_values(by=['State'])\n",
    "sat_scores_states_above_verbal_median.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"%s states are above the median Verbal score.\" % sat_scores_states_above_verbal_median.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For positive skewed distributions the mean value is greater than the median value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Create a column that is the difference between the `Verbal` and `Math` scores\n",
    "\n",
    "Specifically, this should be `Verbal - Math`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_scores['Verbal_Math_diff'] = sat_scores['Verbal'] - sat_scores['Math']\n",
    "sat_scores.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Create two new DataFrames showing states with the greatest difference between scores\n",
    "\n",
    "1. Your first DataFrame should be the 10 states with the greatest gap between `Verbal` and `Math` scores where `Verbal` is greater than `Math`. It should be sorted appropriately to show the ranking of states.\n",
    "2. Your second DataFrame will be the inverse: states with the greatest gap between `Verbal` and `Math` such that `Math` is greater than `Verbal`. Again, this should be sorted appropriately to show rank.\n",
    "3. Print the header of both variables, only showing the top 3 states in each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (sat_scores['Verbal'] > sat_scores['Math'])  # to be sure that verbal score is greater \n",
    "sat_scores_verbal_greater_math = sat_scores[mask].drop(labels=['Rate'], axis=1) \\\n",
    "                                     .sort_values(by=['Verbal_Math_diff'], ascending=False)[:10]\n",
    "sat_scores_verbal_greater_math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (sat_scores['Verbal'] < sat_scores['Math'])  # to be sure that math score is greater \n",
    "sat_scores_math_greater_verbal = sat_scores[mask].drop(labels=['Rate'], axis=1) \\\n",
    "                                     .sort_values(by=['Verbal_Math_diff'], ascending=True)[0:10]\n",
    "sat_scores_math_greater_verbal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_scores_verbal_greater_math.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_scores_math_greater_verbal.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Examine summary statistics\n",
    "\n",
    "---\n",
    "\n",
    "Checking the summary statistics for data is an essential step in the EDA process!\n",
    "\n",
    "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "### 6.1 Create the correlation matrix of your variables (excluding `State`).\n",
    "\n",
    "What does the correlation matrix tell you?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_scores.drop('Verbal_minus_Math', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_scores[['Rate', 'Verbal', 'Math']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rate and Verbal are strong negatively correlated which means that one variable declines while the other increases. Rate and Math are a little be lower negatively correlated than the previous variables. Math and Verbal are strong positively related which means that both either increase or decrease together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "### 6.2 Use pandas'  `.describe()` built-in function on your DataFrame\n",
    "\n",
    "Write up what each of the rows returned by the function indicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_scores.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For columns with numerical values:<br>\n",
    "<b>count</b>: amount of variables<br>\n",
    "<b>mean</b>: the average value<br>\n",
    "<b>std</b>: the standard deviation<br>\n",
    "<b>min</b>: the minimum value<br>\n",
    "<b>25%</b>: 25th percentile (25% of the values are equal or lower than this value)<br>\n",
    "<b>50%</b>: median value<br>\n",
    "<b>75%</b>: 75th percentile<br>\n",
    "<b>max</b>: the maximum value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_scores.describe(include='O')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For columns with non-numerical values:<br>\n",
    "<b>count</b>: amount of variables<br>\n",
    "<b>unique</b>: amount of unique values<br>\n",
    "<b>top</b>: value which occures the most often<br>\n",
    "<b>freq</b>: how many times the <b>top</b> value occures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/xDpSobf.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "### 6.3 Assign and print the _covariance_ matrix for the dataset\n",
    "\n",
    "1. Describe how the covariance matrix is different from the correlation matrix.\n",
    "2. What is the process to convert the covariance into the correlation?\n",
    "3. Why is the correlation matrix preferred to the covariance matrix for examining relationships in your data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_scores[['Rate', 'Verbal', 'Math']].cov()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\text{covariance}(X, Y) = \\sum_{i=1}^N \\frac{(X_i - \\bar{X})(Y_i - \\bar{Y})}{N-1}$$\n",
    "\n",
    "$$ \\text{Pearson correlation}\\;r = {\\rm corr}(X, Y) =\\frac{{\\rm cov}(X, Y)}{{\\rm std}(X){\\rm std}(Y)}$$\n",
    "\n",
    "1. Both measure the relationship between two variables, whereas the covariance only indicates the direction the correlation additionaly shows the strength of the direction. But the correlation matrix is normalized by the standard deviations of the two variables.\n",
    "\n",
    "2. To convert the covariance matrix into a correlation matrix it has to be normalized by the two standard deviations of both variables. And the end the values are in the interval of [-1, 1]. \n",
    "3. With the correlation matrix we can also determine the strength of the relationship of two variables next to the direction of it. It is also a unit-free masure whereas the covariance matrix has the unit of the product of the two variables. The range of [-1, 1] lets us easily detect the strength of the relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "## 7. Performing EDA on \"drug use by age\" data.\n",
    "\n",
    "---\n",
    "\n",
    "You will now switch datasets to one with many more variables. This section of the project is more open-ended - use the techniques you practiced above!\n",
    "\n",
    "We'll work with the \"drug-use-by-age.csv\" data, sourced from and described here: https://github.com/fivethirtyeight/data/tree/master/drug-use-by-age.\n",
    "\n",
    "### 7.1 Load the data using pandas. \n",
    "\n",
    "Does this data require cleaning? Are variables missing? How will this affect your approach to EDA on the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_use_by_age_csv = \"./drug-use-by-age.csv\"  # os.path.dirname(os.getcwd()) + \"/EDA/drug-use-by-age.csv\"\n",
    "drug = pd.read_csv(drug_use_by_age_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "drug.columns = [('').join([letter.lower() \n",
    "                           if letter in string.ascii_letters \n",
    "                           else '_' if i!=0 and not i == len(col)-1 \n",
    "                           else '' for letter in col]) \n",
    "                for i, col in enumerate(drug.columns)]\n",
    "drug.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug.iloc[:, :15].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug.iloc[:,15:].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stimulant_frequency_75perc = np.quantile(drug['stimulant_frequency'], 0.75)\n",
    "print(stimulant_frequency_75perc)\n",
    "drug[drug['stimulant_frequency'] > stimulant_frequency_75perc][['stimulant_frequency']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drug.loc[16, 'stimulant_frequency'] = 34.0\n",
    "drug.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug.drop(drug.index[-1], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug.describe(include='O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in drug.describe(include='O').columns:\n",
    "    print(\"\"\"\n",
    "    Column name: {col}\n",
    "    Unique values: {val}\n",
    "    \"\"\".format(col=col, val=drug[col].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_float(x):\n",
    "    \"Return value converted to numeric float value or nan\"\n",
    "    try:\n",
    "        return float(x)\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, col in enumerate(drug.describe(include='O').columns):\n",
    "    if i > 0:\n",
    "        drug[col] = drug[col].map(convert_to_float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Do a high-level, initial overview of the data\n",
    "\n",
    "Get a feel for what this dataset is all about.\n",
    "\n",
    "Use whichever techniques you'd like, including those from the SAT dataset EDA. The final response to this question should be a written description of what you infer about the dataset.\n",
    "\n",
    "Some things to consider doing:\n",
    "\n",
    "- Look for relationships between variables and subsets of those variables' values\n",
    "- Derive new features from the ones available to help your analysis\n",
    "- Visualize everything!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,30))\n",
    "sns.heatmap(drug.corr(), annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# age as categorical variable and to be used in the correlation matrix to indicate relations\n",
    "drug['age_labelled'] = drug['age'].apply(lambda x: drug[drug['age'] == x].index[0] + 1)\n",
    "drug.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# highest correlations\n",
    "plt.figure(figsize=(30,30))\n",
    "sns.heatmap(drug.corr()[(drug.corr() > 0.8) | (drug.corr() < -0.8)] , annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(zip(drug['age'], drug['age_labelled'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = drug.plot('age_labelled', ['crackuse', 'inhalant_use'], style='.-')\n",
    "ax.set_title('Drug use in percentage per age groups\\n');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = drug.plot('age_labelled', 'alcohol_use', style='.-')\n",
    "ax.set_title('Alcohol use in percentage per age groups\\n');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = drug.plot('age_labelled', ['marijuana_frequency', 'alcohol_frequency'], style='.-')\n",
    "ax.set_title('Frequency per age groups\\n');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference:<br>\n",
    "The drugs with the highest correlation to the age of the candidates are marijuna, alcohol, crack and inhalants. Whereas the consumption of each drug of that list increases with the age only the inhalants have their peak in very early age and decrease while getting older. The other drugs have their peak more or less around the age of 21."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Create a testable hypothesis about this data\n",
    "\n",
    "Requirements for the question:\n",
    "\n",
    "1. Write a specific question you would like to answer with the data (that can be accomplished with EDA).\n",
    "2. Write a description of the \"deliverables\": what will you report after testing/examining your hypothesis?\n",
    "3. Use EDA techniques of your choice, numeric and/or visual, to look into your question.\n",
    "4. Write up your report on what you have found regarding the hypothesis about the data you came up with.\n",
    "\n",
    "\n",
    "Your hypothesis could be on:\n",
    "\n",
    "- Difference of group means\n",
    "- Correlations between variables\n",
    "- Anything else you think is interesting, testable, and meaningful!\n",
    "\n",
    "**Important notes:**\n",
    "\n",
    "You should be only doing EDA _relevant to your question_ here. It is easy to go down rabbit holes trying to look at every facet of your data, and so we want you to get in the practice of specifying a hypothesis you are interested in first and scoping your work to specifically answer that question.\n",
    "\n",
    "Some of you may want to jump ahead to \"modeling\" data to answer your question. This is a topic addressed in the next project and **you should not do this for this project.** We specifically want you to not do modeling to emphasize the importance of performing EDA _before_ you jump to statistical analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "** Question and deliverables**\n",
    "\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Null Hypothesis: Drug consumption is not related to the age of the candidates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Calculating the correlation matrix and detect the direction and strength of the relationships of the individual drugs and the age parameter. After gaining the result try to state inferences why the relationship is given or not. If the null hypothesis is wrong try to divide the ages in useful groups and find the causality for the impact. What are the differences between these groups which could lead to the given result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) See result above (also inference). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Report**\n",
    "\n",
    "\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/xDpSobf.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "## 8. Introduction to dealing with outliers\n",
    "\n",
    "---\n",
    "\n",
    "Outliers are an interesting problem in statistics, in that there is not an agreed upon best way to define them. Subjectivity in selecting and analyzing data is a problem that will recur throughout the course.\n",
    "\n",
    "1. Pull out the rate variable from the sat dataset.\n",
    "2. Are there outliers in the dataset? Define, in words, how you _numerically define outliers._\n",
    "3. Print out the outliers in the dataset.\n",
    "4. Remove the outliers from the dataset.\n",
    "5. Compare the mean, median, and standard deviation of the \"cleaned\" data without outliers to the original. What is different about them and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_outliers(df, col):\n",
    "    \"\"\"Return and print outliers\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): dataframe\n",
    "    col (str): name of the column\n",
    "    \n",
    "    Returns:\n",
    "    lower_outlier (pandas.Series): list of all lower outliers\n",
    "    upper_outlier (pandas.Series): list of all upper outliers\n",
    "    \"\"\"\n",
    "    mean_val = df[col].mean()\n",
    "    std_val = df[col].std()\n",
    "    \n",
    "    mask_lower = (df[col] < mean_val - 3 * std_val)\n",
    "    mask_upper = (df[col] > mean_val + 3 * std_val)\n",
    "    \n",
    "    lower_outlier = df[col][mask_lower]\n",
    "    upper_outlier = df[col][mask_upper]\n",
    "    print(\"In {col} are {amount} lower outliers: {lower_outliers}\".format(col=col, amount=len(lower_outlier), lower_outliers=[v for v in lower_outlier]))\n",
    "    print(\"In {col} are {amount} upper outliers: {upper_outliers}\".format(col=col, amount=len(upper_outlier), upper_outliers=[v for v in upper_outlier]))\n",
    "    \n",
    "    return lower_outlier, upper_outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_outliers(sat_scores, 'Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2)<br>\n",
    "1. Method\n",
    "$$outlier_{lower}=Q_{1}-1.5*IQR$$\n",
    "$$outlier_{upper}=Q_{3}-1.5*IQR$$\n",
    "2. Method\n",
    "$$outlier_{lower}=mean-3*std$$\n",
    "$$outlier_{upper}=mean+3*std$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_scores = pd.read_csv('sat_scores.csv')\n",
    "sat_scores_normalized = sat_scores[['Rate', 'Verbal', 'Math']].apply(standarized)\n",
    "sat_scores_normalized.boxplot(column=['Math', 'Verbal', 'Rate'], figsize=(6,4), color={'boxes':'Blue', 'medians': 'Red'}, medianprops={'linestyle': '--', 'linewidth': 2});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No outliers in the sat_score dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_scores['Math'].sort_values()[27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_val = sat_scores['Math'].mean()\n",
    "std_val = sat_scores['Math'].std()\n",
    "median_val = sat_scores['Math'].median()\n",
    "    \n",
    "z_value = (sat_scores.loc[27, 'Math'] - mean_val)/std_val\n",
    "z_value\n",
    "# This value is not by definition an outlier, but the z-score is close to the limit of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_scores.drop(sat_scores.index[27], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_val_cleaned = sat_scores['Math'].mean()\n",
    "std_val_cleaned = sat_scores['Math'].std()\n",
    "median_val_cleaned = sat_scores['Math'].median()\n",
    "\n",
    "print(f'Median - before: {median_val} | cleaned: {median_val_cleaned}')\n",
    "print(f'Standard deviation - before: {round(std_val, 2)} | cleaned: {round(std_val_cleaned, 2)}')\n",
    "print(f'Mean - before: {mean_val} | cleaned: {round(mean_val_cleaned, 2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) The median value takes the next greater value (if length of the sequence is odd, else the average), because one outlier was dropped.\n",
    "2) Standard deviation is smaller when the dataframe is cleaned, because outliers stretch the range.\n",
    "3) The outlier was a lower outlier and therefore the mean value increases after cleaning the dataframe, because a smaller value was dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/GCAf1UX.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "### 9. Percentile scoring and Spearman rank correlation\n",
    "\n",
    "---\n",
    "\n",
    "### 9.1 Calculate the Spearman correlation of sat `Verbal` and `Math`\n",
    "\n",
    "1. How does the Spearman correlation compare to the Pearson correlation? \n",
    "2. Describe clearly in words the process of calculating the Spearman rank correlation.\n",
    "  - Hint: the word \"rank\" is in the name of the process for a reason!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\text{Spearman correlation}\\;r = 1-\\frac{6\\sum_{i}d^{2}_{i}}{n(n^2-1)}$$\n",
    "\n",
    "d... difference of the ranks of two variables for each row<br>\n",
    "n... amount of numbers per column\n",
    "\n",
    "rank: labelling the values with numbers (ranks) from 1 for the highest value in ordered sequence up to the highest rank for the smallest value\n",
    "\n",
    "The Spearman correlation can therefore also be used to evaluate relationships involving ordinal (categorical) variables.\n",
    "\n",
    "The strengs of the relationships are less with the Spearman correlation formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_scores.corr(method='spearman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\text{Pearson correlation}\\;r = {\\rm corr}(X, Y) =\\frac{{\\rm cov}(X, Y)}{{\\rm std}(X){\\rm std}(Y)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_scores.corr(method='pearson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Percentile scoring\n",
    "\n",
    "Look up percentile scoring of data. In other words, the conversion of numeric data to their equivalent percentile scores.\n",
    "\n",
    "http://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.percentile.html\n",
    "\n",
    "http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.percentileofscore.html\n",
    "\n",
    "1. Convert `Rate` to percentiles in the sat scores as a new column.\n",
    "2. Show the percentile of California in `Rate`.\n",
    "3. How is percentile related to the Spearman rank correlation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "sat_scores['Rate_Percentile'] = sat_scores['Rate'].apply(lambda x: stats.percentileofscore(sat_scores['Rate'], x))\n",
    "sat_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_scores[sat_scores['State'] == 'CA'][['State', 'Rate_Percentile']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percentile is related to the rank in the manner of that the values are sorted and allocated to a new value based on their position of the ordered sequence. Rank as label from 1 for the highest value up to a number for the lowest value. The percentile score is set to 100 (percent) for the highest value up to a proportion of percentage for the lowest value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 Percentiles and outliers\n",
    "\n",
    "1. Why might percentile scoring be useful for dealing with outliers?\n",
    "2. Plot the distribution of a variable of your choice from the drug use dataset.\n",
    "3. Plot the same variable but percentile scored.\n",
    "4. Describe the effect, visually, of converting raw scores to percentile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the 25th and the 75th percentile the interquartile range can be calculated which is used to detect outliers (see formulas above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ages = [age for age in drug['age']]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "drug['alcohol_use'].hist(ax=ax);\n",
    "ax.set_title('Alcohol use distribution\\n')\n",
    "ax.set_xlabel('Alcohol use percentage per age group')\n",
    "ax.set_ylabel('Counts');\n",
    "\n",
    "#print(ages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug['alcohol_use_percentile'] = drug['alcohol_use'].apply(lambda x: stats.percentileofscore(drug['alcohol_use'], x))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "drug['alcohol_use_percentile'].hist(ax=ax);\n",
    "ax.set_title('Alcohol use distribution in percentiles\\n')\n",
    "ax.set_xlabel('Alcohol use percentiles per age group')\n",
    "ax.set_ylabel('Counts');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug[['alcohol_use', 'alcohol_use_percentile']].sort_values(by=['alcohol_use'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percentile score:<br>\n",
    "lowest column value allocated with $$i=100/len(column)$$ which is the step variable for each next higher value in the column in an ordered sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug[['alcohol_use', 'age']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
