{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "69b9a648-bcc7-490d-9f9b-ea244d156bd6"
   },
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 15px; height: 80px\">\n",
    "\n",
    "\n",
    "# Web Scraping for Indeed.com and Predicting Salaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Case Overview\n",
    "\n",
    "You're working as a data scientist for a contracting firm that's rapidly expanding. Now that they have their most valuable employee (you!), they need to leverage data to win more contracts. Your firm offers technology and scientific solutions and wants to be competitive in the hiring market. Your principal wants you to\n",
    "\n",
    "   - determine the industry factors that are most important in predicting the salary amounts for these data.\n",
    "\n",
    "To limit the scope, your principal has suggested that you *focus on data-related job postings*, e.g. data scientist, data analyst, research scientist, business intelligence, and any others you might think of. You may also want to decrease the scope by *limiting your search to a single region.*\n",
    "\n",
    "Hint: Aggregators like [Indeed.com](https://www.indeed.com) regularly pool job postings from a variety of markets and industries.\n",
    "\n",
    "**Goal:** Scrape your own data from a job aggregation tool like Indeed.com in order to collect the data to best answer this question.\n",
    "\n",
    "---\n",
    "\n",
    "### Directions\n",
    "\n",
    "In this project you will be leveraging a variety of skills. The first will be to use the web-scraping and/or API techniques you've learned to collect data on data jobs from Indeed.com or another aggregator. Once you have collected and cleaned the data, you will use it to address the question above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factors that impact salary\n",
    "\n",
    "To predict salary the most appropriate approach would be a regression model.\n",
    "Here instead we just want to estimate which factors (like location, job title, job level, industry sector) lead to high or low salary and work with a classification model. To do so, split the salary into two groups of high and low salary, for example by choosing the median salary as a threshold (in principle you could choose any single or multiple splitting points).\n",
    "\n",
    "Use all the skills you have learned so far to build a predictive model.\n",
    "Whatever you decide to use, the most important thing is to justify your choices and interpret your results. *Communication of your process is key.* Note that most listings **DO NOT** come with salary information. You'll need to be able to extrapolate or predict the expected salaries for these listings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "a948d79c-5527-4c0d-ab23-f5d43ce72056"
   },
   "source": [
    "### Scraping job listings from Indeed.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": true,
    "id": "7203e0c9-e437-4802-a6ad-7dc464f94436"
   },
   "source": [
    "We will be scraping job listings from Indeed.com using BeautifulSoup. Luckily, Indeed.com is a simple text page where we can easily find relevant entries.\n",
    "\n",
    "First, look at the source of an Indeed.com page: (http://www.indeed.com/jobs?q=data+scientist+%2420%2C000&l=New+York&start=10\").\n",
    "\n",
    "Notice, each job listing is underneath a `div` tag with a class name of `result`. We can use BeautifulSoup to extract those. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "9732c901-ae26-4160-8376-42e22dd327df"
   },
   "source": [
    "#### Setup a request (using `requests`) to the URL below. Use BeautifulSoup to parse the page and extract all results (HINT: Look for div tags with class name result)\n",
    "\n",
    "The URL here has many query parameters:\n",
    "\n",
    "- `q` for the job search\n",
    "- This is followed by \"+20,000\" to return results with salaries (or expected salaries >$20,000)\n",
    "- `l` for a location \n",
    "- `start` for what result number to start on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "focus": false,
    "id": "e915023e-6b0d-4982-af2a-b1e0355f4927"
   },
   "outputs": [],
   "source": [
    "URL = \"http://www.indeed.com/jobs?q=data+scientist+%2420%2C000&l=New+York&start=10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "focus": false,
    "id": "2efefc73-064a-482d-b3b5-ddf5508cb4ec"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(URL)\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "results = soup.find_all('div', class_='result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"jobsearch-SerpJobCard unifiedRow row result\" data-jk=\"f1d32093c4f114d2\" data-tn-component=\"organicJob\" id=\"p_f1d32093c4f114d2\">\n",
      " <h2 class=\"title\">\n",
      "  <a class=\"jobtitle turnstileLink\" data-tn-element=\"jobTitle\" href=\"/rc/clk?jk=f1d32093c4f114d2&amp;fccid=fe404d18bb9eef1e&amp;vjs=3\" id=\"jl_f1d32093c4f114d2\" onclick=\"setRefineByCookie(['salest']); return rclk(this,jobmap[0],true,0);\" onmousedown=\"return rclk(this,jobmap[0],0);\" rel=\"noopener nofollow\" target=\"_blank\" title=\"Data Scientist - Publishing Royalties\">\n",
      "   <b>\n",
      "    Data\n",
      "   </b>\n",
      "   <b>\n",
      "    Scientist\n",
      "   </b>\n",
      "   - Publishing Royalties\n",
      "  </a>\n",
      "  <span class=\"new\">\n",
      "   new\n",
      "  </span>\n",
      " </h2>\n",
      " <div class=\"sjcl\">\n",
      "  <div>\n",
      "   <span class=\"company\">\n",
      "    <a class=\"turnstileLink\" data-tn-element=\"companyName\" href=\"/cmp/Spotify\" onmousedown=\"this.href = appendParamsOnce(this.href, 'from=SERP&amp;campaignid=serp-linkcompanyname&amp;fromjk=f1d32093c4f114d2&amp;jcid=fe404d18bb9eef1e')\" rel=\"noopener\" target=\"_blank\">\n",
      "     Spotify\n",
      "    </a>\n",
      "   </span>\n",
      "   <span class=\"ratingsDisplay\">\n",
      "    <a aria-label=\"Company rating 4.3 out of 5 stars\" class=\"ratingNumber\" data-tn-variant=\"cmplinktst2\" href=\"/cmp/Spotify/reviews\" onmousedown=\"this.href = appendParamsOnce(this.href, '?campaignid=cmplinktst2&amp;from=SERP&amp;jt=Data+Scientist+-+Publishing+Royalties&amp;fromjk=f1d32093c4f114d2&amp;jcid=fe404d18bb9eef1e');\" rel=\"noopener\" target=\"_blank\" title=\"Spotify reviews\">\n",
      "     <span class=\"ratingsContent\">\n",
      "      4.3\n",
      "      <svg class=\"starIcon\" height=\"12px\" role=\"img\" width=\"12px\">\n",
      "       <g>\n",
      "        <path d=\"M 12.00,4.34 C 12.00,4.34 7.69,3.97 7.69,3.97 7.69,3.97 6.00,0.00 6.00,0.00 6.00,0.00 4.31,3.98 4.31,3.98 4.31,3.98 0.00,4.34 0.00,4.34 0.00,4.34 3.28,7.18 3.28,7.18 3.28,7.18 2.29,11.40 2.29,11.40 2.29,11.40 6.00,9.16 6.00,9.16 6.00,9.16 9.71,11.40 9.71,11.40 9.71,11.40 8.73,7.18 8.73,7.18 8.73,7.18 12.00,4.34 12.00,4.34 Z\" style=\"fill: #FFB103\">\n",
      "        </path>\n",
      "       </g>\n",
      "      </svg>\n",
      "     </span>\n",
      "    </a>\n",
      "   </span>\n",
      "  </div>\n",
      "  <div class=\"recJobLoc\" data-rc-loc=\"New York, NY\" id=\"recJobLoc_f1d32093c4f114d2\" style=\"display: none\">\n",
      "  </div>\n",
      "  <span class=\"location accessible-contrast-color-location\">\n",
      "   New York, NY\n",
      "  </span>\n",
      " </div>\n",
      " <div class=\"summary\">\n",
      "  <ul style=\"list-style-type:circle;margin-top: 0px;margin-bottom: 0px;padding-left:20px;\">\n",
      "   <li>\n",
      "    You have experience working with financial\n",
      "    <b>\n",
      "     data\n",
      "    </b>\n",
      "    and/or performing\n",
      "    <b>\n",
      "     data\n",
      "    </b>\n",
      "    reconciliations and quality assurance testing.\n",
      "   </li>\n",
      "  </ul>\n",
      " </div>\n",
      " <div class=\"jobsearch-SerpJobCard-footer\">\n",
      "  <div class=\"jobsearch-SerpJobCard-footerActions\">\n",
      "   <div class=\"result-link-bar-container\">\n",
      "    <div class=\"result-link-bar\">\n",
      "     <span class=\"date\">\n",
      "      5 days ago\n",
      "     </span>\n",
      "     <div class=\"tt_set\" id=\"tt_set_0\">\n",
      "      <div class=\"job-reaction\">\n",
      "       <button aria-expanded=\"false\" aria-haspopup=\"true\" aria-label=\"save or dislike\" class=\"job-reaction-kebab\" data-ol-has-click-handler=\"\" onclick=\"toggleKebabMenu('f1d32093c4f114d2', false, event); return false;\" tabindex=\"0\">\n",
      "       </button>\n",
      "       <span class=\"job-reaction-kebab-menu\">\n",
      "        <button class=\"job-reaction-kebab-item job-reaction-save\" data-ol-has-click-handler=\"\" onclick=\"changeJobState('f1d32093c4f114d2', 'save', 'linkbar', false, '');return false;\">\n",
      "         <svg focusable=\"false\" height=\"16\" viewbox=\"0 0 24 24\" width=\"16\">\n",
      "          <g>\n",
      "           <path d=\"M16.5,3A6,6,0,0,0,12,5.09,6,6,0,0,0,7.5,3,5.45,5.45,0,0,0,2,8.5C2,12.28,5.4,15.36,10.55,20L12,21.35,13.45,20C18.6,15.36,22,12.28,22,8.5A5.45,5.45,0,0,0,16.5,3ZM12.1,18.55l-0.1.1-0.1-.1C7.14,14.24,4,11.39,4,8.5A3.42,3.42,0,0,1,7.5,5a3.91,3.91,0,0,1,3.57,2.36h1.87A3.88,3.88,0,0,1,16.5,5,3.42,3.42,0,0,1,20,8.5C20,11.39,16.86,14.24,12.1,18.55Z\" fill=\"#2d2d2d\">\n",
      "           </path>\n",
      "          </g>\n",
      "         </svg>\n",
      "         <span class=\"job-reaction-kebab-item-text\">\n",
      "          Save job\n",
      "         </span>\n",
      "        </button>\n",
      "        <button class=\"job-reaction-kebab-item job-reaction-dislike\" data-ol-has-click-handler=\"\" onclick=\"dislikeJob(false, false, 'f1d32093c4f114d2', 'unsave', 'linkbar', false, '');\">\n",
      "         <span class=\"job-reaction-dislike-icon\">\n",
      "         </span>\n",
      "         <span class=\"job-reaction-kebab-item-text\">\n",
      "          Not interested\n",
      "         </span>\n",
      "        </button>\n",
      "        <button class=\"job-reaction-kebab-item job-reaction-report\" onclick=\"reportJob('f1d32093c4f114d2');\">\n",
      "         <span class=\"job-reaction-report-icon\">\n",
      "         </span>\n",
      "         <span class=\"job-reaction-kebab-item-text\">\n",
      "          Report Job\n",
      "         </span>\n",
      "        </button>\n",
      "       </span>\n",
      "      </div>\n",
      "      <span class=\"result-link-bar-separator\">\n",
      "       ·\n",
      "      </span>\n",
      "      <a class=\"sl resultLink save-job-link\" href=\"#\" id=\"sj_f1d32093c4f114d2\" onclick=\"changeJobState('f1d32093c4f114d2', 'save', 'linkbar', false, ''); return false;\" title=\"Save this job to my.indeed\">\n",
      "       Save job\n",
      "      </a>\n",
      "      <span class=\"result-link-bar-separator\">\n",
      "       ·\n",
      "      </span>\n",
      "      <button aria-expanded=\"false\" class=\"sl resultLink more-link\" id=\"tog_0\" onclick=\"toggleMoreLinks('f1d32093c4f114d2', '0'); return false;\">\n",
      "       More...\n",
      "      </button>\n",
      "     </div>\n",
      "     <script>\n",
      "      if (!window['result_f1d32093c4f114d2']) {window['result_f1d32093c4f114d2'] = {};}window['result_f1d32093c4f114d2']['showSource'] = false; window['result_f1d32093c4f114d2']['source'] = \"Spotify\"; window['result_f1d32093c4f114d2']['loggedIn'] = false; window['result_f1d32093c4f114d2']['showMyJobsLinks'] = false;window['result_f1d32093c4f114d2']['baseMyJobsUrl'] = \"https://myjobs.indeed.com\";window['result_f1d32093c4f114d2']['undoAction'] = \"unsave\";window['result_f1d32093c4f114d2']['relativeJobAge'] = \"5 days ago\";window['result_f1d32093c4f114d2']['jobKey'] = \"f1d32093c4f114d2\"; window['result_f1d32093c4f114d2']['myIndeedAvailable'] = true; window['result_f1d32093c4f114d2']['showMoreActionsLink'] = window['result_f1d32093c4f114d2']['showMoreActionsLink'] || true; window['result_f1d32093c4f114d2']['resultNumber'] = 0; window['result_f1d32093c4f114d2']['jobStateChangedToSaved'] = false; window['result_f1d32093c4f114d2']['searchState'] = \"q=data scientist $20,000&amp;l=New+York&amp;start=10\"; window['result_f1d32093c4f114d2']['basicPermaLink'] = \"https://www.indeed.com\"; window['result_f1d32093c4f114d2']['saveJobFailed'] = false; window['result_f1d32093c4f114d2']['removeJobFailed'] = false; window['result_f1d32093c4f114d2']['requestPending'] = false; window['result_f1d32093c4f114d2']['currentPage'] = \"serp\"; window['result_f1d32093c4f114d2']['sponsored'] = false;window['result_f1d32093c4f114d2']['reportJobButtonEnabled'] = false; window['result_f1d32093c4f114d2']['showMyJobsHired'] = false; window['result_f1d32093c4f114d2']['showSaveForSponsored'] = false; window['result_f1d32093c4f114d2']['showJobAge'] = true; window['result_f1d32093c4f114d2']['showHolisticCard'] = true; window['result_f1d32093c4f114d2']['showDislike'] = true; window['result_f1d32093c4f114d2']['showKebab'] = true; window['result_f1d32093c4f114d2']['showReport'] = true;\n",
      "     </script>\n",
      "    </div>\n",
      "   </div>\n",
      "  </div>\n",
      " </div>\n",
      " <div class=\"tab-container\">\n",
      "  <div class=\"more-links-container result-tab\" id=\"tt_display_0\" style=\"display:none;\">\n",
      "   <div class=\"more_actions\" id=\"more_0\">\n",
      "    <ul>\n",
      "     <li>\n",
      "      <span class=\"mat\">\n",
      "       View all\n",
      "       <a href=\"/q-Spotify-l-New-York,-NY-jobs.html\">\n",
      "        Spotify jobs in New York, NY\n",
      "       </a>\n",
      "       -\n",
      "       <a href=\"/l-New-York,-NY-jobs.html\">\n",
      "        New York jobs\n",
      "       </a>\n",
      "      </span>\n",
      "     </li>\n",
      "     <li>\n",
      "      <span class=\"mat\">\n",
      "       Salary Search:\n",
      "       <a href=\"/salaries/data-scientist-Salaries,-New-York-NY\" onmousedown=\"this.href = appendParamsOnce(this.href, '?campaignid=serp-more&amp;fromjk=f1d32093c4f114d2&amp;from=serp-more');\">\n",
      "        Data Scientist salaries in New York, NY\n",
      "       </a>\n",
      "      </span>\n",
      "     </li>\n",
      "     <li>\n",
      "      <span class=\"mat\">\n",
      "       Learn more about working at\n",
      "       <a href=\"/cmp/Spotify/about\" onmousedown=\"this.href = appendParamsOnce(this.href, '?fromjk=f1d32093c4f114d2&amp;from=serp-more&amp;campaignid=serp-more&amp;jcid=fe404d18bb9eef1e');\">\n",
      "        Spotify\n",
      "       </a>\n",
      "      </span>\n",
      "     </li>\n",
      "     <li>\n",
      "      <span class=\"mat\">\n",
      "       See popular\n",
      "       <a href=\"/cmp/Spotify/faq\" onmousedown=\"this.href = appendParamsOnce(this.href, '?from=serp-more&amp;campaignid=serp-more&amp;fromjk=f1d32093c4f114d2&amp;jcid=fe404d18bb9eef1e');\">\n",
      "        questions &amp; answers about Spotify\n",
      "       </a>\n",
      "      </span>\n",
      "     </li>\n",
      "     <li>\n",
      "      <span class=\"mat\">\n",
      "       Explore career as Data Scientist:\n",
      "       <a href=\"/career/data-scientist\" onmousedown=\"this.href = appendParamsOnce(this.href, 'from=jasx');\">\n",
      "        overview\n",
      "       </a>\n",
      "       ,\n",
      "       <a href=\"/career/data-scientist/career-advice\" onmousedown=\"this.href = appendParamsOnce(this.href, 'from=jasx');\">\n",
      "        career advice\n",
      "       </a>\n",
      "       ,\n",
      "       <a href=\"/career/data-scientist/faq\" onmousedown=\"this.href = appendParamsOnce(this.href, 'from=jasx');\">\n",
      "        FAQs\n",
      "       </a>\n",
      "      </span>\n",
      "     </li>\n",
      "    </ul>\n",
      "   </div>\n",
      "   <a class=\"close-link closeLink\" href=\"#\" onclick=\"toggleMoreLinks('f1d32093c4f114d2'); return false;\" title=\"Close\">\n",
      "   </a>\n",
      "  </div>\n",
      "  <div class=\"dya-container result-tab\">\n",
      "  </div>\n",
      "  <div class=\"tellafriend-container result-tab email_job_content\">\n",
      "  </div>\n",
      "  <div class=\"sign-in-container result-tab\">\n",
      "  </div>\n",
      " </div>\n",
      "</div>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(results[0].prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "bb0b866a-26a7-45e9-8084-5a0f90eb4b3e"
   },
   "source": [
    "Let's look at one result more closely. A single `result` looks like\n",
    "\n",
    "```\n",
    "<div class=\" row result\" data-jk=\"2480d203f7e97210\" data-tn-component=\"organicJob\" id=\"p_2480d203f7e97210\" itemscope=\"\" itemtype=\"http://schema.org/JobPosting\">\n",
    "<h2 class=\"jobtitle\" id=\"jl_2480d203f7e97210\">\n",
    "<a class=\"turnstileLink\" data-tn-element=\"jobTitle\" onmousedown=\"return rclk(this,jobmap[0],1);\" rel=\"nofollow\" target=\"_blank\" title=\"AVP/Quantitative Analyst\">AVP/Quantitative Analyst</a>\n",
    "</h2>\n",
    "<span class=\"company\" itemprop=\"hiringOrganization\" itemtype=\"http://schema.org/Organization\">\n",
    "<span itemprop=\"name\">\n",
    "<a href=\"/cmp/Alliancebernstein?from=SERP&amp;campaignid=serp-linkcompanyname&amp;fromjk=2480d203f7e97210&amp;jcid=b374f2a780e04789\" target=\"_blank\">\n",
    "    AllianceBernstein</a></span>\n",
    "</span>\n",
    "<tr>\n",
    "<td class=\"snip\">\n",
    "<nobr>$117,500 - $127,500 a year</nobr>\n",
    "<div>\n",
    "<span class=\"summary\" itemprop=\"description\">\n",
    "C onduct quantitative and statistical research as well as portfolio management for various investment portfolios. Collaborate with Quantitative Analysts and</span>\n",
    "</div>\n",
    "</div>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "</div>\n",
    "```\n",
    "\n",
    "While this has some more verbose elements removed, we can see that there is some structure to the above:\n",
    "- The salary is in a `span` with `class='salaryText'`.\n",
    "- The title of a job is in a link with class set to `jobtitle` and a `data-tn-element='jobTitle'`.  \n",
    "- The location is set in a `span` with `class='location'`. \n",
    "- The company is set in a `span` with `class='company'`. \n",
    "- Decide which other components could be relevant, for example the region or the summary of the job advert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = {\n",
    "    'salary': [],\n",
    "    'job_title': [],\n",
    "    'location': [],\n",
    "    'company': [],\n",
    "    'company_rating': [],\n",
    "    'description': []\n",
    "}\n",
    "\n",
    "for job in results:\n",
    "    try:\n",
    "        jobs['salary'].append(job.find('span', class_='salaryText').text)\n",
    "    except:\n",
    "        jobs['salary'].append(np.nan)\n",
    "        \n",
    "    try:\n",
    "        # job.find('h2', class_='title').find('a')['title']\n",
    "        jobs['job_title'].append(job.find('a', attrs={'data-tn-element': 'jobTitle'})['title'])\n",
    "    except:\n",
    "        jobs['job_title'].append(np.nan)\n",
    "        \n",
    "    try:\n",
    "        jobs['location'].append(job.find('span', class_='location').text.strip())  \n",
    "    except:\n",
    "        jobs['location'].append(np.nan)\n",
    "        \n",
    "    try:\n",
    "        company_name = job.find('a', attrs={'data-tn-element':'companyName'})\n",
    "        if company_name:\n",
    "            jobs['company'].append(company_name.text.strip())\n",
    "        else:\n",
    "            jobs['company'].append(job.find('span', class_='company').text.strip())\n",
    "    except:\n",
    "        jobs['company'].append(np.nan)\n",
    "        \n",
    "    try:\n",
    "        jobs['company_rating'].append(job.find('span', class_='ratingsContent').text.strip())\n",
    "    except:\n",
    "        jobs['company_rating'].append(np.nan)\n",
    "        \n",
    "    try:\n",
    "        lists = job.find('div', class_='summary').find_all('li')   \n",
    "        jobs['description'].append((' ').join([lst.text for lst in lists]))\n",
    "    except:\n",
    "        jobs['description'].append(np.nan)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'company': ['Spotify',\n",
      "             'Turner',\n",
      "             'Vee',\n",
      "             'Codecademy',\n",
      "             'MSCI Inc.',\n",
      "             'QuaEra',\n",
      "             'Rhino',\n",
      "             'White Plains Hospital',\n",
      "             'Olo',\n",
      "             'Turner',\n",
      "             'Atlassian',\n",
      "             'Naval Nuclear Laboratory',\n",
      "             'Dataiku',\n",
      "             'MassMutual',\n",
      "             'New York University'],\n",
      " 'company_rating': ['4.3',\n",
      "                    '4.0',\n",
      "                    '3.9',\n",
      "                    '4.2',\n",
      "                    '4.0',\n",
      "                    nan,\n",
      "                    nan,\n",
      "                    '4.0',\n",
      "                    '2.6',\n",
      "                    '4.0',\n",
      "                    '4.6',\n",
      "                    '2.8',\n",
      "                    nan,\n",
      "                    '3.7',\n",
      "                    '4.2'],\n",
      " 'description': ['You have experience working with financial data and/or '\n",
      "                 'performing data reconciliations and quality assurance '\n",
      "                 'testing.',\n",
      "                 'Develop video labeling models to identify objects, '\n",
      "                 'individuals and scene breaks of interest. Work with human '\n",
      "                 'annotators to develop a rigorous precision measure…',\n",
      "                 \"You will help build Vee's data capability and data strategy. \"\n",
      "                 'Solid background in data mining and statistical analysis.',\n",
      "                 'Work with our data science and engineering teams to maintain '\n",
      "                 'data integrity. You have strong data intuition and knowledge '\n",
      "                 'of using data science best practices…',\n",
      "                 'Use statistical methods and programing to analyze, '\n",
      "                 'interpret, and visualize data. Ability to overcome data '\n",
      "                 'quality challenges with creative solutions, including…',\n",
      "                 'Experience with R, Python, PySpark and data visualization '\n",
      "                 'software such as Tableau. Ability to efficiently construct '\n",
      "                 'data sets from large scale distributed…',\n",
      "                 \"Actively contribute to Rhino's Data culture by building out \"\n",
      "                 'core data models, tooling and best practices as well as '\n",
      "                 'training other Rhinos on using our data…',\n",
      "                 'Expert technical knowledge in data warehousing, advanced '\n",
      "                 'analytics, and data visualization. Integrate data from '\n",
      "                 'disparate sources.',\n",
      "                 'Handling data responsibly by maintaining data governance, '\n",
      "                 'privacy, and documentation. Hands on data extraction, data '\n",
      "                 'analysis, data cleaning, preparation,…',\n",
      "                 'Experience in data processing using SQL. The HBO Max Data '\n",
      "                 'Science team is responsible for designing and executing '\n",
      "                 'end-to-end data science solution to transform…',\n",
      "                 'As the ideal data scientist, you are someone who can both do '\n",
      "                 'a technical dive and communicate effectively with others.',\n",
      "                 'Processing, refining, and verifying the integrity of data '\n",
      "                 'used for analysis. Experience with data visualization '\n",
      "                 'packages, such as Tableau, SAS, and Rstudio.',\n",
      "                 'They should also be excited about learning new technologies '\n",
      "                 'and modeling techniques while being able to explain their '\n",
      "                 'work to other data scientists and clients.',\n",
      "                 '3+ years working with data and relevant computational '\n",
      "                 'frameworks and systemsÂ. Assemble data sets from disparate '\n",
      "                 'sources and analyze using appropriate…',\n",
      "                 \"Completion of Master's degree in disciplines aligned with \"\n",
      "                 'CDS faculty research by the start date.'],\n",
      " 'job_title': ['Data Scientist - Publishing Royalties',\n",
      "               'Virtual Data Science Intern, Summer 2021',\n",
      "               'NLP Data Scientist Internship',\n",
      "               'Data Scientist, Analytics & Inference - New York or Remote',\n",
      "               'ESG Data Scientist',\n",
      "               'Data Scientist',\n",
      "               'Data Scientist',\n",
      "               'Data Scientist',\n",
      "               'Data Scientist',\n",
      "               'Virtual HBO Max Data Science Intern –\\xa0Summer 2021',\n",
      "               'Data Scientist, Enterprise, Trello (US Remote)',\n",
      "               'Data Scientist',\n",
      "               'Data Scientist - AI Consulting',\n",
      "               'Data Scientist',\n",
      "               'Research Scientist-Center for Data Science'],\n",
      " 'location': ['New York, NY',\n",
      "              'New York, NY',\n",
      "              'New York State',\n",
      "              'New York, NY',\n",
      "              'New York, NY 10007 (Financial District area)',\n",
      "              'New York, NY 10018 (Garment District area)',\n",
      "              'New York State',\n",
      "              'White Plains, NY 10601',\n",
      "              'New York, NY 10012 (NoHo area)',\n",
      "              'New York, NY',\n",
      "              'New York, NY',\n",
      "              'Niskayuna, NY 12309',\n",
      "              'New York, NY',\n",
      "              'New York, NY',\n",
      "              'New York, NY 10012 (Greenwich Village area)'],\n",
      " 'salary': [nan,\n",
      "            nan,\n",
      "            nan,\n",
      "            nan,\n",
      "            nan,\n",
      "            nan,\n",
      "            nan,\n",
      "            nan,\n",
      "            nan,\n",
      "            nan,\n",
      "            nan,\n",
      "            nan,\n",
      "            nan,\n",
      "            nan,\n",
      "            nan]}\n"
     ]
    }
   ],
   "source": [
    "pprint(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>salary</th>\n",
       "      <th>job_title</th>\n",
       "      <th>location</th>\n",
       "      <th>company</th>\n",
       "      <th>company_rating</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Data Scientist - Publishing Royalties</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>Spotify</td>\n",
       "      <td>4.3</td>\n",
       "      <td>You have experience working with financial dat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Virtual Data Science Intern, Summer 2021</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>Turner</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Develop video labeling models to identify obje...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NLP Data Scientist Internship</td>\n",
       "      <td>New York State</td>\n",
       "      <td>Vee</td>\n",
       "      <td>3.9</td>\n",
       "      <td>You will help build Vee's data capability and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Data Scientist, Analytics &amp; Inference - New Yo...</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>Codecademy</td>\n",
       "      <td>4.2</td>\n",
       "      <td>Work with our data science and engineering tea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>ESG Data Scientist</td>\n",
       "      <td>New York, NY 10007 (Financial District area)</td>\n",
       "      <td>MSCI Inc.</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Use statistical methods and programing to anal...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   salary                                          job_title  \\\n",
       "0     NaN              Data Scientist - Publishing Royalties   \n",
       "1     NaN           Virtual Data Science Intern, Summer 2021   \n",
       "2     NaN                      NLP Data Scientist Internship   \n",
       "3     NaN  Data Scientist, Analytics & Inference - New Yo...   \n",
       "4     NaN                                 ESG Data Scientist   \n",
       "\n",
       "                                       location     company company_rating  \\\n",
       "0                                  New York, NY     Spotify            4.3   \n",
       "1                                  New York, NY      Turner            4.0   \n",
       "2                                New York State         Vee            3.9   \n",
       "3                                  New York, NY  Codecademy            4.2   \n",
       "4  New York, NY 10007 (Financial District area)   MSCI Inc.            4.0   \n",
       "\n",
       "                                         description  \n",
       "0  You have experience working with financial dat...  \n",
       "1  Develop video labeling models to identify obje...  \n",
       "2  You will help build Vee's data capability and ...  \n",
       "3  Work with our data science and engineering tea...  \n",
       "4  Use statistical methods and programing to anal...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(jobs).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "f1eddb90-4ba8-483c-a229-77e93aa53119"
   },
   "source": [
    "### Write 4 functions to extract each item: location, company, job, and salary.\n",
    "\n",
    "Example: \n",
    "```python\n",
    "def extract_location_from_result(result):\n",
    "    return result.find ...\n",
    "```\n",
    "\n",
    "\n",
    "- **Make sure these functions are robust and can handle cases where the data/field may not be available.**\n",
    "    - Remember to check if a field is empty or `None` for attempting to call methods on it.\n",
    "    - Remember to use `try/except` if you anticipate errors.\n",
    "- **Test** the functions on the results above and simple examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def not_null(content):\n",
    "    if content not in ('', None):\n",
    "        return content\n",
    "    else:\n",
    "        raise Exception('No content found!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "focus": false,
    "id": "a1af53c9-9090-494f-b82e-cadb60a54909"
   },
   "outputs": [],
   "source": [
    "def extract_location_from_result(result):\n",
    "    try:\n",
    "        data = result.find('span', class_='location').text.strip()\n",
    "        return not_null(data)\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_company_from_result(result):\n",
    "    try:\n",
    "        company_name = result.find('a', attrs={'data-tn-element':'companyName'})\n",
    "        if company_name:\n",
    "            return not_null(company_name.text.strip())\n",
    "        else:\n",
    "            data = result.find('span', class_='company').text.strip()\n",
    "            return not_null(data)\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_job_from_result(result):\n",
    "    try:\n",
    "        return not_null(result.find('a', attrs={'data-tn-element': 'jobTitle'})['title'])\n",
    "    except:\n",
    "        return np.nan\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_salary_from_result(result):\n",
    "    try:\n",
    "        return not_null(result.find('span', class_='salaryText').text.strip())\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rating_from_result(result): \n",
    "    try:\n",
    "        return not_null(result.find('span', class_='ratingsContent').text.strip())\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_summary_from_result(result):\n",
    "    try:\n",
    "        lists = result.find('div', class_='summary').find_all('li')   \n",
    "        return not_null((' ').join([lst.text for lst in lists]))\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(URL)\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "results = soup.find_all('div', class_='result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>location</th>\n",
       "      <th>company</th>\n",
       "      <th>salary</th>\n",
       "      <th>summary</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist - Publishing Royalties</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>Spotify</td>\n",
       "      <td>NaN</td>\n",
       "      <td>You have experience working with financial dat...</td>\n",
       "      <td>4.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Virtual Data Science Intern, Summer 2021</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>Turner</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Develop video labeling models to identify obje...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NLP Data Scientist Internship</td>\n",
       "      <td>New York State</td>\n",
       "      <td>Vee</td>\n",
       "      <td>NaN</td>\n",
       "      <td>You will help build Vee's data capability and ...</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist, Analytics &amp; Inference - New Yo...</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>Codecademy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Work with our data science and engineering tea...</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ESG Data Scientist</td>\n",
       "      <td>New York, NY 10007 (Financial District area)</td>\n",
       "      <td>MSCI Inc.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Use statistical methods and programing to anal...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>New York, NY 10018 (Garment District area)</td>\n",
       "      <td>QuaEra</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Experience with R, Python, PySpark and data vi...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>New York State</td>\n",
       "      <td>Rhino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Actively contribute to Rhino's Data culture by...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>White Plains, NY 10601</td>\n",
       "      <td>White Plains Hospital</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Expert technical knowledge in data warehousing...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>New York, NY 10012 (NoHo area)</td>\n",
       "      <td>Olo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Handling data responsibly by maintaining data ...</td>\n",
       "      <td>2.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Virtual HBO Max Data Science Intern – Summer 2021</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>Turner</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Experience in data processing using SQL. The H...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           job_title  \\\n",
       "0              Data Scientist - Publishing Royalties   \n",
       "1           Virtual Data Science Intern, Summer 2021   \n",
       "2                      NLP Data Scientist Internship   \n",
       "3  Data Scientist, Analytics & Inference - New Yo...   \n",
       "4                                 ESG Data Scientist   \n",
       "5                                     Data Scientist   \n",
       "6                                     Data Scientist   \n",
       "7                                     Data Scientist   \n",
       "8                                     Data Scientist   \n",
       "9  Virtual HBO Max Data Science Intern – Summer 2021   \n",
       "\n",
       "                                       location                company  \\\n",
       "0                                  New York, NY                Spotify   \n",
       "1                                  New York, NY                 Turner   \n",
       "2                                New York State                    Vee   \n",
       "3                                  New York, NY             Codecademy   \n",
       "4  New York, NY 10007 (Financial District area)              MSCI Inc.   \n",
       "5    New York, NY 10018 (Garment District area)                 QuaEra   \n",
       "6                                New York State                  Rhino   \n",
       "7                        White Plains, NY 10601  White Plains Hospital   \n",
       "8                New York, NY 10012 (NoHo area)                    Olo   \n",
       "9                                  New York, NY                 Turner   \n",
       "\n",
       "   salary                                            summary rating  \n",
       "0     NaN  You have experience working with financial dat...    4.3  \n",
       "1     NaN  Develop video labeling models to identify obje...    4.0  \n",
       "2     NaN  You will help build Vee's data capability and ...    3.9  \n",
       "3     NaN  Work with our data science and engineering tea...    4.2  \n",
       "4     NaN  Use statistical methods and programing to anal...    4.0  \n",
       "5     NaN  Experience with R, Python, PySpark and data vi...    NaN  \n",
       "6     NaN  Actively contribute to Rhino's Data culture by...    NaN  \n",
       "7     NaN  Expert technical knowledge in data warehousing...    4.0  \n",
       "8     NaN  Handling data responsibly by maintaining data ...    2.6  \n",
       "9     NaN  Experience in data processing using SQL. The H...    4.0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salary = []\n",
    "job_title = [] \n",
    "location = []\n",
    "company = []\n",
    "summary = []\n",
    "rating = []\n",
    "\n",
    "for job in results:\n",
    "    salary.append(extract_salary_from_result(job))\n",
    "    job_title.append(extract_job_from_result(job))\n",
    "    location.append(extract_location_from_result(job))\n",
    "    company.append(extract_company_from_result(job))\n",
    "    rating.append(extract_rating_from_result(job))\n",
    "    summary.append(extract_summary_from_result(job))\n",
    "    \n",
    "\n",
    "job_market = pd.DataFrame({'job_title': job_title,\n",
    "                           'location': location,\n",
    "                           'company': company,\n",
    "                           'salary': salary,\n",
    "                           'summary': summary,\n",
    "                           'rating': rating})\n",
    "\n",
    "job_market.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "34070e89-9521-4b45-90c8-57a6599aac68"
   },
   "source": [
    "Now, to scale up our scraping, we need to accumulate more results. We can do this by examining the URL above.\n",
    "\n",
    "- \"http://www.indeed.com/jobs?q=data+scientist+%2420%2C000&l=New+York&start=10\"\n",
    "\n",
    "There are two query parameters here we can alter to collect more results, the `l=New+York` and the `start=10`. The first controls the location of the results (so we can try a different city). The second controls where in the results to start and gives 10 results (thus, we can keep incrementing by 10 to go further in the list)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://www.indeed.com/jobs?q=data+scientist+%2420%2C000&l=New+York&start=10%22\n",
    "\n",
    "titles = ['data scientist', \n",
    "          'data analyst',\n",
    "          'research scientist',\n",
    "          'business intelligence',\n",
    "          'database developer',\n",
    "          'data engineer',\n",
    "          'database administrator']\n",
    "\n",
    "cities = ['New York',\n",
    "          'Dallas',\n",
    "          'Boston',\n",
    "          'Houston',\n",
    "          'San Francisco',\n",
    "          'Seattle',\n",
    "          'Austin',\n",
    "          'Miami',\n",
    "          'New Orleans',\n",
    "          'Atlanta',\n",
    "          'Jacksonville',\n",
    "          'Chicago',\n",
    "          'Philadelphia',\n",
    "          'Las Vegas',\n",
    "          'Los Angeles',\n",
    "          'Phoenix']\n",
    "\n",
    "titles_encoded = ('%2C+').join([t.replace(' ', '+') for t in set(titles)])\n",
    "\n",
    "# Ascii Encoding Reference:\n",
    "# space -> %20\n",
    "# # -> %23\n",
    "# $ -> %24\n",
    "# % -> %25\n",
    "# & -> %26\n",
    "# , -> %2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.scrapehero.com/how-to-fake-and-rotate-user-agents-using-python-3/\n",
    "\n",
    "user_agent_list = [\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1.1 Safari/605.1.15',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:77.0) Gecko/20100101 Firefox/77.0',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:77.0) Gecko/20100101 Firefox/77.0',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36',\n",
    "]\n",
    "\n",
    "headers = {\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\", \n",
    "    \"Accept-Encoding\": \"gzip, deflate, sdch\", \n",
    "    \"Accept-Language\": \"en-GB,en-US;q=0.9,en;q=0.8\", \n",
    "    \"Dnt\": \"1\", # do not track (1... prefers not to be tracked)\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Upgrade-Insecure-Requests\": \"1\",\n",
    "#     \"Host\": \"indeed.com\",\n",
    "#     \"Cache-Control\": \"max-age=0\",\n",
    "  }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.scrapehero.com/how-to-rotate-proxies-and-ip-addresses-using-python-3/\n",
    "# xpath expressions:\n",
    "# / -> selects from the root node (if occurring after a node then referring to children node)\n",
    "# // -> selects nodes from the current node\n",
    "# . -> selects current node\n",
    "\n",
    "from lxml.html import fromstring\n",
    "def get_proxies():\n",
    "#     url = 'https://free-proxy-list.net/'\n",
    "    url = 'https://www.us-proxy.org/'\n",
    "    response = requests.get(url)\n",
    "    # fromstring() returns document_fromstring or fragment_fromstring\n",
    "    parser = fromstring(response.text)\n",
    "    proxies = set()\n",
    "    for i in parser.xpath('//tbody/tr')[:20]:\n",
    "        if i.xpath('.//td[7][contains(text(),\"yes\")]'):  # <td class='hx'>yes</td>\n",
    "            #Grabbing IP and corresponding PORT\n",
    "            # i.xpath('.//td[1]')[0].text\n",
    "            proxy = \":\".join([i.xpath('.//td[1]/text()')[0], i.xpath('.//td[2]/text()')[0]])\n",
    "            proxies.add(proxy)\n",
    "    return proxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_indeed(cities, titles_encoded, headers=None, user_agent_list=None, delay=False):\n",
    "\n",
    "    salary = []\n",
    "    job_title = [] \n",
    "    location = []\n",
    "    company = []\n",
    "    rating = []\n",
    "    summary = []\n",
    "    url_template = \"http://www.indeed.com/jobs?q={job}+%2420%2C000&l={location}&start={page}\"\n",
    "    \n",
    "    if type(cities) != list:\n",
    "        raise Exception('cities must be a list')\n",
    "        return\n",
    "\n",
    "#     from itertools import cycle\n",
    "#     proxies = get_proxies()\n",
    "#     proxy_pool = cycle(proxies)\n",
    "\n",
    "    for c in tqdm(set(cities)):\n",
    "        if user_agent_list:\n",
    "            user_agent = random.choice(user_agent_list)\n",
    "            headers['User-Agent'] = user_agent\n",
    "#         proxy = next(proxy_pool)\n",
    "#         proxy_flag = False\n",
    "\n",
    "#         while not proxy_flag:\n",
    "#             try:\n",
    "#                 response = requests.get('http://www.indeed.com', proxies={'http': proxy, 'https': proxy})\n",
    "#                 print(f\"Request {c}\")\n",
    "#                 print(response.json())\n",
    "#                 proxy_flag = True\n",
    "#             except:\n",
    "#                 proxy = next(proxy_pool)\n",
    "#                 print(\"Skipping. Connnection error\")\n",
    "\n",
    "        for page in tqdm(range(10, 800, 10), position=0):\n",
    "            try:\n",
    "                r = requests.get(url_template.format(job=titles_encoded,\n",
    "                                                     location=c.replace(' ', '+'),\n",
    "                                                     page=page),\n",
    "                                 headers=headers)\n",
    "#                                  proxies={\"http\": proxy, \"https\": proxy})\n",
    "            except:\n",
    "                raise Exception('check headers')\n",
    "\n",
    "            soup = BeautifulSoup(r.text, 'html.parser')\n",
    "            results = soup.find_all('div', class_='result')\n",
    "\n",
    "            # when the last result is shown a message box appears which includes \n",
    "            # a paragraph with a class attribute called 'dupetext'\n",
    "            duplicate_msg = soup.find('p', class_='dupetext')\n",
    "\n",
    "            for job in results:\n",
    "                salary.append(extract_salary_from_result(job))\n",
    "                job_title.append(extract_job_from_result(job))\n",
    "                location.append(extract_location_from_result(job))\n",
    "                company.append(extract_company_from_result(job))\n",
    "                rating.append(extract_rating_from_result(job))\n",
    "                summary.append(extract_summary_from_result(job))\n",
    "\n",
    "            if not results or duplicate_msg:  # nothing found or end result is reached\n",
    "                if delay:\n",
    "                    time.sleep(15 + random.random() * 10)  # 15-25 sec, prevent to submit captchas\n",
    "                print(f'{c} with {page/10} pages.')\n",
    "                break\n",
    "            elif page % 80 == 0:\n",
    "                if delay:\n",
    "                    time.sleep(10 + random.random() * 5)  # 10-15 sec\n",
    "            else:\n",
    "#                 print(f'city: {c}, page: {page}')\n",
    "                if delay:\n",
    "                    time.sleep(1 + random.random() * 3)  # 1-4 sec\n",
    "                \n",
    "    df = pd.DataFrame({'job_title': job_title,\n",
    "                       'location': location,\n",
    "                       'company': company,\n",
    "                       'salary': salary,\n",
    "                       'rating': rating,\n",
    "                       'summary': summary})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_file_exists(filename, cities, titles_encoded, headers=None, user_agent_list=None, delay=False):\n",
    "    file = Path(f\"./datasets/{filename}.csv\")\n",
    "    if not file.exists():\n",
    "        df = scrape_indeed(cities=cities, titles_encoded=titles_encoded, \n",
    "                           headers=headers, user_agent_list=user_agent_list, \n",
    "                           delay=delay)\n",
    "        df.to_csv(f'./datasets/{filename}.csv', index=False)\n",
    "    else:\n",
    "        print('file already exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file already exists\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>location</th>\n",
       "      <th>company</th>\n",
       "      <th>salary</th>\n",
       "      <th>rating</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Syracuse, NY</td>\n",
       "      <td>Excelacom</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.2</td>\n",
       "      <td>Bachelor’s Degree or higher. Responds to data ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Analyst/Assistant Research Scientist (par...</td>\n",
       "      <td>New York, NY 10012 (Greenwich Village area)</td>\n",
       "      <td>New York University</td>\n",
       "      <td>$18 an hour</td>\n",
       "      <td>4.2</td>\n",
       "      <td>Cultivate deep familiarity with methodological...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Business Intelligence Analyst</td>\n",
       "      <td>New York, NY 10176 (Murray Hill area)</td>\n",
       "      <td>CBS Interactive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.5</td>\n",
       "      <td>As a business intelligence analyst, you’ll lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Business Data Analyst</td>\n",
       "      <td>Melville, NY 11747</td>\n",
       "      <td>North American Partners in Anesthesia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1+ year of hands-on experience with MS SQL as ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>Northwestern Mutual</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.8</td>\n",
       "      <td>This team focuses on developing data science m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           job_title  \\\n",
       "0                                       Data Analyst   \n",
       "1  Data Analyst/Assistant Research Scientist (par...   \n",
       "2                      Business Intelligence Analyst   \n",
       "3                              Business Data Analyst   \n",
       "4                                     Data Scientist   \n",
       "\n",
       "                                      location  \\\n",
       "0                                 Syracuse, NY   \n",
       "1  New York, NY 10012 (Greenwich Village area)   \n",
       "2        New York, NY 10176 (Murray Hill area)   \n",
       "3                           Melville, NY 11747   \n",
       "4                                 New York, NY   \n",
       "\n",
       "                                 company       salary  rating  \\\n",
       "0                              Excelacom          NaN     3.2   \n",
       "1                    New York University  $18 an hour     4.2   \n",
       "2                        CBS Interactive          NaN     3.5   \n",
       "3  North American Partners in Anesthesia          NaN     2.9   \n",
       "4                    Northwestern Mutual          NaN     3.8   \n",
       "\n",
       "                                             summary  \n",
       "0  Bachelor’s Degree or higher. Responds to data ...  \n",
       "1  Cultivate deep familiarity with methodological...  \n",
       "2  As a business intelligence analyst, you’ll lea...  \n",
       "3  1+ year of hands-on experience with MS SQL as ...  \n",
       "4  This team focuses on developing data science m...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_file_exists('df_0', cities=[cities[0]], titles_encoded=titles_encoded, headers=headers, user_agent_list=user_agent_list)\n",
    "df_0 = pd.read_csv('./datasets/df_0.csv')\n",
    "df_0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57, 6)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_0[df_0['salary'].notnull()].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "e8beed7c-3e42-40c0-810f-5f67f8f885a0"
   },
   "source": [
    "### Complete the following code to collect results from multiple cities and starting points. \n",
    "- Enter your city below to add it to the search.\n",
    "- Remember to convert your salary to U.S. Dollars to match the other cities if the currency is different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file already exists\n"
     ]
    }
   ],
   "source": [
    "check_file_exists('df_1', cities=[cities[1]], titles_encoded=titles_encoded, \n",
    "                  headers=headers, user_agent_list=user_agent_list)\n",
    "df_1 = pd.read_csv('./datasets/df_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file already exists\n"
     ]
    }
   ],
   "source": [
    "check_file_exists('df_2', cities=[cities[2]], titles_encoded=titles_encoded, \n",
    "                  headers=headers, user_agent_list=user_agent_list)\n",
    "df_2 = pd.read_csv('./datasets/df_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file already exists\n"
     ]
    }
   ],
   "source": [
    "check_file_exists('df_3', cities=[cities[3]], titles_encoded=titles_encoded, \n",
    "                  headers=headers, user_agent_list=user_agent_list)\n",
    "df_3 = pd.read_csv('./datasets/df_3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file already exists\n"
     ]
    }
   ],
   "source": [
    "check_file_exists('df_4', cities=[cities[4]], titles_encoded=titles_encoded, \n",
    "                  headers=headers, user_agent_list=user_agent_list)\n",
    "df_4 = pd.read_csv('./datasets/df_4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file already exists\n"
     ]
    }
   ],
   "source": [
    "check_file_exists('df_5', cities=[cities[5]], titles_encoded=titles_encoded)\n",
    "df_5 = pd.read_csv('./datasets/df_5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▎ | 66/79 [02:35<00:30,  2.36s/it]\n",
      "100%|██████████| 1/1 [02:35<00:00, 155.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Austin with 67.0 pages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "check_file_exists('df_6', cities=[cities[6]], titles_encoded=titles_encoded, \n",
    "                  headers=headers, user_agent_list=user_agent_list)\n",
    "df_6 = pd.read_csv('./datasets/df_6.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 59/79 [02:40<00:54,  2.71s/it]\n",
      "100%|██████████| 1/1 [02:40<00:00, 160.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Miami with 60.0 pages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "check_file_exists('df_7', cities=[cities[7]], titles_encoded=titles_encoded, \n",
    "                  headers=headers, user_agent_list=user_agent_list)\n",
    "df_7 = pd.read_csv('./datasets/df_7.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 24/79 [00:43<01:39,  1.80s/it]\n",
      "100%|██████████| 1/1 [00:43<00:00, 43.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Orleans with 25.0 pages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "check_file_exists('df_8', cities=[cities[8]], titles_encoded=titles_encoded, \n",
    "                  headers=headers, user_agent_list=user_agent_list)\n",
    "df_8 = pd.read_csv('./datasets/df_8.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 67/79 [02:01<00:21,  1.81s/it]\n",
      "100%|██████████| 1/1 [02:01<00:00, 121.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atlanta with 68.0 pages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "check_file_exists('df_9', cities=[cities[9]], titles_encoded=titles_encoded, \n",
    "                  headers=headers, user_agent_list=user_agent_list)\n",
    "df_9 = pd.read_csv('./datasets/df_9.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 56/79 [01:47<00:44,  1.92s/it]\n",
      "100%|██████████| 1/1 [01:47<00:00, 107.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacksonville with 57.0 pages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "check_file_exists('df_10', cities=[cities[10]], titles_encoded=titles_encoded, \n",
    "                  headers=headers, user_agent_list=user_agent_list)\n",
    "df_10 = pd.read_csv('./datasets/df_10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▎ | 66/79 [02:01<00:24,  1.85s/it]\n",
      "100%|██████████| 1/1 [02:01<00:00, 121.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chicago with 67.0 pages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "check_file_exists('df_11', cities=[cities[11]], titles_encoded=titles_encoded, \n",
    "                  headers=headers, user_agent_list=user_agent_list)\n",
    "df_11 = pd.read_csv('./datasets/df_11.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 65/79 [01:37<00:21,  1.50s/it]\n",
      "100%|██████████| 1/1 [01:37<00:00, 97.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Philadelphia with 66.0 pages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "check_file_exists('df_12', cities=[cities[12]], titles_encoded=titles_encoded, \n",
    "                  headers=headers, user_agent_list=user_agent_list)\n",
    "df_12 = pd.read_csv('./datasets/df_12.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 42/79 [01:18<01:08,  1.86s/it]\n",
      "100%|██████████| 1/1 [01:18<00:00, 78.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Las Vegas with 43.0 pages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "check_file_exists('df_13', cities=[cities[13]], titles_encoded=titles_encoded, \n",
    "                  headers=headers, user_agent_list=user_agent_list)\n",
    "df_13 = pd.read_csv('./datasets/df_13.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 63/79 [01:53<00:28,  1.81s/it]\n",
      "100%|██████████| 1/1 [01:53<00:00, 113.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los Angeles with 64.0 pages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "check_file_exists('df_14', cities=[cities[14]], titles_encoded=titles_encoded, \n",
    "                  headers=headers, user_agent_list=user_agent_list)\n",
    "df_14 = pd.read_csv('./datasets/df_14.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 62/79 [02:11<00:36,  2.13s/it]\n",
      "100%|██████████| 1/1 [02:11<00:00, 131.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phoenix with 63.0 pages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "check_file_exists('df_15', cities=[cities[15]], titles_encoded=titles_encoded, \n",
    "                  headers=headers, user_agent_list=user_agent_list)\n",
    "df_15 = pd.read_csv('./datasets/df_15.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 65/79 [02:16<00:29,  2.10s/it]\n",
      "100%|██████████| 1/1 [02:16<00:00, 136.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "London with 66.0 pages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "MY_CITY = 'London'  # no exchange conversion needed, autimatically set to dollar\n",
    "check_file_exists('df_16', cities=[MY_CITY], titles_encoded=titles_encoded, \n",
    "                  headers=headers, user_agent_list=user_agent_list)\n",
    "df_16 = pd.read_csv('./datasets/df_16.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(715, 7)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14992, 7)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if MY_CITY not in cities:\n",
    "#     cities.append(MY_CITY)\n",
    "# for i in range(len(cities)):\n",
    "#     globals()[f'df_{i}']['city'] = np.repeat(cities[i], globals()[f'df_{i}'].shape[0])\n",
    "    \n",
    "jobs = pd.concat([df_0, df_1, df_2, df_3, df_4, df_5, df_6, df_7, df_8,\n",
    "                  df_9, df_10, df_11, df_12, df_13, df_14, df_15, df_16], axis=0)\n",
    "jobs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>location</th>\n",
       "      <th>company</th>\n",
       "      <th>salary</th>\n",
       "      <th>rating</th>\n",
       "      <th>summary</th>\n",
       "      <th>city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>Research Analyst Part-Time</td>\n",
       "      <td>Philadelphia, PA 19106 (Old City area)</td>\n",
       "      <td>Federal Reserve Bank of Philadelphia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.3</td>\n",
       "      <td>Coordinates research projects and directs acti...</td>\n",
       "      <td>Philadelphia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>Nevada, Testing Coordinator (NVVA)</td>\n",
       "      <td>Las Vegas, NV</td>\n",
       "      <td>Stride, Inc.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Essential Functions: Reasonable accommodations...</td>\n",
       "      <td>Las Vegas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Irving, TX</td>\n",
       "      <td>TCS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.8</td>\n",
       "      <td>Minimum 5 years of experience. Retail Banking ...</td>\n",
       "      <td>Dallas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>657</th>\n",
       "      <td>Sr. Associate - Projects</td>\n",
       "      <td>Plano, TX</td>\n",
       "      <td>Cognizant Technology Solutions</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.9</td>\n",
       "      <td>Participate in requirement gathering JAD sessi...</td>\n",
       "      <td>Dallas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>Senior Analyst, Corporate Development</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Convoy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.7</td>\n",
       "      <td>Support integration efforts by partnering with...</td>\n",
       "      <td>Seattle</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 job_title  \\\n",
       "122             Research Analyst Part-Time   \n",
       "517     Nevada, Testing Coordinator (NVVA)   \n",
       "115                           Data Analyst   \n",
       "657               Sr. Associate - Projects   \n",
       "546  Senior Analyst, Corporate Development   \n",
       "\n",
       "                                   location  \\\n",
       "122  Philadelphia, PA 19106 (Old City area)   \n",
       "517                           Las Vegas, NV   \n",
       "115                              Irving, TX   \n",
       "657                               Plano, TX   \n",
       "546                             Seattle, WA   \n",
       "\n",
       "                                  company salary  rating  \\\n",
       "122  Federal Reserve Bank of Philadelphia    NaN     4.3   \n",
       "517                          Stride, Inc.    NaN     NaN   \n",
       "115                                   TCS    NaN     3.8   \n",
       "657        Cognizant Technology Solutions    NaN     3.9   \n",
       "546                                Convoy    NaN     3.7   \n",
       "\n",
       "                                               summary          city  \n",
       "122  Coordinates research projects and directs acti...  Philadelphia  \n",
       "517  Essential Functions: Reasonable accommodations...     Las Vegas  \n",
       "115  Minimum 5 years of experience. Retail Banking ...        Dallas  \n",
       "657  Participate in requirement gathering JAD sessi...        Dallas  \n",
       "546  Support integration efforts by partnering with...       Seattle  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2061"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs[jobs['salary'].notnull()].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "20339c09-5032-4e27-91be-286e9b46cd13"
   },
   "source": [
    "#### Use the functions you wrote above to parse out the 4 fields - location, title, company and salary. Create a dataframe from the results with those 4 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from selenium import webdriver\n",
    "# from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def not_null1(content):\n",
    "#     if content not in ('', None):\n",
    "#         return content\n",
    "#     else:\n",
    "#         raise Exception('No content found!')\n",
    "        \n",
    "# def extract_location_from_result1(result):\n",
    "#     try:\n",
    "#         data = result.find_element_by_css_selector('span.location').text.strip()\n",
    "#         return not_null1(data)\n",
    "#     except:\n",
    "#         return np.nan\n",
    "    \n",
    "    \n",
    "# def extract_job_from_result1(result):\n",
    "#     try:\n",
    "#         return not_null1(result.find_element_by_css_selector('a[data-tn-element=jobTitle]').get_attribute('title'))                                                        \n",
    "#     except:\n",
    "#         return np.nan\n",
    "    \n",
    "# def extract_company_from_result1(result):\n",
    "#     try:\n",
    "#         company_name = result.find_element_by_css_selector('a[data-tn-element=companyName]') \n",
    "#         if company_name:\n",
    "#             return company_name.text.strip()\n",
    "#         else:\n",
    "#             data = result.find_element_by_css_selector('span.company').text.strip()\n",
    "#             return not_null1(data)\n",
    "#     except:\n",
    "#         return np.nan\n",
    "    \n",
    "# def extract_salary_from_result1(result):\n",
    "#     try:\n",
    "#         return not_null1(result.find_element_by_css_selector('span.salaryText').text.strip())\n",
    "#     except:\n",
    "#         return np.nan\n",
    "\n",
    "# driver = webdriver.Chrome(executable_path='/Users/gabriel/Desktop/DataScience/chromedriver')\n",
    "\n",
    "\n",
    "# def extract_summary1(result):\n",
    "#     sleep(2)\n",
    "\n",
    "#     try:\n",
    "#         summary_html = result.find_element_by_css_selector('div#vjs-desc').get_attribute('innerHTML')\n",
    "#         return not_null1(summary_html)\n",
    "#     except:\n",
    "#         return np.nan\n",
    "\n",
    "\n",
    "# url_template = \"http://www.indeed.com/jobs?q={job}+%2420%2C000&l={location}&start={page}\"\n",
    "# max_results_per_city = 10 #5000 \n",
    "\n",
    "# titles = ['data scientist', \n",
    "#           'data analyst',\n",
    "#           'research scientist',\n",
    "#           'business intelligence',\n",
    "#           'database developer',\n",
    "#           'data engineer',\n",
    "#           'database administrator']\n",
    "\n",
    "# titles_encoded = ('%2C+').join([t.replace(' ', '+') for t in titles])\n",
    "# titles_conded = 'data+scientist'\n",
    "                        \n",
    "# salary1 = []\n",
    "# job_title1 = [] \n",
    "# location1 = []\n",
    "# company1 = []\n",
    "# summary1 = []\n",
    "\n",
    "# for city in tqdm(set(['New+York'])): #, 'Chicago', 'San+Francisco', 'Austin', 'Seattle', \n",
    "# #     'Los+Angeles', 'Philadelphia', 'Atlanta', 'Dallas', 'Pittsburgh', \n",
    "# #     'Portland', 'Phoenix', 'Denver', 'Houston', 'Miami', YOUR_CITY])):\n",
    "#     counter = 0\n",
    "#     for start in range(0, max_results_per_city, 10):\n",
    "#         r = driver.get(url_template.format(job=titles_encoded,\n",
    "#                                              location=city,\n",
    "#                                              page=start))\n",
    "        \n",
    "#         results = driver.find_elements_by_css_selector('div.result')\n",
    "#         try:\n",
    "#             duplicate_test = driver.find_element_by_css_selector('p.dupetext')\n",
    "#         except:\n",
    "#             duplicate_test = None\n",
    "#         try:\n",
    "#             duplicate_test2 = driver.find_element_by_css_selector('div.related_searches') \n",
    "#         except:\n",
    "#             duplacte_test2 = None\n",
    "        \n",
    "#         print(f'City: {city}, Page: {start}, counter: {counter}, duplicate_test: {bool(duplicate_test) or bool(duplicate_test2)}')\n",
    "\n",
    "        \n",
    "#         if duplicate_test or duplicate_test2:\n",
    "#             counter += 1    \n",
    "#             if counter > 1:\n",
    "#                 break\n",
    "    \n",
    "\n",
    "#         for job in results:\n",
    "#             salary1.append(extract_salary_from_result1(job))\n",
    "#             job_title1.append(extract_job_from_result1(job))\n",
    "#             location1.append(extract_location_from_result1(job))\n",
    "#             company1.append(extract_company_from_result1(job))\n",
    "#             job.click()\n",
    "#             summary1.append(extract_summary1(driver))\n",
    "                        \n",
    "# test1 = pd.DataFrame({'tilte': job_title1, 'location': location1, 'salary': salary1, 'company': company1,\n",
    "#                      'summary': summary1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "ff98ce64-78a7-441f-a675-63464e32c834"
   },
   "source": [
    "Lastly, we need to clean up salary data. \n",
    "\n",
    "1. Only a small number of the scraped results have salary information - only these will be used for modeling.\n",
    "1. Some of the salaries are not yearly but hourly or weekly, these will not be useful to us for now.\n",
    "1. Some of the entries may be duplicated.\n",
    "1. The salaries are given as text and usually with ranges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "ff98ce64-78a7-441f-a675-63464e32c834"
   },
   "source": [
    "#### Find the entries with annual salary entries, by filtering the entries without salaries or salaries that are not yearly (filter those that refer to hour or week). Also, remove duplicate entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = jobs_list.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jobs[jobs['salary'].notnull()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs.dropna(subset=['salary'], inplace=True)\n",
    "jobs = jobs[jobs['salary'].str.contains(r'year')]\n",
    "jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "7d4bc860-b214-4f75-9cd0-b234830b1ec2"
   },
   "source": [
    "#### Write a function that takes a salary string and converts it to a number, averaging a salary range if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sal_to_num(salary):    \n",
    "#     sal= int    \n",
    "#     '''Takes in a salary entry (as string) and extracts the digits as an integer.    \n",
    "#     If the salary is in a range (eg 20,000-25,00) it returns the average'''    \n",
    "    \n",
    "#     try:        \n",
    "#         sal = int(''.join(re.findall(r'([\\d-])', salary)))    \n",
    "#     except:        \n",
    "#         sal = (int(''.join(re.findall(r'([\\d-])', salary)).split(\"-\")[0]) + \n",
    "#                int(''.join(re.findall(r'([\\d-])', salary)).split(\"-\")[1])) / 2            \n",
    "        \n",
    "#     return int(sal)        \n",
    "\n",
    "# df.loc[:,'Salary'] = df.Salary.map(lambda x: sal_to_num(x)).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "focus": false,
    "id": "a0f701e0-80bd-40ba-9101-4535860c0968"
   },
   "outputs": [],
   "source": [
    "def convert_salary(salary, currency_rate=1):\n",
    "    try:\n",
    "        sal = np.mean([float(re.sub(r'[, ]', '', s)) for s in re.findall(r'\\d+[, ]\\d{3,}', salary)])\n",
    "    except:\n",
    "        sal = np.nan\n",
    "    \n",
    "    return sal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs['salary'] = jobs['salary'].apply(lambda x: convert_salary(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "43e71edd-210e-42b1-9336-70a931f048af"
   },
   "source": [
    "### Save your results as a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs.to_csv('jobs.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "243e949e-2742-40af-872e-fec475fd306c"
   },
   "source": [
    "### Load in the the data of scraped salaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "focus": false,
    "id": "588f9845-6143-4bcc-bfd1-85d45b79303d"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('jobs.csv')\n",
    "jobs = df.copy()\n",
    "jobs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "c7631f51-07f2-4c79-a093-3e9bc7849a48"
   },
   "source": [
    "### We want to predict a binary variable - whether the salary was low or high. Compute the median salary and create a new binary variable that is true when the salary is high (above the median).\n",
    "\n",
    "We could also perform Linear Regression (or any regression) to predict the salary value here. Instead, we are going to convert this into a _binary_ classification problem, by predicting two classes, HIGH vs LOW salary.\n",
    "\n",
    "While performing regression may be better, performing classification may help remove some of the noise of the extreme salaries. We don't have to choose the `median` as the splitting point - we could also split on the 75th percentile or any other reasonable breaking point.\n",
    "\n",
    "In fact, the ideal scenario may be to predict many levels of salaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "focus": false,
    "id": "c20d2498-151c-44c3-a453-3a333c79a0ac"
   },
   "outputs": [],
   "source": [
    "jobs.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs[['rating', 'salary']].corr().iloc[0,1].round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs['rating'].fillna(value=jobs['rating'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs['summary'].fillna(value='Nothing', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_location(data, c=True):\n",
    "    a = re.findall(r'[A-Z][a-z]+', data)\n",
    "    city = (' ').join(a) if a else np.nan\n",
    "    b = re.search(r'[A-Z]{2}', data)\n",
    "    state =  b.group() if b else np.nan\n",
    "    if c:\n",
    "        return city\n",
    "    else:\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs['city'] = jobs['location'].apply(lambda x: split_location(x))\n",
    "jobs['state'] = jobs['location'].apply(lambda x: split_location(x, False))\n",
    "jobs.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_states_indices = jobs[jobs['state'].isnull()].index.tolist()\n",
    "empty_states_indices\n",
    "jobs.loc[empty_states_indices, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jobs['state'].iloc[empty_states_indices] = 'NY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs['state'].fillna(value='NY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs.describe(include=['O'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs['job_title'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs['job_title'].sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs['company'].str.lower().nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jobs['company'].apply(lambda x: x.split(',')[0]).nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_salary = jobs['salary'].median()\n",
    "median_salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs['high_salary'] = jobs['salary'].map(lambda x: 1 if x > median_salary else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs['state'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs = jobs.corr()  # only numerical columns\n",
    "mask = np.zeros_like(corrs, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "sns.heatmap(corrs, mask=mask, annot=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "\n",
    "sns.boxplot(['salary'], data=jobs, orient='h', fliersize=8, \n",
    "            linewidth=1.5, saturation=0.5, whis=1.5, ax=ax)\n",
    "ax.set_title('Salary\\n', fontsize=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_outlier_index = jobs.loc[jobs['salary']> 300000].index\n",
    "jobs.drop(index=salary_outlier_index, inplace=True)\n",
    "jobs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "\n",
    "sns.boxplot(['rating'], data=jobs, orient='h', fliersize=8, \n",
    "            linewidth=1.5, saturation=0.5, whis=1.5, ax=ax)\n",
    "ax.set_title('Rating\\n', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(jobs, hue='high_salary');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "a7afb2c0-d41e-4779-8216-91cd8dd4473f"
   },
   "source": [
    "#### Thought experiment: What is the baseline accuracy for this model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "focus": false,
    "id": "87a17d3d-b7f4-4747-9f75-f9af1d18a174"
   },
   "outputs": [],
   "source": [
    "baseline_accuracy = round(jobs['high_salary'].value_counts(normalize=True).max(), 4)\n",
    "baseline_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "4fb29de2-5b98-474c-a4ad-5170b72b9aea"
   },
   "source": [
    "### Create a classification model to predict High/Low salary. \n",
    "\n",
    "\n",
    "- Start by ONLY using the location as a feature.\n",
    "- Use at least two different classifiers you find suitable.\n",
    "- Remember that scaling your features might be necessary.\n",
    "- Display the coefficients/feature importances and write a short summary of what they mean.\n",
    "- Create a few new variables in your dataframe to represent interesting features of a job title (e.g. whether 'Senior' or 'Manager' is in the title).\n",
    "- Incorporate other text features from the title or summary that you believe will predict the salary.\n",
    "- Then build new classification models including also those features. Do they add any value?\n",
    "- Tune your models by testing parameter ranges, regularization strengths, etc. Discuss how that affects your models.\n",
    "- Discuss model coefficients or feature importances as applicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, make_union, make_pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = jobs[['city', 'state', 'high_salary']]\n",
    "X = pd.get_dummies(X, prefix=['city', 'state'], columns=['city', 'state'], drop_first=True)\n",
    "X.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = X.pop('high_salary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropDummyfierPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, columns_to_drop=None, columns_to_dummify=None, drop_first=True):\n",
    "        self.feature_names = []\n",
    "        self.columns_to_drop = columns_to_drop\n",
    "        self.columns_to_dummify = columns_to_dummify\n",
    "        self.drop_first = drop_first\n",
    "        \n",
    "    def _drop_unused_cols(self, X):\n",
    "        \"\"\"Try to drop each given column by its own\"\"\"\n",
    "        for col in self.columns_to_drop:\n",
    "            try:\n",
    "                X = X.drop(col, axis=1)\n",
    "            except:\n",
    "                pass\n",
    "        return X\n",
    "\n",
    "    def _make_dummy_cols(self, X):\n",
    "        X = pd.get_dummies(X, columns=self.columns_to_dummify, drop_first=self.drop_first)\n",
    "        return X\n",
    "    \n",
    "    def transform(self, X, *args):\n",
    "        X = self._make_dummy_cols(X)\n",
    "        X = self._drop_unused_cols(X)\n",
    "        self.feature_names = X.columns\n",
    "        return X\n",
    "    \n",
    "    def fit(self, X, *args):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, column):\n",
    "        self.column = column\n",
    "        \n",
    "    def fit(self, X, *args):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, *args):\n",
    "        return X[self.column].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, BaggingClassifier, \\\n",
    "AdaBoostClassifier, AdaBoostRegressor, GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                    stratify=y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'estimator': [DecisionTreeClassifier(), \n",
    "                  DecisionTreeRegressor(),\n",
    "                  LogisticRegression(solver='liblinear'),\n",
    "                  KNeighborsClassifier(),\n",
    "                 ],\n",
    "    \n",
    "    'param_grid': [\n",
    "        {\n",
    "            'max_depth': [1, 2, 3],\n",
    "            'max_features': [None, 1, 2, 3],\n",
    "            'min_samples_split': [2, 3, 4, 20,  30, 50],\n",
    "            'ccp_alpha': [0, 0.001, 0.005, 0.01]\n",
    "        },\n",
    "        {\n",
    "            'max_depth': [1, 2, 3],\n",
    "            'max_features': [None, 1, 2, 3],\n",
    "            'min_samples_split': [2, 3, 4, 20,  30, 50],\n",
    "            'ccp_alpha': [0, 0.001, 0.005, 0.01]\n",
    "        },\n",
    "        {\n",
    "            'C': np.logspace(-4, 4, 15), \n",
    "            'penalty': ['l2', 'l1'],\n",
    "        },\n",
    "        {\n",
    "            'n_neighbors': range(3, 100, 2),\n",
    "            'weights': ['uniform', 'distance']\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estimators_scores(X_train, y_train, X_test=None, y_test=None, \n",
    "                          models=[], params=[], verbose=1, n_jobs=2, cv=5):\n",
    "    best_estimators = []\n",
    "    for i, _ in enumerate(models):\n",
    "        p = {'param_grid': params[i]}\n",
    "        gs = GridSearchCV(estimator=models[i], \n",
    "                          verbose = verbose,\n",
    "                          n_jobs = n_jobs,\n",
    "                          cv = cv,\n",
    "                          **p)\n",
    "        gs.fit(X_train, y_train)\n",
    "        model_name = type(gs.estimator).__name__\n",
    "        print('{model} - Accuracy score: {score}'.format(model=model_name,\n",
    "                                                         score=gs.score(X_train, y_train).round(4)))\n",
    "        print('{model} - CV training score: {score}'.format(model=model_name, \n",
    "                                                            score=gs.best_score_.round(4)))\n",
    "        \n",
    "        if isinstance(X_test, pd.DataFrame) and isinstance(y_test, pd.Series):\n",
    "            print('{model} - CV test score: {score}'.format(model=model_name,\n",
    "                                                            score=gs.score(X_test, y_test).round(4)))\n",
    "        print()\n",
    "        best_estimators.append(gs)\n",
    "    return best_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = get_estimators_scores(X_train, y_train, X_test, y_test,\n",
    "                                  models = model_params['estimator'], \n",
    "                                  params = model_params['param_grid'],\n",
    "                                  cv = skf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DecisionTreeClassifier/ DecisionTreeRegressor:\n",
    "- Feature Importance is computed as the total reduction of the criterion brought by that feature. Because the default setting for that classifier was used, it refers to the gini score. The state California has the highest effect to the classification in the given case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DecisionTreeClassifier\n",
    "pd.DataFrame(list(zip(X_train.columns, estimators[0].best_estimator_.feature_importances_)), \n",
    "             columns=['Feature', 'Importance']\n",
    "            ).sort_values(by='Importance', ascending=False)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DecisionTreeRegressor\n",
    "pd.DataFrame(list(zip(X_train.columns, estimators[1].best_estimator_.feature_importances_)), \n",
    "             columns=['Feature', 'Importance']\n",
    "            ).sort_values(by='Importance', ascending=False)[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LogisticRegression:\n",
    "- If following link function is used:\n",
    "$$P(y=1|X)=\\frac{1}{1+e^{-z}}$$,\n",
    "then is the probability to predict class 1 by 50% if the sum of all coefficients multiplied by their features is 0. Otherwise if z increases the probability for predicting class 1 increases as well and vice versa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LogisticRegression\n",
    "pd.DataFrame({'Feature': X_train.columns, \n",
    "              'Coefficient': estimators[2].best_estimator_.coef_[0], \n",
    "              'Abs_Coefficient': abs(estimators[2].best_estimator_.coef_[0])}\n",
    "            ).sort_values(by='Abs_Coefficient', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = jobs.copy()\n",
    "X.drop(['salary'], axis=1, inplace=True)\n",
    "y = X.pop('high_salary')\n",
    "#X.reset_index(inplace=True)\n",
    "X.drop(columns=['location', 'summary'], inplace=True)\n",
    "X = pd.get_dummies(X, columns=['company', 'city', 'state'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                    stratify=y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvec = TfidfVectorizer(stop_words='english',\n",
    "                            #sublinear_tf=True,\n",
    "                            max_df=0.3,\n",
    "                            max_features=1000)\n",
    "\n",
    "tvec.fit(X_train['job_title']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_job_title = tvec.transform(X_train['job_title'])\n",
    "X_test_job_title = tvec.transform(X_test['job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(X_train_job_title.toarray(),\n",
    "                  columns=tvec.get_feature_names())\n",
    "\n",
    "df_train.transpose().sort_values(0, ascending=False).transpose().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame(X_test_job_title.toarray(),\n",
    "                  columns=tvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.drop(columns=['job_title'], axis=1, inplace=True)\n",
    "X_test.drop(columns=['job_title'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.reset_index(inplace=True)\n",
    "X_test.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat([X_train, df_train], axis=1)\n",
    "X_test = pd.concat([X_test, df_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X= pd.concat([X.loc[:, ~X.columns.isin(['job_title'])], df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jobs_prep = DropDummyfierPreprocessor(columns_to_drop=['location', 'summary'], \n",
    "#                               columns_to_dummify=['company', 'city', 'state'])\n",
    "\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# pipe = Pipeline(steps=[('jobs_prep', jobs_prep),\n",
    "#                        ('scaler', scaler)])\n",
    "\n",
    "# p = pipe.fit(X, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = get_estimators_scores(X_train, y_train, X_test, y_test,\n",
    "                                  models = model_params['estimator'], \n",
    "                                  params = model_params['param_grid'],\n",
    "                                  cv = skf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "# confusion matrix\n",
    "def docm(y_true, y_pred, labels=None):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    if labels is not None:\n",
    "        cols = ['pred_' + c for c in labels]\n",
    "        df = pd.DataFrame(cm, index=labels, columns=cols)\n",
    "    else:\n",
    "        cols = ['pred_'+str(i) for i in range(len(cm))]\n",
    "        df = pd.DataFrame(cm, columns=cols)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DecisionTreeClassifier\n",
    "docm(y_test, estimators[0].best_estimator_.predict(X_test), labels=['high_salary', 'low_salary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LogisticRegression\n",
    "docm(y_test, estimators[2].best_estimator_.predict(X_test), labels=['high_salary', 'low_salary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(gamma='scale')\n",
    "rfc = RandomForestClassifier(n_estimators=100)\n",
    "etc = ExtraTreesClassifier(n_estimators=100)\n",
    "rc = RidgeClassifier(tol=1e-2, solver=\"sag\")\n",
    "per = Perceptron(max_iter=1000, tol=1e-3)\n",
    "pac = PassiveAggressiveClassifier(max_iter=1000, tol=1e-3)\n",
    "lsvc = LinearSVC(loss='squared_hinge', \n",
    "                 penalty='l2',\n",
    "                 max_iter=100000,\n",
    "                 #tol=0.01, \n",
    "                 dual=False)\n",
    "sgdc = SGDClassifier(alpha=.0001,\n",
    "                     penalty=\"elasticnet\",\n",
    "                     max_iter=1000,\n",
    "                     tol=1e-3)\n",
    "knn = estimators[3].best_estimator_\n",
    "bg_knn = BaggingClassifier(base_estimator=knn, \n",
    "                           n_estimators=100, \n",
    "                           max_samples=0.8,\n",
    "                           max_features=0.8)\n",
    "dtc = estimators[0].best_estimator_\n",
    "bg_dtc = BaggingClassifier(base_estimator=dtc, \n",
    "                           n_estimators=100, \n",
    "                           max_samples=0.8,\n",
    "                           max_features=0.8)\n",
    "adaboost_dtc = AdaBoostClassifier(base_estimator=dtc,\n",
    "                                  n_estimators=10,\n",
    "                                  algorithm='SAMME')\n",
    "gradientboost = GradientBoostingClassifier(n_estimators=100,\n",
    "                                           criterion='mse',\n",
    "                                           loss='exponential',\n",
    "                                           max_depth=3,\n",
    "                                           learning_rate=1.0)\n",
    "\n",
    "\n",
    "for model in [svc, rfc, etc, rc, per, pac, lsvc, sgdc, bg_knn, bg_dtc, adaboost_dtc, gradientboost]:\n",
    "    model.fit(X_train, y_train)\n",
    "    model_name = type(model).__name__\n",
    "    print('{model} - Accuracy score: {score}' \\\n",
    "              .format(model=model_name,\n",
    "                      score=model.score(X_train, y_train).round(4)))\n",
    "    print('{model} - CV training score: {score}' \\\n",
    "              .format(model=model_name,\n",
    "                      score=cross_val_score(model, X_train, y_train, cv=skf).mean().round(4)))\n",
    "    print('{model} - CV testing score: {score}' \\\n",
    "              .format(model=model_name, \n",
    "                      score=cross_val_score(model, X_test, y_test, cv=skf).mean().round(4)))    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tuning of hyperparameters with gridsearch\n",
    "model_params = {\n",
    "    'models': [RandomForestClassifier(n_estimators=200), \n",
    "               ExtraTreesClassifier(n_estimators=200), \n",
    "               LinearSVC(dual=False)],\n",
    "    'gs_params': [\n",
    "        {\n",
    "            'max_depth': [1, 2, 3, None],\n",
    "            'max_features': [None, 1, 2, 3],\n",
    "            'min_samples_split': [2, 3, 4, 20,  30, 50],\n",
    "            'ccp_alpha': [0, 0.001, 0.005, 0.01, 0.1]\n",
    "        },\n",
    "        {\n",
    "            'max_depth': [1, 2, 3, None],\n",
    "            'max_features': [None, 1, 2, 3],\n",
    "            'min_samples_split': [2, 3, 4, 20,  30, 50],\n",
    "            'ccp_alpha': [0, 0.001, 0.005, 0.01, 0.1]\n",
    "        },\n",
    "        {\n",
    "            'penalty': ['l2', 'l1'],\n",
    "            'loss': ['hinge', 'squared_hinge'],\n",
    "            'C': [1, 10, 100, 1000],\n",
    "            'fit_intercept': [True, False]\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = get_estimators_scores(X_train, y_train, X_test, y_test,\n",
    "                                  models = model_params['models'], \n",
    "                                  params = model_params['gs_params'],\n",
    "                                  cv = skf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "4fb29de2-5b98-474c-a4ad-5170b72b9aea"
   },
   "source": [
    "### Model evaluation:\n",
    "\n",
    "Your boss would rather tell a client incorrectly that they would get a lower salary job than tell a client incorrectly that they would get a high salary job. Adjust one of your models to ease his mind, and explain what it is doing and any tradeoffs.\n",
    "\n",
    "\n",
    "- Use cross-validation to evaluate your models.\n",
    "- Evaluate the accuracy, AUC, precision and recall of the models.\n",
    "- Plot the ROC and precision-recall curves for at least one of your models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, plot_confusion_matrix, \\\n",
    "plot_roc_curve, recall_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#estimators[0].best_estimator_.predict(X_train, y_train)\n",
    "rfc = RandomForestClassifier(n_estimators=100, n_jobs=2, class_weight={0:.6, 1:.4})\n",
    "\n",
    "params = {\n",
    "    'max_depth': [1, 3, None],\n",
    "    'max_features': [None, 1, 3],\n",
    "    'min_samples_split': [2, 3, 50],\n",
    "    'ccp_alpha': [0, 0.001, 0.1]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(rfc, # scoring='precision', \n",
    "                  param_grid=params, \n",
    "                  verbose=1, \n",
    "                  n_jobs=2,\n",
    "                  cv=5)\n",
    "gs.fit(X_train, y_train)\n",
    "print('Accuracy score:', gs.score(X_train, y_train))\n",
    "print('CV score:', gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rather to predict false negative than false positive, that's why class 0 was weighted higher \n",
    "# (and the gridsearch was layed out for precision scoring)\n",
    "plot_confusion_matrix(gs, X_train, y_train, cmap='Blues', labels=[1, 0], values_format='.0f')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(gs, X_test, y_test, cmap='Blues', labels=[1, 0], values_format='.0f')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_at_threshold(x, threshold=0.5):\n",
    "    if x >= threshold:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_train = gs.predict(X_train)\n",
    "predictions_test = gs.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase the threshold for the probability of class 1 to have a higher precision on prediction of class 0\n",
    "Y_prediction = pd.DataFrame(gs.predict_proba(X_train), columns=['high_salary', 'low_salary'])\n",
    "Y_prediction['predict_with_thres'] = Y_prediction['high_salary'].apply(predict_at_threshold, threshold=0.6)\n",
    "Y_prediction['true_value'] = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_prediction.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training accuracy:', accuracy_score(y_train, predictions_train))\n",
    "print('Testing accuracy:', accuracy_score(y_test, predictions_test))\n",
    "\n",
    "print('Training precision:', precision_score(y_train, predictions_train))\n",
    "print('Testing precision:', precision_score(y_test, predictions_test))\n",
    "\n",
    "print('Training recall:', recall_score(y_train, predictions_train))\n",
    "print('Testing recall:', recall_score(y_test, predictions_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score, plot_roc_curve, plot_precision_recall_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the ROC curve you want to gain a really strong slope in the beginning with as less false positive rate as possible. The best score would be having a area under the curve of 1 to have a perfect prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "plot_roc_curve(gs.best_estimator_, X_train, y_train, ax=ax)\n",
    "ax.plot([0, 1], [0, 1], 'k--', linewidth=4)  # baseline\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The precision shows the ratio of how often the positive labelled class was predicted correclty over \n",
    "all predictions which were done for that class. Whereas the recall depicts the ratio of that class over it occurence\n",
    "in the dataset. That means that you can predict less, but more precise to have a high precision score, but then chance\n",
    "increases that the class occurence more often then you actually predict it and then the recall decreases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.fig, ax = plt.subplots(figsize=(6, 6))\n",
    "plot_precision_recall_curve(gs, X_train, y_train, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "4fb29de2-5b98-474c-a4ad-5170b72b9aea"
   },
   "source": [
    "<img src=\"http://imgur.com/xDpSobf.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "### Bonus:\n",
    "\n",
    "- Answer the salary discussion by using your model to explain the tradeoffs between detecting high vs low salary positions. \n",
    "- Discuss the differences and explain when you want a high-recall or a high-precision model in this scenario.\n",
    "- Obtain the ROC/precision-recall curves for the different models you studied (at least the tuned model of each category) and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "focus": false,
    "id": "068dc1cf-7fd7-4f27-a1f1-7f0a5a221d29"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize your results in an executive summary written for a non-technical audience.\n",
    "   \n",
    "- Writeups should be at least 500-1000 words, defining any technical terms, explaining your approach, as well as any risks and limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR TEXT HERE IN MARKDOWN FORMAT "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/xDpSobf.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "### BONUS\n",
    "\n",
    "Convert your executive summary into a public blog post of at least 500 words, in which you document your approach in a tutorial for other aspiring data scientists. Link to this in your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR LINK HERE IN MARKDOWN FORMAT "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
