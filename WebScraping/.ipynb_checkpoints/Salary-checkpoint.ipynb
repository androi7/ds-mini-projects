{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "69b9a648-bcc7-490d-9f9b-ea244d156bd6"
   },
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 15px; height: 80px\">\n",
    "\n",
    "\n",
    "# Web Scraping for Indeed.com and Predicting Salaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Case Overview\n",
    "\n",
    "You're working as a data scientist for a contracting firm that's rapidly expanding. Now that they have their most valuable employee (you!), they need to leverage data to win more contracts. Your firm offers technology and scientific solutions and wants to be competitive in the hiring market. Your principal wants you to\n",
    "\n",
    "   - determine the industry factors that are most important in predicting the salary amounts for these data.\n",
    "\n",
    "To limit the scope, your principal has suggested that you *focus on data-related job postings*, e.g. data scientist, data analyst, research scientist, business intelligence, and any others you might think of. You may also want to decrease the scope by *limiting your search to a single region.*\n",
    "\n",
    "Hint: Aggregators like [Indeed.com](https://www.indeed.com) regularly pool job postings from a variety of markets and industries.\n",
    "\n",
    "**Goal:** Scrape your own data from a job aggregation tool like Indeed.com in order to collect the data to best answer this question.\n",
    "\n",
    "---\n",
    "\n",
    "### Directions\n",
    "\n",
    "In this project you will be leveraging a variety of skills. The first will be to use the web-scraping and/or API techniques you've learned to collect data on data jobs from Indeed.com or another aggregator. Once you have collected and cleaned the data, you will use it to address the question above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factors that impact salary\n",
    "\n",
    "To predict salary the most appropriate approach would be a regression model.\n",
    "Here instead we just want to estimate which factors (like location, job title, job level, industry sector) lead to high or low salary and work with a classification model. To do so, split the salary into two groups of high and low salary, for example by choosing the median salary as a threshold (in principle you could choose any single or multiple splitting points).\n",
    "\n",
    "Use all the skills you have learned so far to build a predictive model.\n",
    "Whatever you decide to use, the most important thing is to justify your choices and interpret your results. *Communication of your process is key.* Note that most listings **DO NOT** come with salary information. You'll need to be able to extrapolate or predict the expected salaries for these listings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "a948d79c-5527-4c0d-ab23-f5d43ce72056"
   },
   "source": [
    "### Scraping job listings from Indeed.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": true,
    "id": "7203e0c9-e437-4802-a6ad-7dc464f94436"
   },
   "source": [
    "We will be scraping job listings from Indeed.com using BeautifulSoup. Luckily, Indeed.com is a simple text page where we can easily find relevant entries.\n",
    "\n",
    "First, look at the source of an Indeed.com page: (http://www.indeed.com/jobs?q=data+scientist+%2420%2C000&l=New+York&start=10\").\n",
    "\n",
    "Notice, each job listing is underneath a `div` tag with a class name of `result`. We can use BeautifulSoup to extract those. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "9732c901-ae26-4160-8376-42e22dd327df"
   },
   "source": [
    "#### Setup a request (using `requests`) to the URL below. Use BeautifulSoup to parse the page and extract all results (HINT: Look for div tags with class name result)\n",
    "\n",
    "The URL here has many query parameters:\n",
    "\n",
    "- `q` for the job search\n",
    "- This is followed by \"+20,000\" to return results with salaries (or expected salaries >$20,000)\n",
    "- `l` for a location \n",
    "- `start` for what result number to start on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "focus": false,
    "id": "e915023e-6b0d-4982-af2a-b1e0355f4927"
   },
   "outputs": [],
   "source": [
    "URL = \"http://www.indeed.com/jobs?q=data+scientist+%2420%2C000&l=New+York&start=10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "focus": false,
    "id": "2efefc73-064a-482d-b3b5-ddf5508cb4ec"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(URL)\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "results = soup.find_all('div', class_='result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"jobsearch-SerpJobCard unifiedRow row result\" data-jk=\"ebdd0b8b53bcbc37\" data-tn-component=\"organicJob\" id=\"p_ebdd0b8b53bcbc37\">\n",
      " <h2 class=\"title\">\n",
      "  <a class=\"jobtitle turnstileLink\" data-tn-element=\"jobTitle\" href=\"/rc/clk?jk=ebdd0b8b53bcbc37&amp;fccid=92b6ad9b1bd886b6&amp;vjs=3\" id=\"jl_ebdd0b8b53bcbc37\" onclick=\"setRefineByCookie(['salest']); return rclk(this,jobmap[0],true,0);\" onmousedown=\"return rclk(this,jobmap[0],0);\" rel=\"noopener nofollow\" target=\"_blank\" title=\"Virtual Data Science Intern, Summer 2021\">\n",
      "   Virtual\n",
      "   <b>\n",
      "    Data\n",
      "   </b>\n",
      "   Science Intern, Summer 2021\n",
      "  </a>\n",
      "  <span class=\"new\">\n",
      "   new\n",
      "  </span>\n",
      " </h2>\n",
      " <div class=\"sjcl\">\n",
      "  <div>\n",
      "   <span class=\"company\">\n",
      "    <a class=\"turnstileLink\" data-tn-element=\"companyName\" href=\"/cmp/Turner\" onmousedown=\"this.href = appendParamsOnce(this.href, 'from=SERP&amp;campaignid=serp-linkcompanyname&amp;fromjk=ebdd0b8b53bcbc37&amp;jcid=92b6ad9b1bd886b6')\" rel=\"noopener\" target=\"_blank\">\n",
      "     Turner\n",
      "    </a>\n",
      "   </span>\n",
      "   <span class=\"ratingsDisplay\">\n",
      "    <a aria-label=\"Company rating 4.0 out of 5 stars\" class=\"ratingNumber\" data-tn-variant=\"cmplinktst2\" href=\"/cmp/Turner/reviews\" onmousedown=\"this.href = appendParamsOnce(this.href, '?campaignid=cmplinktst2&amp;from=SERP&amp;jt=Virtual+Data+Science+Intern%2C+Summer+2021&amp;fromjk=ebdd0b8b53bcbc37&amp;jcid=92b6ad9b1bd886b6');\" rel=\"noopener\" target=\"_blank\" title=\"Turner reviews\">\n",
      "     <span class=\"ratingsContent\">\n",
      "      4.0\n",
      "      <svg class=\"starIcon\" height=\"12px\" role=\"img\" width=\"12px\">\n",
      "       <g>\n",
      "        <path d=\"M 12.00,4.34 C 12.00,4.34 7.69,3.97 7.69,3.97 7.69,3.97 6.00,0.00 6.00,0.00 6.00,0.00 4.31,3.98 4.31,3.98 4.31,3.98 0.00,4.34 0.00,4.34 0.00,4.34 3.28,7.18 3.28,7.18 3.28,7.18 2.29,11.40 2.29,11.40 2.29,11.40 6.00,9.16 6.00,9.16 6.00,9.16 9.71,11.40 9.71,11.40 9.71,11.40 8.73,7.18 8.73,7.18 8.73,7.18 12.00,4.34 12.00,4.34 Z\" style=\"fill: #FFB103\">\n",
      "        </path>\n",
      "       </g>\n",
      "      </svg>\n",
      "     </span>\n",
      "    </a>\n",
      "   </span>\n",
      "  </div>\n",
      "  <div class=\"recJobLoc\" data-rc-loc=\"New York, NY\" id=\"recJobLoc_ebdd0b8b53bcbc37\" style=\"display: none\">\n",
      "  </div>\n",
      "  <span class=\"location accessible-contrast-color-location\">\n",
      "   New York, NY\n",
      "  </span>\n",
      "  <span class=\"remote-bullet\">\n",
      "   •\n",
      "  </span>\n",
      "  <span class=\"remote\">\n",
      "   Remote\n",
      "  </span>\n",
      " </div>\n",
      " <div class=\"summary\">\n",
      "  <ul style=\"list-style-type:circle;margin-top: 0px;margin-bottom: 0px;padding-left:20px;\">\n",
      "   <li style=\"margin-bottom:0px;\">\n",
      "    Develop video labeling models to identify objects, individuals and scene breaks of interest.\n",
      "   </li>\n",
      "   <li>\n",
      "    Work with human annotators to develop a rigorous precision measure…\n",
      "   </li>\n",
      "  </ul>\n",
      " </div>\n",
      " <div class=\"jobsearch-SerpJobCard-footer\">\n",
      "  <div class=\"jobsearch-SerpJobCard-footerActions\">\n",
      "   <div class=\"result-link-bar-container\">\n",
      "    <div class=\"result-link-bar\">\n",
      "     <span class=\"date\">\n",
      "      5 days ago\n",
      "     </span>\n",
      "     <div class=\"tt_set\" id=\"tt_set_0\">\n",
      "      <div class=\"job-reaction\">\n",
      "       <button aria-expanded=\"false\" aria-haspopup=\"true\" aria-label=\"save or dislike\" class=\"job-reaction-kebab\" data-ol-has-click-handler=\"\" onclick=\"toggleKebabMenu('ebdd0b8b53bcbc37', false, event); return false;\" tabindex=\"0\">\n",
      "       </button>\n",
      "       <span class=\"job-reaction-kebab-menu\">\n",
      "        <button class=\"job-reaction-kebab-item job-reaction-save\" data-ol-has-click-handler=\"\" onclick=\"changeJobState('ebdd0b8b53bcbc37', 'save', 'linkbar', false, '');return false;\">\n",
      "         <svg focusable=\"false\" height=\"16\" viewbox=\"0 0 24 24\" width=\"16\">\n",
      "          <g>\n",
      "           <path d=\"M16.5,3A6,6,0,0,0,12,5.09,6,6,0,0,0,7.5,3,5.45,5.45,0,0,0,2,8.5C2,12.28,5.4,15.36,10.55,20L12,21.35,13.45,20C18.6,15.36,22,12.28,22,8.5A5.45,5.45,0,0,0,16.5,3ZM12.1,18.55l-0.1.1-0.1-.1C7.14,14.24,4,11.39,4,8.5A3.42,3.42,0,0,1,7.5,5a3.91,3.91,0,0,1,3.57,2.36h1.87A3.88,3.88,0,0,1,16.5,5,3.42,3.42,0,0,1,20,8.5C20,11.39,16.86,14.24,12.1,18.55Z\" fill=\"#2d2d2d\">\n",
      "           </path>\n",
      "          </g>\n",
      "         </svg>\n",
      "         <span class=\"job-reaction-kebab-item-text\">\n",
      "          Save job\n",
      "         </span>\n",
      "        </button>\n",
      "        <button class=\"job-reaction-kebab-item job-reaction-dislike\" data-ol-has-click-handler=\"\" onclick=\"dislikeJob(false, false, 'ebdd0b8b53bcbc37', 'unsave', 'linkbar', false, '');\">\n",
      "         <span class=\"job-reaction-dislike-icon\">\n",
      "         </span>\n",
      "         <span class=\"job-reaction-kebab-item-text\">\n",
      "          Not interested\n",
      "         </span>\n",
      "        </button>\n",
      "        <button class=\"job-reaction-kebab-item job-reaction-report\" onclick=\"reportJob('ebdd0b8b53bcbc37');\">\n",
      "         <span class=\"job-reaction-report-icon\">\n",
      "         </span>\n",
      "         <span class=\"job-reaction-kebab-item-text\">\n",
      "          Report Job\n",
      "         </span>\n",
      "        </button>\n",
      "       </span>\n",
      "      </div>\n",
      "      <span class=\"result-link-bar-separator\">\n",
      "       ·\n",
      "      </span>\n",
      "      <a class=\"sl resultLink save-job-link\" href=\"#\" id=\"sj_ebdd0b8b53bcbc37\" onclick=\"changeJobState('ebdd0b8b53bcbc37', 'save', 'linkbar', false, ''); return false;\" title=\"Save this job to my.indeed\">\n",
      "       Save job\n",
      "      </a>\n",
      "      <span class=\"result-link-bar-separator\">\n",
      "       ·\n",
      "      </span>\n",
      "      <button aria-expanded=\"false\" class=\"sl resultLink more-link\" id=\"tog_0\" onclick=\"toggleMoreLinks('ebdd0b8b53bcbc37', '0'); return false;\">\n",
      "       More...\n",
      "      </button>\n",
      "     </div>\n",
      "     <script>\n",
      "      if (!window['result_ebdd0b8b53bcbc37']) {window['result_ebdd0b8b53bcbc37'] = {};}window['result_ebdd0b8b53bcbc37']['showSource'] = false; window['result_ebdd0b8b53bcbc37']['source'] = \"Time Warner\"; window['result_ebdd0b8b53bcbc37']['loggedIn'] = false; window['result_ebdd0b8b53bcbc37']['showMyJobsLinks'] = false;window['result_ebdd0b8b53bcbc37']['baseMyJobsUrl'] = \"https://myjobs.indeed.com\";window['result_ebdd0b8b53bcbc37']['undoAction'] = \"unsave\";window['result_ebdd0b8b53bcbc37']['relativeJobAge'] = \"5 days ago\";window['result_ebdd0b8b53bcbc37']['jobKey'] = \"ebdd0b8b53bcbc37\"; window['result_ebdd0b8b53bcbc37']['myIndeedAvailable'] = true; window['result_ebdd0b8b53bcbc37']['showMoreActionsLink'] = window['result_ebdd0b8b53bcbc37']['showMoreActionsLink'] || true; window['result_ebdd0b8b53bcbc37']['resultNumber'] = 0; window['result_ebdd0b8b53bcbc37']['jobStateChangedToSaved'] = false; window['result_ebdd0b8b53bcbc37']['searchState'] = \"q=data scientist $20,000&amp;l=New+York&amp;start=10\"; window['result_ebdd0b8b53bcbc37']['basicPermaLink'] = \"https://www.indeed.com\"; window['result_ebdd0b8b53bcbc37']['saveJobFailed'] = false; window['result_ebdd0b8b53bcbc37']['removeJobFailed'] = false; window['result_ebdd0b8b53bcbc37']['requestPending'] = false; window['result_ebdd0b8b53bcbc37']['currentPage'] = \"serp\"; window['result_ebdd0b8b53bcbc37']['sponsored'] = false;window['result_ebdd0b8b53bcbc37']['reportJobButtonEnabled'] = false; window['result_ebdd0b8b53bcbc37']['showMyJobsHired'] = false; window['result_ebdd0b8b53bcbc37']['showSaveForSponsored'] = false; window['result_ebdd0b8b53bcbc37']['showJobAge'] = true; window['result_ebdd0b8b53bcbc37']['showHolisticCard'] = true; window['result_ebdd0b8b53bcbc37']['showDislike'] = true; window['result_ebdd0b8b53bcbc37']['showKebab'] = true; window['result_ebdd0b8b53bcbc37']['showReport'] = true;\n",
      "     </script>\n",
      "    </div>\n",
      "   </div>\n",
      "  </div>\n",
      " </div>\n",
      " <div class=\"tab-container\">\n",
      "  <div class=\"more-links-container result-tab\" id=\"tt_display_0\" style=\"display:none;\">\n",
      "   <div class=\"more_actions\" id=\"more_0\">\n",
      "    <ul>\n",
      "     <li>\n",
      "      <span class=\"mat\">\n",
      "       View all\n",
      "       <a href=\"/q-Turner-l-New-York,-NY-jobs.html\">\n",
      "        Turner jobs in New York, NY\n",
      "       </a>\n",
      "       -\n",
      "       <a href=\"/l-New-York,-NY-jobs.html\">\n",
      "        New York jobs\n",
      "       </a>\n",
      "      </span>\n",
      "     </li>\n",
      "     <li>\n",
      "      <span class=\"mat\">\n",
      "       Salary Search:\n",
      "       <a href=\"/salaries/data-science-intern-Salaries,-New-York-NY\" onmousedown=\"this.href = appendParamsOnce(this.href, '?campaignid=serp-more&amp;fromjk=ebdd0b8b53bcbc37&amp;from=serp-more');\">\n",
      "        Data Science Intern salaries in New York, NY\n",
      "       </a>\n",
      "      </span>\n",
      "     </li>\n",
      "     <li>\n",
      "      <span class=\"mat\">\n",
      "       Learn more about working at\n",
      "       <a href=\"/cmp/Turner\" onmousedown=\"this.href = appendParamsOnce(this.href, '?fromjk=ebdd0b8b53bcbc37&amp;from=serp-more&amp;campaignid=serp-more&amp;jcid=92b6ad9b1bd886b6');\">\n",
      "        Turner\n",
      "       </a>\n",
      "      </span>\n",
      "     </li>\n",
      "     <li>\n",
      "      <span class=\"mat\">\n",
      "       See popular\n",
      "       <a href=\"/cmp/Turner/faq\" onmousedown=\"this.href = appendParamsOnce(this.href, '?from=serp-more&amp;campaignid=serp-more&amp;fromjk=ebdd0b8b53bcbc37&amp;jcid=92b6ad9b1bd886b6');\">\n",
      "        questions &amp; answers about Turner\n",
      "       </a>\n",
      "      </span>\n",
      "     </li>\n",
      "    </ul>\n",
      "   </div>\n",
      "   <a class=\"close-link closeLink\" href=\"#\" onclick=\"toggleMoreLinks('ebdd0b8b53bcbc37'); return false;\" title=\"Close\">\n",
      "   </a>\n",
      "  </div>\n",
      "  <div class=\"dya-container result-tab\">\n",
      "  </div>\n",
      "  <div class=\"tellafriend-container result-tab email_job_content\">\n",
      "  </div>\n",
      "  <div class=\"sign-in-container result-tab\">\n",
      "  </div>\n",
      " </div>\n",
      "</div>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(results[0].prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "bb0b866a-26a7-45e9-8084-5a0f90eb4b3e"
   },
   "source": [
    "Let's look at one result more closely. A single `result` looks like\n",
    "\n",
    "```\n",
    "<div class=\" row result\" data-jk=\"2480d203f7e97210\" data-tn-component=\"organicJob\" id=\"p_2480d203f7e97210\" itemscope=\"\" itemtype=\"http://schema.org/JobPosting\">\n",
    "<h2 class=\"jobtitle\" id=\"jl_2480d203f7e97210\">\n",
    "<a class=\"turnstileLink\" data-tn-element=\"jobTitle\" onmousedown=\"return rclk(this,jobmap[0],1);\" rel=\"nofollow\" target=\"_blank\" title=\"AVP/Quantitative Analyst\">AVP/Quantitative Analyst</a>\n",
    "</h2>\n",
    "<span class=\"company\" itemprop=\"hiringOrganization\" itemtype=\"http://schema.org/Organization\">\n",
    "<span itemprop=\"name\">\n",
    "<a href=\"/cmp/Alliancebernstein?from=SERP&amp;campaignid=serp-linkcompanyname&amp;fromjk=2480d203f7e97210&amp;jcid=b374f2a780e04789\" target=\"_blank\">\n",
    "    AllianceBernstein</a></span>\n",
    "</span>\n",
    "<tr>\n",
    "<td class=\"snip\">\n",
    "<nobr>$117,500 - $127,500 a year</nobr>\n",
    "<div>\n",
    "<span class=\"summary\" itemprop=\"description\">\n",
    "C onduct quantitative and statistical research as well as portfolio management for various investment portfolios. Collaborate with Quantitative Analysts and</span>\n",
    "</div>\n",
    "</div>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "</div>\n",
    "```\n",
    "\n",
    "While this has some more verbose elements removed, we can see that there is some structure to the above:\n",
    "- The salary is in a `span` with `class='salaryText'`.\n",
    "- The title of a job is in a link with class set to `jobtitle` and a `data-tn-element='jobTitle'`.  \n",
    "- The location is set in a `span` with `class='location'`. \n",
    "- The company is set in a `span` with `class='company'`. \n",
    "- Decide which other components could be relevant, for example the region or the summary of the job advert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = {\n",
    "    'salary': [],\n",
    "    'job_title': [],\n",
    "    'location': [],\n",
    "    'company': [],\n",
    "    'company_rating': [],\n",
    "    'description': []\n",
    "}\n",
    "\n",
    "for job in results:\n",
    "    try:\n",
    "        jobs['salary'].append(job.find('span', class_='salaryText').text)\n",
    "    except:\n",
    "        jobs['salary'].append(np.nan)\n",
    "        \n",
    "    try:\n",
    "        # job.find('h2', class_='title').find('a')['title']\n",
    "        jobs['job_title'].append(job.find('a', attrs={'data-tn-element': 'jobTitle'})['title'])\n",
    "    except:\n",
    "        jobs['job_title'].append(np.nan)\n",
    "        \n",
    "    try:\n",
    "        jobs['location'].append(job.find('span', class_='location').text.strip())  \n",
    "    except:\n",
    "        jobs['location'].append(np.nan)\n",
    "        \n",
    "    try:\n",
    "        company_name = job.find('a', attrs={'data-tn-element':'companyName'})\n",
    "        if company_name:\n",
    "            jobs['company'].append(company_name.text.strip())\n",
    "        else:\n",
    "            jobs['company'].append(job.find('span', class_='company').text.strip())\n",
    "    except:\n",
    "        jobs['company'].append(np.nan)\n",
    "        \n",
    "    try:\n",
    "        jobs['company_rating'].append(job.find('span', class_='ratingsContent').text.strip())\n",
    "    except:\n",
    "        jobs['company_rating'].append(np.nan)\n",
    "        \n",
    "    try:\n",
    "        lists = job.find('div', class_='summary').find_all('li')   \n",
    "        jobs['description'].append((' ').join([lst.text for lst in lists]))\n",
    "    except:\n",
    "        jobs['description'].append(np.nan)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'company': ['Turner',\n",
      "             'AT&T',\n",
      "             'Naval Nuclear Laboratory',\n",
      "             'AT&T',\n",
      "             'Atlassian',\n",
      "             'Spotify',\n",
      "             'QuaEra',\n",
      "             'Vee',\n",
      "             'White Plains Hospital',\n",
      "             'Rhino',\n",
      "             'MassMutual',\n",
      "             'Olo',\n",
      "             'Jefferies & Company, Inc.',\n",
      "             'New York University',\n",
      "             'Neuberger Berman'],\n",
      " 'company_rating': ['4.0',\n",
      "                    '3.7',\n",
      "                    '2.8',\n",
      "                    '3.7',\n",
      "                    '4.6',\n",
      "                    '4.3',\n",
      "                    nan,\n",
      "                    '3.9',\n",
      "                    '4.0',\n",
      "                    nan,\n",
      "                    '3.7',\n",
      "                    '2.6',\n",
      "                    '3.7',\n",
      "                    '4.2',\n",
      "                    '3.9'],\n",
      " 'description': ['Develop video labeling models to identify objects, '\n",
      "                 'individuals and scene breaks of interest. Work with human '\n",
      "                 'annotators to develop a rigorous precision measure…',\n",
      "                 'This program will have our interns work side-by-side, '\n",
      "                 'virtually this year, with the team’s data scientists and '\n",
      "                 'analysts while we create an impact on our…',\n",
      "                 'Processing, refining, and verifying the integrity of data '\n",
      "                 'used for analysis. Experience with data visualization '\n",
      "                 'packages, such as Tableau, SAS, and Rstudio.',\n",
      "                 'Hands-on attitude toward problem-solving, including a '\n",
      "                 'willingness to dig into terabytes of data and quickly '\n",
      "                 'construct tools or models.',\n",
      "                 'As the ideal data scientist, you are someone who can both do '\n",
      "                 'a technical dive and communicate effectively with others.',\n",
      "                 'You have experience working with financial data and/or '\n",
      "                 'performing data reconciliations and quality assurance '\n",
      "                 'testing.',\n",
      "                 'Experience with R, Python, PySpark and data visualization '\n",
      "                 'software such as Tableau. Ability to efficiently construct '\n",
      "                 'data sets from large scale distributed…',\n",
      "                 \"You will help build Vee's data capability and data strategy. \"\n",
      "                 'Solid background in data mining and statistical analysis.',\n",
      "                 'Expert technical knowledge in data warehousing, advanced '\n",
      "                 'analytics, and data visualization. Integrate data from '\n",
      "                 'disparate sources.',\n",
      "                 \"Actively contribute to Rhino's Data culture by building out \"\n",
      "                 'core data models, tooling and best practices as well as '\n",
      "                 'training other Rhinos on using our data…',\n",
      "                 '3+ years working with data and relevant computational '\n",
      "                 'frameworks and systemsÂ. Assemble data sets from disparate '\n",
      "                 'sources and analyze using appropriate…',\n",
      "                 'Handling data responsibly by maintaining data governance, '\n",
      "                 'privacy, and documentation. Hands on data extraction, data '\n",
      "                 'analysis, data cleaning, preparation,…',\n",
      "                 '4+ years of experience within consulting, data research, '\n",
      "                 'data science or similar roles. Keen interest in the “data '\n",
      "                 'economy” within financial services.',\n",
      "                 \"Completion of Master's degree in disciplines aligned with \"\n",
      "                 'CDS faculty research by the start date.',\n",
      "                 'Identify, develop and apply statistical and machine learning '\n",
      "                 'techniques to large data sets for data pre-processing, '\n",
      "                 'feature engineering, patten recognition and…'],\n",
      " 'job_title': ['Virtual Data Science Intern, Summer 2021',\n",
      "               'Virtual Data Science Intern, Summer 2021',\n",
      "               'Data Scientist',\n",
      "               'Data Scientist',\n",
      "               'Data Scientist, Enterprise, Trello (US Remote)',\n",
      "               'Data Scientist - Publishing Royalties',\n",
      "               'Data Scientist',\n",
      "               'NLP Data Scientist Internship',\n",
      "               'Data Scientist',\n",
      "               'Data Scientist',\n",
      "               'Data Scientist',\n",
      "               'Data Scientist',\n",
      "               'Data Strategy Data Scientist',\n",
      "               'Research Scientist-Center for Data Science',\n",
      "               'Data Scientist'],\n",
      " 'location': ['New York, NY',\n",
      "              'New York, NY',\n",
      "              'Niskayuna, NY 12309',\n",
      "              'New York, NY',\n",
      "              'New York, NY',\n",
      "              'New York, NY',\n",
      "              'New York, NY 10018 (Garment District area)',\n",
      "              'New York State',\n",
      "              'White Plains, NY 10601',\n",
      "              'New York State',\n",
      "              'New York, NY',\n",
      "              'New York, NY 10012 (NoHo area)',\n",
      "              'New York, NY 10022 (Midtown area)',\n",
      "              'New York, NY 10012 (Greenwich Village area)',\n",
      "              'New York, NY'],\n",
      " 'salary': [nan,\n",
      "            nan,\n",
      "            nan,\n",
      "            nan,\n",
      "            nan,\n",
      "            nan,\n",
      "            nan,\n",
      "            nan,\n",
      "            nan,\n",
      "            nan,\n",
      "            nan,\n",
      "            nan,\n",
      "            nan,\n",
      "            nan,\n",
      "            nan]}\n"
     ]
    }
   ],
   "source": [
    "pprint(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>salary</th>\n",
       "      <th>job_title</th>\n",
       "      <th>location</th>\n",
       "      <th>company</th>\n",
       "      <th>company_rating</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Virtual Data Science Intern, Summer 2021</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>Turner</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Develop video labeling models to identify obje...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Virtual Data Science Intern, Summer 2021</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>AT&amp;T</td>\n",
       "      <td>3.7</td>\n",
       "      <td>This program will have our interns work side-b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Niskayuna, NY 12309</td>\n",
       "      <td>Naval Nuclear Laboratory</td>\n",
       "      <td>2.8</td>\n",
       "      <td>Processing, refining, and verifying the integr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>AT&amp;T</td>\n",
       "      <td>3.7</td>\n",
       "      <td>Hands-on attitude toward problem-solving, incl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Data Scientist, Enterprise, Trello (US Remote)</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>Atlassian</td>\n",
       "      <td>4.6</td>\n",
       "      <td>As the ideal data scientist, you are someone w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   salary                                       job_title  \\\n",
       "0     NaN        Virtual Data Science Intern, Summer 2021   \n",
       "1     NaN        Virtual Data Science Intern, Summer 2021   \n",
       "2     NaN                                  Data Scientist   \n",
       "3     NaN                                  Data Scientist   \n",
       "4     NaN  Data Scientist, Enterprise, Trello (US Remote)   \n",
       "\n",
       "              location                   company company_rating  \\\n",
       "0         New York, NY                    Turner            4.0   \n",
       "1         New York, NY                      AT&T            3.7   \n",
       "2  Niskayuna, NY 12309  Naval Nuclear Laboratory            2.8   \n",
       "3         New York, NY                      AT&T            3.7   \n",
       "4         New York, NY                 Atlassian            4.6   \n",
       "\n",
       "                                         description  \n",
       "0  Develop video labeling models to identify obje...  \n",
       "1  This program will have our interns work side-b...  \n",
       "2  Processing, refining, and verifying the integr...  \n",
       "3  Hands-on attitude toward problem-solving, incl...  \n",
       "4  As the ideal data scientist, you are someone w...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(jobs).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "f1eddb90-4ba8-483c-a229-77e93aa53119"
   },
   "source": [
    "### Write 4 functions to extract each item: location, company, job, and salary.\n",
    "\n",
    "Example: \n",
    "```python\n",
    "def extract_location_from_result(result):\n",
    "    return result.find ...\n",
    "```\n",
    "\n",
    "\n",
    "- **Make sure these functions are robust and can handle cases where the data/field may not be available.**\n",
    "    - Remember to check if a field is empty or `None` for attempting to call methods on it.\n",
    "    - Remember to use `try/except` if you anticipate errors.\n",
    "- **Test** the functions on the results above and simple examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def not_null(content):\n",
    "    if content not in ('', None):\n",
    "        return content\n",
    "    else:\n",
    "        raise Exception('No content found!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "focus": false,
    "id": "a1af53c9-9090-494f-b82e-cadb60a54909"
   },
   "outputs": [],
   "source": [
    "def extract_location_from_result(result):\n",
    "    try:\n",
    "        data = result.find('span', class_='location').text.strip()\n",
    "        return not_null(data)\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_company_from_result(result):\n",
    "    try:\n",
    "        company_name = result.find('a', attrs={'data-tn-element':'companyName'})\n",
    "        if company_name:\n",
    "            return not_null(company_name.text.strip())\n",
    "        else:\n",
    "            data = result.find('span', class_='company').text.strip()\n",
    "            return not_null(data)\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_job_from_result(result):\n",
    "    try:\n",
    "        return not_null(result.find('a', attrs={'data-tn-element': 'jobTitle'})['title'])\n",
    "    except:\n",
    "        return np.nan\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_salary_from_result(result):\n",
    "    try:\n",
    "        return not_null(result.find('span', class_='salaryText').text.strip())\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rating_from_result(result): \n",
    "    try:\n",
    "        return not_null(result.find('span', class_='ratingsContent').text.strip())\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_summary_from_result(result):\n",
    "    try:\n",
    "        lists = result.find('div', class_='summary').find_all('li')   \n",
    "        return not_null((' ').join([lst.text for lst in lists]))\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(URL)\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "results = soup.find_all('div', class_='result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>location</th>\n",
       "      <th>company</th>\n",
       "      <th>salary</th>\n",
       "      <th>summary</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Healthcare Data Scientist</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>pulseData</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Works with the data science and engineering te...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>New York State</td>\n",
       "      <td>Unite Us</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Experience with healthcare data and analytics ...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Associate Data Scientist</td>\n",
       "      <td>Buffalo, NY 14213 (Grant Ferry area)</td>\n",
       "      <td>Rich Products Corporation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The role may also support the related roles of...</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ESG Data Scientist</td>\n",
       "      <td>New York, NY 10007 (Financial District area)</td>\n",
       "      <td>MSCI Inc.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Use statistical methods and programing to anal...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist, Analytics &amp; Inference - New Yo...</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>Codecademy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Work with our data science and engineering tea...</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Virtual Data Science Intern, Summer 2021</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>Turner</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Develop video labeling models to identify obje...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Virtual Data Science Intern, Summer 2021</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>AT&amp;T</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This program will have our interns work side-b...</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Niskayuna, NY 12309</td>\n",
       "      <td>Naval Nuclear Laboratory</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Processing, refining, and verifying the integr...</td>\n",
       "      <td>2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>AT&amp;T</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hands-on attitude toward problem-solving, incl...</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Scientist, Enterprise, Trello (US Remote)</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>Atlassian</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As the ideal data scientist, you are someone w...</td>\n",
       "      <td>4.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           job_title  \\\n",
       "0                          Healthcare Data Scientist   \n",
       "1                                     Data Scientist   \n",
       "2                           Associate Data Scientist   \n",
       "3                                 ESG Data Scientist   \n",
       "4  Data Scientist, Analytics & Inference - New Yo...   \n",
       "5           Virtual Data Science Intern, Summer 2021   \n",
       "6           Virtual Data Science Intern, Summer 2021   \n",
       "7                                     Data Scientist   \n",
       "8                                     Data Scientist   \n",
       "9     Data Scientist, Enterprise, Trello (US Remote)   \n",
       "\n",
       "                                       location                    company  \\\n",
       "0                                  New York, NY                  pulseData   \n",
       "1                                New York State                   Unite Us   \n",
       "2          Buffalo, NY 14213 (Grant Ferry area)  Rich Products Corporation   \n",
       "3  New York, NY 10007 (Financial District area)                  MSCI Inc.   \n",
       "4                                  New York, NY                 Codecademy   \n",
       "5                                  New York, NY                     Turner   \n",
       "6                                  New York, NY                       AT&T   \n",
       "7                           Niskayuna, NY 12309   Naval Nuclear Laboratory   \n",
       "8                                  New York, NY                       AT&T   \n",
       "9                                  New York, NY                  Atlassian   \n",
       "\n",
       "   salary                                            summary rating  \n",
       "0     NaN  Works with the data science and engineering te...    NaN  \n",
       "1     NaN  Experience with healthcare data and analytics ...    5.0  \n",
       "2     NaN  The role may also support the related roles of...    3.6  \n",
       "3     NaN  Use statistical methods and programing to anal...    4.0  \n",
       "4     NaN  Work with our data science and engineering tea...    4.2  \n",
       "5     NaN  Develop video labeling models to identify obje...    4.0  \n",
       "6     NaN  This program will have our interns work side-b...    3.7  \n",
       "7     NaN  Processing, refining, and verifying the integr...    2.8  \n",
       "8     NaN  Hands-on attitude toward problem-solving, incl...    3.7  \n",
       "9     NaN  As the ideal data scientist, you are someone w...    4.6  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salary = []\n",
    "job_title = [] \n",
    "location = []\n",
    "company = []\n",
    "summary = []\n",
    "rating = []\n",
    "\n",
    "for job in results:\n",
    "    salary.append(extract_salary_from_result(job))\n",
    "    job_title.append(extract_job_from_result(job))\n",
    "    location.append(extract_location_from_result(job))\n",
    "    company.append(extract_company_from_result(job))\n",
    "    rating.append(extract_rating_from_result(job))\n",
    "    summary.append(extract_summary_from_result(job))\n",
    "    \n",
    "\n",
    "job_market = pd.DataFrame({'job_title': job_title,\n",
    "                           'location': location,\n",
    "                           'company': company,\n",
    "                           'salary': salary,\n",
    "                           'summary': summary,\n",
    "                           'rating': rating})\n",
    "\n",
    "job_market.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "34070e89-9521-4b45-90c8-57a6599aac68"
   },
   "source": [
    "Now, to scale up our scraping, we need to accumulate more results. We can do this by examining the URL above.\n",
    "\n",
    "- \"http://www.indeed.com/jobs?q=data+scientist+%2420%2C000&l=New+York&start=10\"\n",
    "\n",
    "There are two query parameters here we can alter to collect more results, the `l=New+York` and the `start=10`. The first controls the location of the results (so we can try a different city). The second controls where in the results to start and gives 10 results (thus, we can keep incrementing by 10 to go further in the list)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://www.indeed.com/jobs?q=data+scientist+%2420%2C000&l=New+York&start=10%22\n",
    "\n",
    "titles = ['data scientist', \n",
    "          'data analyst',\n",
    "          'research scientist',\n",
    "          'business intelligence',\n",
    "          'database developer',\n",
    "          'data engineer',\n",
    "          'database administrator']\n",
    "\n",
    "cities = ['New York',\n",
    "          'Dallas',\n",
    "          'Boston',\n",
    "          'Houston',\n",
    "          'San Francisco',\n",
    "          'Seattle',\n",
    "          'Austin',\n",
    "          'Miami',\n",
    "          'New Orleans',\n",
    "          'Atlanta',\n",
    "          'Jacksonville',\n",
    "          'Chicago',\n",
    "          'Philadelphia',\n",
    "          'Las Vegas',\n",
    "          'Los Angeles',\n",
    "          'Phoenix']\n",
    "\n",
    "titles_encoded = ('%2C+').join([t.replace(' ', '+') for t in set(titles)])\n",
    "\n",
    "# Ascii Encoding Reference:\n",
    "# space -> %20\n",
    "# # -> %23\n",
    "# $ -> %24\n",
    "# % -> %25\n",
    "# & -> %26\n",
    "# , -> %2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.scrapehero.com/how-to-fake-and-rotate-user-agents-using-python-3/\n",
    "\n",
    "user_agent_list = [\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1.1 Safari/605.1.15',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:77.0) Gecko/20100101 Firefox/77.0',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:77.0) Gecko/20100101 Firefox/77.0',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36',\n",
    "]\n",
    "\n",
    "headers = {\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\", \n",
    "    \"Accept-Encoding\": \"gzip, deflate, sdch\", \n",
    "    \"Accept-Language\": \"en-GB,en-US;q=0.9,en;q=0.8\", \n",
    "    \"Dnt\": \"1\", # do not track (1... prefers not to be tracked)\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Upgrade-Insecure-Requests\": \"1\",\n",
    "#     \"Host\": \"indeed.com\",\n",
    "#     \"Cache-Control\": \"max-age=0\",\n",
    "  }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.scrapehero.com/how-to-rotate-proxies-and-ip-addresses-using-python-3/\n",
    "# xpath expressions:\n",
    "# / -> selects from the root node (if occurring after a node then referring to children node)\n",
    "# // -> selects nodes from the current node\n",
    "# . -> selects current node\n",
    "\n",
    "from lxml.html import fromstring\n",
    "def get_proxies():\n",
    "#     url = 'https://free-proxy-list.net/'\n",
    "    url = 'https://www.us-proxy.org/'\n",
    "    response = requests.get(url)\n",
    "    # fromstring() returns document_fromstring or fragment_fromstring\n",
    "    parser = fromstring(response.text)\n",
    "    proxies = set()\n",
    "    for i in parser.xpath('//tbody/tr')[:20]:\n",
    "        if i.xpath('.//td[7][contains(text(),\"yes\")]'):  # <td class='hx'>yes</td>\n",
    "            #Grabbing IP and corresponding PORT\n",
    "            # i.xpath('.//td[1]')[0].text\n",
    "            proxy = \":\".join([i.xpath('.//td[1]/text()')[0], i.xpath('.//td[2]/text()')[0]])\n",
    "            proxies.add(proxy)\n",
    "    return proxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_indeed(cities, titles_encoded, headers=None, user_agent_list=None, delay=False):\n",
    "\n",
    "    salary = []\n",
    "    job_title = [] \n",
    "    location = []\n",
    "    company = []\n",
    "    rating = []\n",
    "    summary = []\n",
    "    url_template = \"http://www.indeed.com/jobs?q={job}+%2420%2C000&l={location}&start={page}\"\n",
    "    \n",
    "    if type(cities) != list:\n",
    "        raise Exception('cities must be a list')\n",
    "        return\n",
    "\n",
    "#     from itertools import cycle\n",
    "#     proxies = get_proxies()\n",
    "#     proxy_pool = cycle(proxies)\n",
    "\n",
    "    for c in tqdm(set(cities)):\n",
    "        if user_agent_list:\n",
    "            user_agent = random.choice(user_agent_list)\n",
    "            headers['User-Agent'] = user_agent\n",
    "#         proxy = next(proxy_pool)\n",
    "#         proxy_flag = False\n",
    "\n",
    "#         while not proxy_flag:\n",
    "#             try:\n",
    "#                 response = requests.get('http://www.indeed.com', proxies={'http': proxy, 'https': proxy})\n",
    "#                 print(f\"Request {c}\")\n",
    "#                 print(response.json())\n",
    "#                 proxy_flag = True\n",
    "#             except:\n",
    "#                 proxy = next(proxy_pool)\n",
    "#                 print(\"Skipping. Connnection error\")\n",
    "\n",
    "        for page in tqdm(range(10, 800, 10), position=0):\n",
    "            try:\n",
    "                r = requests.get(url_template.format(job=titles_encoded,\n",
    "                                                     location=c.replace(' ', '+'),\n",
    "                                                     page=page),\n",
    "                                 headers=headers)\n",
    "#                                  proxies={\"http\": proxy, \"https\": proxy})\n",
    "            except:\n",
    "                raise Exception('check headers')\n",
    "\n",
    "            soup = BeautifulSoup(r.text, 'html.parser')\n",
    "            results = soup.find_all('div', class_='result')\n",
    "\n",
    "            # when the last result is shown a message box appears which includes \n",
    "            # a paragraph with a class attribute called 'dupetext'\n",
    "            duplicate_msg = soup.find('p', class_='dupetext')\n",
    "\n",
    "            for job in results:\n",
    "                salary.append(extract_salary_from_result(job))\n",
    "                job_title.append(extract_job_from_result(job))\n",
    "                location.append(extract_location_from_result(job))\n",
    "                company.append(extract_company_from_result(job))\n",
    "                rating.append(extract_rating_from_result(job))\n",
    "                summary.append(extract_summary_from_result(job))\n",
    "\n",
    "            if not results or duplicate_msg:  # nothing found or end result is reached\n",
    "                if delay:\n",
    "                    time.sleep(15 + random.random() * 10)  # 15-25 sec, prevent to submit captchas\n",
    "                print(f'{c} with {page/10} pages.')\n",
    "                break\n",
    "            elif page % 80 == 0:\n",
    "                if delay:\n",
    "                    time.sleep(10 + random.random() * 5)  # 10-15 sec\n",
    "            else:\n",
    "#                 print(f'city: {c}, page: {page}')\n",
    "                if delay:\n",
    "                    time.sleep(1 + random.random() * 3)  # 1-4 sec\n",
    "                \n",
    "    df = pd.DataFrame({'job_title': job_title,\n",
    "                       'location': location,\n",
    "                       'company': company,\n",
    "                       'salary': salary,\n",
    "                       'rating': rating,\n",
    "                       'summary': summary})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_file_exists(filename, cities, titles_encoded, headers=None, user_agent_list=None, delay=False):\n",
    "    file = Path(f\"./datasets/{filename}.csv\")\n",
    "    if not file.exists():\n",
    "        df = scrape_indeed(cities=cities, titles_encoded=titles_encoded, \n",
    "                           headers=headers, user_agent_list=user_agent_list, \n",
    "                           delay=delay)\n",
    "        df.to_csv(f'./datasets/{filename}.csv', index=False)\n",
    "    else:\n",
    "        print('file already exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file already exists\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>location</th>\n",
       "      <th>company</th>\n",
       "      <th>salary</th>\n",
       "      <th>rating</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Syracuse, NY</td>\n",
       "      <td>Excelacom</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.2</td>\n",
       "      <td>Bachelor’s Degree or higher. Responds to data ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Analyst/Assistant Research Scientist (par...</td>\n",
       "      <td>New York, NY 10012 (Greenwich Village area)</td>\n",
       "      <td>New York University</td>\n",
       "      <td>$18 an hour</td>\n",
       "      <td>4.2</td>\n",
       "      <td>Cultivate deep familiarity with methodological...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Business Intelligence Analyst</td>\n",
       "      <td>New York, NY 10176 (Murray Hill area)</td>\n",
       "      <td>CBS Interactive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.5</td>\n",
       "      <td>As a business intelligence analyst, you’ll lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Business Data Analyst</td>\n",
       "      <td>Melville, NY 11747</td>\n",
       "      <td>North American Partners in Anesthesia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1+ year of hands-on experience with MS SQL as ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>Northwestern Mutual</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.8</td>\n",
       "      <td>This team focuses on developing data science m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           job_title  \\\n",
       "0                                       Data Analyst   \n",
       "1  Data Analyst/Assistant Research Scientist (par...   \n",
       "2                      Business Intelligence Analyst   \n",
       "3                              Business Data Analyst   \n",
       "4                                     Data Scientist   \n",
       "\n",
       "                                      location  \\\n",
       "0                                 Syracuse, NY   \n",
       "1  New York, NY 10012 (Greenwich Village area)   \n",
       "2        New York, NY 10176 (Murray Hill area)   \n",
       "3                           Melville, NY 11747   \n",
       "4                                 New York, NY   \n",
       "\n",
       "                                 company       salary  rating  \\\n",
       "0                              Excelacom          NaN     3.2   \n",
       "1                    New York University  $18 an hour     4.2   \n",
       "2                        CBS Interactive          NaN     3.5   \n",
       "3  North American Partners in Anesthesia          NaN     2.9   \n",
       "4                    Northwestern Mutual          NaN     3.8   \n",
       "\n",
       "                                             summary  \n",
       "0  Bachelor’s Degree or higher. Responds to data ...  \n",
       "1  Cultivate deep familiarity with methodological...  \n",
       "2  As a business intelligence analyst, you’ll lea...  \n",
       "3  1+ year of hands-on experience with MS SQL as ...  \n",
       "4  This team focuses on developing data science m...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_file_exists('df_0', cities=[cities[0]], titles_encoded=titles_encoded, headers=headers, user_agent_list=user_agent_list)\n",
    "df_0 = pd.read_csv('./datasets/df_0.csv')\n",
    "df_0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57, 6)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_0[df_0['salary'].notnull()].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "e8beed7c-3e42-40c0-810f-5f67f8f885a0"
   },
   "source": [
    "### Complete the following code to collect results from multiple cities and starting points. \n",
    "- Enter your city below to add it to the search.\n",
    "- Remember to convert your salary to U.S. Dollars to match the other cities if the currency is different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "20339c09-5032-4e27-91be-286e9b46cd13"
   },
   "source": [
    "#### Use the functions you wrote above to parse out the 4 fields - location, title, company and salary. Create a dataframe from the results with those 4 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file already exists\n"
     ]
    }
   ],
   "source": [
    "check_file_exists('df_1', cities=[cities[1]], titles_encoded=titles_encoded, \n",
    "                  headers=headers, user_agent_list=user_agent_list)\n",
    "df_1 = pd.read_csv('./datasets/df_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file already exists\n"
     ]
    }
   ],
   "source": [
    "check_file_exists('df_2', cities=[cities[2]], titles_encoded=titles_encoded, \n",
    "                  headers=headers, user_agent_list=user_agent_list)\n",
    "df_2 = pd.read_csv('./datasets/df_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file already exists\n"
     ]
    }
   ],
   "source": [
    "check_file_exists('df_3', cities=[cities[3]], titles_encoded=titles_encoded, \n",
    "                  headers=headers, user_agent_list=user_agent_list)\n",
    "df_3 = pd.read_csv('./datasets/df_3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file already exists\n"
     ]
    }
   ],
   "source": [
    "check_file_exists('df_4', cities=[cities[4]], titles_encoded=titles_encoded, \n",
    "                  headers=headers, user_agent_list=user_agent_list)\n",
    "df_4 = pd.read_csv('./datasets/df_4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file already exists\n"
     ]
    }
   ],
   "source": [
    "check_file_exists('df_5', cities=[cities[5]], titles_encoded=titles_encoded)\n",
    "df_5 = pd.read_csv('./datasets/df_5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file already exists\n"
     ]
    }
   ],
   "source": [
    "check_file_exists('df_6', cities=[cities[6]], titles_encoded=titles_encoded, \n",
    "                  headers=headers, user_agent_list=user_agent_list)\n",
    "df_6 = pd.read_csv('./datasets/df_6.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file already exists\n"
     ]
    }
   ],
   "source": [
    "check_file_exists('df_7', cities=[cities[7]], titles_encoded=titles_encoded, \n",
    "                  headers=headers, user_agent_list=user_agent_list)\n",
    "df_7 = pd.read_csv('./datasets/df_7.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file already exists\n"
     ]
    }
   ],
   "source": [
    "check_file_exists('df_8', cities=[cities[8]], titles_encoded=titles_encoded, \n",
    "                  headers=headers, user_agent_list=user_agent_list)\n",
    "df_8 = pd.read_csv('./datasets/df_8.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file already exists\n"
     ]
    }
   ],
   "source": [
    "check_file_exists('df_9', cities=[cities[9]], titles_encoded=titles_encoded, \n",
    "                  headers=headers, user_agent_list=user_agent_list)\n",
    "df_9 = pd.read_csv('./datasets/df_9.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file already exists\n"
     ]
    }
   ],
   "source": [
    "check_file_exists('df_10', cities=[cities[10]], titles_encoded=titles_encoded, \n",
    "                  headers=headers, user_agent_list=user_agent_list)\n",
    "df_10 = pd.read_csv('./datasets/df_10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file already exists\n"
     ]
    }
   ],
   "source": [
    "check_file_exists('df_11', cities=[cities[11]], titles_encoded=titles_encoded, \n",
    "                  headers=headers, user_agent_list=user_agent_list)\n",
    "df_11 = pd.read_csv('./datasets/df_11.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file already exists\n"
     ]
    }
   ],
   "source": [
    "check_file_exists('df_12', cities=[cities[12]], titles_encoded=titles_encoded, \n",
    "                  headers=headers, user_agent_list=user_agent_list)\n",
    "df_12 = pd.read_csv('./datasets/df_12.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file already exists\n"
     ]
    }
   ],
   "source": [
    "check_file_exists('df_13', cities=[cities[13]], titles_encoded=titles_encoded, \n",
    "                  headers=headers, user_agent_list=user_agent_list)\n",
    "df_13 = pd.read_csv('./datasets/df_13.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file already exists\n"
     ]
    }
   ],
   "source": [
    "check_file_exists('df_14', cities=[cities[14]], titles_encoded=titles_encoded, \n",
    "                  headers=headers, user_agent_list=user_agent_list)\n",
    "df_14 = pd.read_csv('./datasets/df_14.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file already exists\n"
     ]
    }
   ],
   "source": [
    "check_file_exists('df_15', cities=[cities[15]], titles_encoded=titles_encoded, \n",
    "                  headers=headers, user_agent_list=user_agent_list)\n",
    "df_15 = pd.read_csv('./datasets/df_15.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file already exists\n"
     ]
    }
   ],
   "source": [
    "MY_CITY = 'London'  # no exchange conversion needed, autimatically set to dollar\n",
    "check_file_exists('df_16', cities=[MY_CITY], titles_encoded=titles_encoded, \n",
    "                  headers=headers, user_agent_list=user_agent_list)\n",
    "df_16 = pd.read_csv('./datasets/df_16.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14992, 7)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if MY_CITY not in cities:\n",
    "    cities.append(MY_CITY)\n",
    "for i in range(len(cities)):\n",
    "    globals()[f'df_{i}']['city'] = np.repeat(cities[i], globals()[f'df_{i}'].shape[0])\n",
    "    \n",
    "df_indeed = pd.concat([df_0, df_1, df_2, df_3, df_4, df_5, df_6, df_7, df_8,\n",
    "                       df_9, df_10, df_11, df_12, df_13, df_14, df_15, df_16], axis=0)\n",
    "df_indeed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>location</th>\n",
       "      <th>company</th>\n",
       "      <th>salary</th>\n",
       "      <th>rating</th>\n",
       "      <th>summary</th>\n",
       "      <th>city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>HRIS Analyst</td>\n",
       "      <td>New Orleans, LA</td>\n",
       "      <td>Tulane University</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.1</td>\n",
       "      <td>Thorough understanding of essential business f...</td>\n",
       "      <td>New Orleans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>Salesforce Business Systems Analyst</td>\n",
       "      <td>Las Vegas, NV</td>\n",
       "      <td>Aristocrat Technologies</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.4</td>\n",
       "      <td>Experience collaborating with business stakeho...</td>\n",
       "      <td>Las Vegas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Redwood City, CA</td>\n",
       "      <td>CrowdRise</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Collaborate with analysts, engineers and busin...</td>\n",
       "      <td>San Francisco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>Business Intel Analyst Lead</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Nordstrom</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.9</td>\n",
       "      <td>Bachelor's Degree in quantitative field (or eq...</td>\n",
       "      <td>Seattle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>Business Analyst (Distribution Finance)</td>\n",
       "      <td>Dallas, TX</td>\n",
       "      <td>National Life Group</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.7</td>\n",
       "      <td>Educate customers at all levels of the organiz...</td>\n",
       "      <td>Dallas</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   job_title          location  \\\n",
       "75                              HRIS Analyst   New Orleans, LA   \n",
       "186      Salesforce Business Systems Analyst     Las Vegas, NV   \n",
       "292                            Data Engineer  Redwood City, CA   \n",
       "233              Business Intel Analyst Lead       Seattle, WA   \n",
       "116  Business Analyst (Distribution Finance)        Dallas, TX   \n",
       "\n",
       "                     company salary  rating  \\\n",
       "75         Tulane University    NaN     4.1   \n",
       "186  Aristocrat Technologies    NaN     3.4   \n",
       "292                CrowdRise    NaN     NaN   \n",
       "233                Nordstrom    NaN     3.9   \n",
       "116      National Life Group    NaN     3.7   \n",
       "\n",
       "                                               summary           city  \n",
       "75   Thorough understanding of essential business f...    New Orleans  \n",
       "186  Experience collaborating with business stakeho...      Las Vegas  \n",
       "292  Collaborate with analysts, engineers and busin...  San Francisco  \n",
       "233  Bachelor's Degree in quantitative field (or eq...        Seattle  \n",
       "116  Educate customers at all levels of the organiz...         Dallas  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_indeed.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2061"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_indeed[df_indeed['salary'].notnull()].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from selenium import webdriver\n",
    "# from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def not_null1(content):\n",
    "#     if content not in ('', None):\n",
    "#         return content\n",
    "#     else:\n",
    "#         raise Exception('No content found!')\n",
    "        \n",
    "# def extract_location_from_result1(result):\n",
    "#     try:\n",
    "#         data = result.find_element_by_css_selector('span.location').text.strip()\n",
    "#         return not_null1(data)\n",
    "#     except:\n",
    "#         return np.nan\n",
    "    \n",
    "    \n",
    "# def extract_job_from_result1(result):\n",
    "#     try:\n",
    "#         return not_null1(result.find_element_by_css_selector('a[data-tn-element=jobTitle]').get_attribute('title'))                                                        \n",
    "#     except:\n",
    "#         return np.nan\n",
    "    \n",
    "# def extract_company_from_result1(result):\n",
    "#     try:\n",
    "#         company_name = result.find_element_by_css_selector('a[data-tn-element=companyName]') \n",
    "#         if company_name:\n",
    "#             return company_name.text.strip()\n",
    "#         else:\n",
    "#             data = result.find_element_by_css_selector('span.company').text.strip()\n",
    "#             return not_null1(data)\n",
    "#     except:\n",
    "#         return np.nan\n",
    "    \n",
    "# def extract_salary_from_result1(result):\n",
    "#     try:\n",
    "#         return not_null1(result.find_element_by_css_selector('span.salaryText').text.strip())\n",
    "#     except:\n",
    "#         return np.nan\n",
    "\n",
    "# driver = webdriver.Chrome(executable_path='/Users/gabriel/Desktop/DataScience/chromedriver')\n",
    "\n",
    "\n",
    "# def extract_summary1(result):\n",
    "#     sleep(2)\n",
    "\n",
    "#     try:\n",
    "#         summary_html = result.find_element_by_css_selector('div#vjs-desc').get_attribute('innerHTML')\n",
    "#         return not_null1(summary_html)\n",
    "#     except:\n",
    "#         return np.nan\n",
    "\n",
    "\n",
    "# url_template = \"http://www.indeed.com/jobs?q={job}+%2420%2C000&l={location}&start={page}\"\n",
    "# max_results_per_city = 10 #5000 \n",
    "\n",
    "# titles = ['data scientist', \n",
    "#           'data analyst',\n",
    "#           'research scientist',\n",
    "#           'business intelligence',\n",
    "#           'database developer',\n",
    "#           'data engineer',\n",
    "#           'database administrator']\n",
    "\n",
    "# titles_encoded = ('%2C+').join([t.replace(' ', '+') for t in titles])\n",
    "# titles_conded = 'data+scientist'\n",
    "                        \n",
    "# salary1 = []\n",
    "# job_title1 = [] \n",
    "# location1 = []\n",
    "# company1 = []\n",
    "# summary1 = []\n",
    "\n",
    "# for city in tqdm(set(['New+York'])): #, 'Chicago', 'San+Francisco', 'Austin', 'Seattle', \n",
    "# #     'Los+Angeles', 'Philadelphia', 'Atlanta', 'Dallas', 'Pittsburgh', \n",
    "# #     'Portland', 'Phoenix', 'Denver', 'Houston', 'Miami', YOUR_CITY])):\n",
    "#     counter = 0\n",
    "#     for start in range(0, max_results_per_city, 10):\n",
    "#         r = driver.get(url_template.format(job=titles_encoded,\n",
    "#                                              location=city,\n",
    "#                                              page=start))\n",
    "        \n",
    "#         results = driver.find_elements_by_css_selector('div.result')\n",
    "#         try:\n",
    "#             duplicate_test = driver.find_element_by_css_selector('p.dupetext')\n",
    "#         except:\n",
    "#             duplicate_test = None\n",
    "#         try:\n",
    "#             duplicate_test2 = driver.find_element_by_css_selector('div.related_searches') \n",
    "#         except:\n",
    "#             duplacte_test2 = None\n",
    "        \n",
    "#         print(f'City: {city}, Page: {start}, counter: {counter}, duplicate_test: {bool(duplicate_test) or bool(duplicate_test2)}')\n",
    "\n",
    "        \n",
    "#         if duplicate_test or duplicate_test2:\n",
    "#             counter += 1    \n",
    "#             if counter > 1:\n",
    "#                 break\n",
    "    \n",
    "\n",
    "#         for job in results:\n",
    "#             salary1.append(extract_salary_from_result1(job))\n",
    "#             job_title1.append(extract_job_from_result1(job))\n",
    "#             location1.append(extract_location_from_result1(job))\n",
    "#             company1.append(extract_company_from_result1(job))\n",
    "#             job.click()\n",
    "#             summary1.append(extract_summary1(driver))\n",
    "                        \n",
    "# test1 = pd.DataFrame({'tilte': job_title1, 'location': location1, 'salary': salary1, 'company': company1,\n",
    "#                      'summary': summary1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "ff98ce64-78a7-441f-a675-63464e32c834"
   },
   "source": [
    "Lastly, we need to clean up salary data. \n",
    "\n",
    "1. Only a small number of the scraped results have salary information - only these will be used for modeling.\n",
    "1. Some of the salaries are not yearly but hourly or weekly, these will not be useful to us for now.\n",
    "1. Some of the entries may be duplicated.\n",
    "1. The salaries are given as text and usually with ranges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "ff98ce64-78a7-441f-a675-63464e32c834"
   },
   "source": [
    "#### Find the entries with annual salary entries, by filtering the entries without salaries or salaries that are not yearly (filter those that refer to hour or week). Also, remove duplicate entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = df_indeed.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1538, 7)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs.drop_duplicates(inplace=True)\n",
    "jobs[jobs['salary'].notnull()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs.dropna(subset=['salary'], inplace=True)\n",
    "# jobs = jobs[jobs['salary'].str.contains(r'year')]\n",
    "# jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "487"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs[jobs['salary'].str.contains('hour')].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "975"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs[jobs['salary'].str.contains('year')].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs[jobs['salary'].str.contains('month')].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "$219 a day       1\n",
       "$40 per class    1\n",
       "Name: salary, dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs[~jobs['salary'].str.contains('hour|year|month')]['salary'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_hourly = jobs[jobs['salary'].str.contains('hour')].copy()\n",
    "jobs_monthly = jobs[jobs['salary'].str.contains('month')].copy()\n",
    "jobs_yearly = jobs[jobs['salary'].str.contains('year')].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "7d4bc860-b214-4f75-9cd0-b234830b1ec2"
   },
   "source": [
    "#### Write a function that takes a salary string and converts it to a number, averaging a salary range if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['$60,000 - $70,000 a year' '$123,438 a year' '$50,000 - $60,000 a year'\n",
      " '$55,000 - $60,000 a year' '$63,809 - $80,882 a year']\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "print(jobs_yearly['salary'].sample(5).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['$4,301 - $7,500 a month' '$4,096 - $5,766 a month'\n",
      " '$6,999 - $7,900 a month' '$4,500 - $4,900 a month' '$4,095 a month']\n"
     ]
    }
   ],
   "source": [
    "print(jobs_monthly['salary'].sample(5).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['From $60 an hour' '$41 - $50 an hour' '$50 - $55 an hour'\n",
      " 'From $13 an hour' '$14 - $29 an hour']\n"
     ]
    }
   ],
   "source": [
    "print(jobs_hourly['salary'].sample(5).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sal_to_num(salary):    \n",
    "#     sal= int    \n",
    "#     '''Takes in a salary entry (as string) and extracts the digits as an integer.    \n",
    "#     If the salary is in a range (eg 20,000-25,00) it returns the average'''    \n",
    "    \n",
    "#     try:        \n",
    "#         sal = int(''.join(re.findall(r'([\\d-])', salary)))    \n",
    "#     except:        \n",
    "#         sal = (int(''.join(re.findall(r'([\\d-])', salary)).split(\"-\")[0]) + \n",
    "#                int(''.join(re.findall(r'([\\d-])', salary)).split(\"-\")[1])) / 2            \n",
    "        \n",
    "#     return int(sal)        \n",
    "\n",
    "# df.loc[:,'Salary'] = df.Salary.map(lambda x: sal_to_num(x)).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "focus": false,
    "id": "a0f701e0-80bd-40ba-9101-4535860c0968"
   },
   "outputs": [],
   "source": [
    "def convert_salary(salary, period='y', currency_rate=1):\n",
    "    try:\n",
    "        if period == 'y':\n",
    "            # [, ] -> space for some other currencies than dollar\n",
    "            sal = np.mean([float(re.sub(r'[, ]', '', s)) for s in re.findall(r'\\d+[, ]\\d{3,}', salary)])\n",
    "        elif period == 'm':\n",
    "            # could be less than $1000 therefore using r'\\d*[, ]?'\n",
    "            sal = np.mean([float(re.sub(r'[, ]', '', s)) for s in re.findall(r'\\d*[, ]?\\d{3,}', salary)])  \n",
    "        elif period == 'h':\n",
    "            # get rid of decimal points -> (?<![\\.|,])\\d{2,3} only give number back if there is no previous decimal point or coma \n",
    "            sal = np.mean([float(re.sub(r'[, ]', '', s)) for s in re.findall(r'(?<![\\.|,])\\d{2,3}|\\d[, ]?\\d{3}', salary)]) \n",
    "        else:\n",
    "            raise Exception(\"define period: ['y', 'm', 'h']\")\n",
    "        sal *= currency_rate\n",
    "        \n",
    "    except:\n",
    "        sal = np.nan\n",
    "    \n",
    "    return sal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_hourly['salary'] = jobs_hourly['salary'].apply(lambda x: convert_salary(x, 'h'))\n",
    "jobs_monthly['salary'] = jobs_monthly['salary'].apply(lambda x: convert_salary(x, 'm'))\n",
    "jobs_yearly['salary'] = jobs_yearly['salary'].apply(lambda x: convert_salary(x, 'y'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000.0    32\n",
       "50000.0    27\n",
       "70000.0    24\n",
       "55000.0    22\n",
       "65000.0    22\n",
       "Name: salary, dtype: int64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs_yearly['salary'].value_counts().iloc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "43e71edd-210e-42b1-9336-70a931f048af"
   },
   "source": [
    "### Save your results as a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ != \"__main__\":\n",
    "    jobs_hourly.to_csv('./datasets/jobs_hourly.csv', index=False)\n",
    "    jobs_monthly.to_csv('./datasets/jobs_monthly.csv', index=False)\n",
    "    jobs_yearly.to_csv('./datasets/jobs_yearly.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "243e949e-2742-40af-872e-fec475fd306c"
   },
   "source": [
    "### Load in the the data of scraped salaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "focus": false,
    "id": "588f9845-6143-4bcc-bfd1-85d45b79303d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>location</th>\n",
       "      <th>company</th>\n",
       "      <th>salary</th>\n",
       "      <th>rating</th>\n",
       "      <th>summary</th>\n",
       "      <th>city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Intelligence Operations Specialist</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>US Department of Homeland Security</td>\n",
       "      <td>144688.0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>Presents formal briefings or written reports o...</td>\n",
       "      <td>New York</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Investigative Analyst</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>New York District Attorney's Office</td>\n",
       "      <td>48909.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>One year experience preferred, either as a par...</td>\n",
       "      <td>New York</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Business Analyst</td>\n",
       "      <td>Brooklyn, NY</td>\n",
       "      <td>Cyber Tech Group</td>\n",
       "      <td>65000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Providing legal and scholarly research; Keen e...</td>\n",
       "      <td>New York</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Research Scientist 1 (Biostatistics or Health ...</td>\n",
       "      <td>Albany, NY 12237</td>\n",
       "      <td>Health, Department of</td>\n",
       "      <td>72364.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Research Scientist will work under the dir...</td>\n",
       "      <td>New York</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Business Analyst</td>\n",
       "      <td>Manhattan, NY</td>\n",
       "      <td>NYC HOUSING AUTHORITY</td>\n",
       "      <td>70437.5</td>\n",
       "      <td>3.8</td>\n",
       "      <td>Fields of finance; economic, fiscal or statist...</td>\n",
       "      <td>New York</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           job_title          location  \\\n",
       "0                 Intelligence Operations Specialist      New York, NY   \n",
       "1                              Investigative Analyst      New York, NY   \n",
       "2                                   Business Analyst      Brooklyn, NY   \n",
       "3  Research Scientist 1 (Biostatistics or Health ...  Albany, NY 12237   \n",
       "4                                   Business Analyst     Manhattan, NY   \n",
       "\n",
       "                               company    salary  rating  \\\n",
       "0   US Department of Homeland Security  144688.0     3.8   \n",
       "1  New York District Attorney's Office   48909.0     4.4   \n",
       "2                     Cyber Tech Group   65000.0     NaN   \n",
       "3                Health, Department of   72364.0     NaN   \n",
       "4                NYC HOUSING AUTHORITY   70437.5     3.8   \n",
       "\n",
       "                                             summary      city  \n",
       "0  Presents formal briefings or written reports o...  New York  \n",
       "1  One year experience preferred, either as a par...  New York  \n",
       "2  Providing legal and scholarly research; Keen e...  New York  \n",
       "3  The Research Scientist will work under the dir...  New York  \n",
       "4  Fields of finance; economic, fiscal or statist...  New York  "
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./datasets/jobs_yearly.csv')\n",
    "jobs = df.copy()\n",
    "jobs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(975, 7)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "c7631f51-07f2-4c79-a093-3e9bc7849a48"
   },
   "source": [
    "### We want to predict a binary variable - whether the salary was low or high. Compute the median salary and create a new binary variable that is true when the salary is high (above the median).\n",
    "\n",
    "We could also perform Linear Regression (or any regression) to predict the salary value here. Instead, we are going to convert this into a _binary_ classification problem, by predicting two classes, HIGH vs LOW salary.\n",
    "\n",
    "While performing regression may be better, performing classification may help remove some of the noise of the extreme salaries. We don't have to choose the `median` as the splitting point - we could also split on the 75th percentile or any other reasonable breaking point.\n",
    "\n",
    "In fact, the ideal scenario may be to predict many levels of salaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 975 entries, 0 to 974\n",
      "Data columns (total 7 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   job_title  975 non-null    object \n",
      " 1   location   851 non-null    object \n",
      " 2   company    975 non-null    object \n",
      " 3   salary     975 non-null    float64\n",
      " 4   rating     574 non-null    float64\n",
      " 5   summary    971 non-null    object \n",
      " 6   city       975 non-null    object \n",
      "dtypes: float64(2), object(5)\n",
      "memory usage: 53.4+ KB\n"
     ]
    }
   ],
   "source": [
    "jobs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "focus": false,
    "id": "c20d2498-151c-44c3-a453-3a333c79a0ac"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>salary</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>975.000000</td>\n",
       "      <td>574.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>84166.722051</td>\n",
       "      <td>3.752787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>34348.361173</td>\n",
       "      <td>0.556025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>29500.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>60000.000000</td>\n",
       "      <td>3.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>78081.500000</td>\n",
       "      <td>3.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>101061.500000</td>\n",
       "      <td>4.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>400000.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              salary      rating\n",
       "count     975.000000  574.000000\n",
       "mean    84166.722051    3.752787\n",
       "std     34348.361173    0.556025\n",
       "min     29500.000000    1.000000\n",
       "25%     60000.000000    3.500000\n",
       "50%     78081.500000    3.800000\n",
       "75%    101061.500000    4.200000\n",
       "max    400000.000000    5.000000"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0489"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs[['rating', 'salary']].corr().iloc[0,1].round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rating    401\n",
       "dtype: int64"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs[['rating']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jobs['rating'].fillna(value=jobs['rating'].median(), inplace=True)\n",
    "if 'rating' in jobs.columns:\n",
    "    jobs.drop(['rating'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "job_title      0\n",
       "location     124\n",
       "company        0\n",
       "salary         0\n",
       "summary        4\n",
       "city           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>location</th>\n",
       "      <th>company</th>\n",
       "      <th>salary</th>\n",
       "      <th>summary</th>\n",
       "      <th>city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Houston</td>\n",
       "      <td>Checkmate Partners</td>\n",
       "      <td>205000.0</td>\n",
       "      <td>Working cross-functionally with data scientist...</td>\n",
       "      <td>Houston</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Houston</td>\n",
       "      <td>JES Tech</td>\n",
       "      <td>55000.0</td>\n",
       "      <td>Bachelor’s degree or certification in Data Sci...</td>\n",
       "      <td>Houston</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>Measurement Data Analyst</td>\n",
       "      <td>Houston</td>\n",
       "      <td>SPL, Inc.</td>\n",
       "      <td>80000.0</td>\n",
       "      <td>Associates degree (A.A.) or equivalent from a ...</td>\n",
       "      <td>Houston</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    job_title location             company    salary  \\\n",
       "102             Data Engineer  Houston  Checkmate Partners  205000.0   \n",
       "116            Data Scientist  Houston            JES Tech   55000.0   \n",
       "117  Measurement Data Analyst  Houston           SPL, Inc.   80000.0   \n",
       "\n",
       "                                               summary     city  \n",
       "102  Working cross-functionally with data scientist...  Houston  \n",
       "116  Bachelor’s degree or certification in Data Sci...  Houston  \n",
       "117  Associates degree (A.A.) or equivalent from a ...  Houston  "
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs['summary'].fillna(value='Nothing', inplace=True)\n",
    "empty_location_idx = jobs[jobs['location'].isnull()].index\n",
    "jobs.loc[empty_location_idx, 'location'] = jobs.loc[empty_location_idx].apply(lambda col: col['city'], axis=1)\n",
    "jobs.loc[empty_location_idx].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26     New York State\n",
       "102           Houston\n",
       "116           Houston\n",
       "117           Houston\n",
       "118           Houston\n",
       "Name: location, dtype: object"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs[~jobs['location'].str.contains(',')]['location'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        New York, NY\n",
       "1        New York, NY\n",
       "2        Brooklyn, NY\n",
       "3    Albany, NY 12237\n",
       "4       Manhattan, NY\n",
       "Name: location, dtype: object"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs[jobs['location'].str.contains(',')]['location'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_location(data, c=True):\n",
    "    \"\"\"\n",
    "    This function returns either the city or the state name depending on the parameter c\n",
    "    (if c is True then city else state), but only if the string contains a comma. Otherwise\n",
    "    it returns the raw text. \n",
    "    \n",
    "    Parameters:\n",
    "    data [String]: location data\n",
    "    c [Boolean]: True for city name, False for state name\n",
    "    \n",
    "    Return:\n",
    "    city [String]: city name\n",
    "    or\n",
    "    state [String]: state name\n",
    "    or\n",
    "    data [String]: raw data\n",
    "    \"\"\"\n",
    "    if ',' in data:\n",
    "        first_data, *second_data = data.split(',')  # in the case of several commas\n",
    "        splitted_names = re.findall(r'[A-Z][a-z]+', first_data)\n",
    "        city = (' ').join(splitted_names) if splitted_names else np.nan\n",
    "        state_match = re.search(r'[A-Z]{2}', second_data[0])\n",
    "        state =  state_match.group() if state_match else np.nan\n",
    "    else:\n",
    "        return data\n",
    "    if c:\n",
    "        return city\n",
    "    else:\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>location</th>\n",
       "      <th>company</th>\n",
       "      <th>salary</th>\n",
       "      <th>summary</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Intelligence Operations Specialist</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>US Department of Homeland Security</td>\n",
       "      <td>144688.0</td>\n",
       "      <td>Presents formal briefings or written reports o...</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Investigative Analyst</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>New York District Attorney's Office</td>\n",
       "      <td>48909.0</td>\n",
       "      <td>One year experience preferred, either as a par...</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Business Analyst</td>\n",
       "      <td>Brooklyn, NY</td>\n",
       "      <td>Cyber Tech Group</td>\n",
       "      <td>65000.0</td>\n",
       "      <td>Providing legal and scholarly research; Keen e...</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>NY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            job_title      location  \\\n",
       "0  Intelligence Operations Specialist  New York, NY   \n",
       "1               Investigative Analyst  New York, NY   \n",
       "2                    Business Analyst  Brooklyn, NY   \n",
       "\n",
       "                               company    salary  \\\n",
       "0   US Department of Homeland Security  144688.0   \n",
       "1  New York District Attorney's Office   48909.0   \n",
       "2                     Cyber Tech Group   65000.0   \n",
       "\n",
       "                                             summary      city state  \n",
       "0  Presents formal briefings or written reports o...  New York    NY  \n",
       "1  One year experience preferred, either as a par...  New York    NY  \n",
       "2  Providing legal and scholarly research; Keen e...  Brooklyn    NY  "
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs['city'] = jobs['location'].apply(lambda x: split_location(x))\n",
    "jobs['state'] = jobs['location'].apply(lambda x: split_location(x, False))\n",
    "jobs.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# empty_states_indices = jobs[jobs['state'].isnull()].index.tolist()\n",
    "# empty_states_indices\n",
    "# jobs.loc[empty_states_indices, :]\n",
    "jobs['state'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TX                168\n",
       "FL                107\n",
       "AZ                 93\n",
       "CA                 82\n",
       "GA                 76\n",
       "IL                 68\n",
       "NV                 62\n",
       "WA                 52\n",
       "MA                 34\n",
       "NY                 34\n",
       "LA                 24\n",
       "OH                 22\n",
       "Austin             21\n",
       "Phoenix            20\n",
       "PA                 20\n",
       "Los Angeles        19\n",
       "San Francisco      15\n",
       "Miami              14\n",
       "Atlanta            13\n",
       "Houston            11\n",
       "Chicago            11\n",
       "NJ                  5\n",
       "DE                  2\n",
       "IN                  1\n",
       "New York State      1\n",
       "Name: state, dtype: int64"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs['state'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs['state'].replace({'Los Angeles': 'CA', \n",
    "                       'San Francisco': 'CA',\n",
    "                       'Austin': 'TX',\n",
    "                       'Houston': 'TX',\n",
    "                       'Chicago': 'IL',\n",
    "                       'Phoenix': 'AZ',\n",
    "                       'Miami': 'FL',\n",
    "                       'Atlanta': 'GA',\n",
    "                       'New York State': 'NY'}, inplace=True)\n",
    "jobs.drop(['location'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>company</th>\n",
       "      <th>summary</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>975</td>\n",
       "      <td>975</td>\n",
       "      <td>975</td>\n",
       "      <td>975</td>\n",
       "      <td>975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>825</td>\n",
       "      <td>677</td>\n",
       "      <td>905</td>\n",
       "      <td>150</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>Indeed</td>\n",
       "      <td>Experience in an ML engineer or data scientist...</td>\n",
       "      <td>Austin</td>\n",
       "      <td>TX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>14</td>\n",
       "      <td>29</td>\n",
       "      <td>8</td>\n",
       "      <td>96</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   job_title company  \\\n",
       "count                    975     975   \n",
       "unique                   825     677   \n",
       "top     Senior Data Engineer  Indeed   \n",
       "freq                      14      29   \n",
       "\n",
       "                                                  summary    city state  \n",
       "count                                                 975     975   975  \n",
       "unique                                                905     150    16  \n",
       "top     Experience in an ML engineer or data scientist...  Austin    TX  \n",
       "freq                                                    8      96   200  "
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs.describe(include=['O'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200                               Senior Data Engineer\n",
       "829            Board Certified Behavior Analyst (BCBA)\n",
       "368                              Sr. Software Engineer\n",
       "198    Sr. Analyst, Organizational Strategy and Design\n",
       "55                        Analyst I, Budget (226 Days)\n",
       "357                              Engineering Associate\n",
       "955                                    Premium Auditor\n",
       "588                              Mechanical Engineer I\n",
       "163                                Mechanical Engineer\n",
       "383                                   PROCESS ENGINEER\n",
       "Name: job_title, dtype: object"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs['job_title'].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78081.5"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "median_salary = jobs['salary'].median()\n",
    "median_salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "\n",
    "sns.boxplot(x=jobs['salary'], orient='h', fliersize=8, \n",
    "            linewidth=1.5, saturation=0.5, whis=3.0, ax=ax)\n",
    "ax.set_title('Salary\\n', fontsize=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_outlier_idx = jobs.loc[jobs['salary']> 220000].index\n",
    "jobs.drop(index=salary_outlier_idx, inplace=True)\n",
    "jobs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jobs['high_salary'] = jobs['salary'].map(lambda x: 1 if x > median_salary else 0)\n",
    "jobs.loc[:, 'salary'] = jobs['salary'].map(lambda x: 'High' if x > median_salary else 'Low')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "\n",
    "sns.boxplot(['rating'], data=jobs, orient='h', fliersize=8, \n",
    "            linewidth=1.5, saturation=0.5, whis=1.5, ax=ax)\n",
    "ax.set_title('Rating\\n', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(jobs, hue='high_salary');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "a7afb2c0-d41e-4779-8216-91cd8dd4473f"
   },
   "source": [
    "#### Thought experiment: What is the baseline accuracy for this model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "focus": false,
    "id": "87a17d3d-b7f4-4747-9f75-f9af1d18a174"
   },
   "outputs": [],
   "source": [
    "baseline_accuracy = round(jobs['high_salary'].value_counts(normalize=True).max(), 4)\n",
    "baseline_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "4fb29de2-5b98-474c-a4ad-5170b72b9aea"
   },
   "source": [
    "### Create a classification model to predict High/Low salary. \n",
    "\n",
    "\n",
    "- Start by ONLY using the location as a feature.\n",
    "- Use at least two different classifiers you find suitable.\n",
    "- Remember that scaling your features might be necessary.\n",
    "- Display the coefficients/feature importances and write a short summary of what they mean.\n",
    "- Create a few new variables in your dataframe to represent interesting features of a job title (e.g. whether 'Senior' or 'Manager' is in the title).\n",
    "- Incorporate other text features from the title or summary that you believe will predict the salary.\n",
    "- Then build new classification models including also those features. Do they add any value?\n",
    "- Tune your models by testing parameter ranges, regularization strengths, etc. Discuss how that affects your models.\n",
    "- Discuss model coefficients or feature importances as applicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, make_union, make_pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = jobs[['city', 'state', 'high_salary']]\n",
    "X = pd.get_dummies(X, prefix=['city', 'state'], columns=['city', 'state'], drop_first=True)\n",
    "X.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = X.pop('high_salary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropDummyfierPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, columns_to_drop=None, columns_to_dummify=None, drop_first=True):\n",
    "        self.feature_names = []\n",
    "        self.columns_to_drop = columns_to_drop\n",
    "        self.columns_to_dummify = columns_to_dummify\n",
    "        self.drop_first = drop_first\n",
    "        \n",
    "    def _drop_unused_cols(self, X):\n",
    "        \"\"\"Try to drop each given column by its own\"\"\"\n",
    "        for col in self.columns_to_drop:\n",
    "            try:\n",
    "                X = X.drop(col, axis=1)\n",
    "            except:\n",
    "                pass\n",
    "        return X\n",
    "\n",
    "    def _make_dummy_cols(self, X):\n",
    "        X = pd.get_dummies(X, columns=self.columns_to_dummify, drop_first=self.drop_first)\n",
    "        return X\n",
    "    \n",
    "    def transform(self, X, *args):\n",
    "        X = self._make_dummy_cols(X)\n",
    "        X = self._drop_unused_cols(X)\n",
    "        self.feature_names = X.columns\n",
    "        return X\n",
    "    \n",
    "    def fit(self, X, *args):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, column):\n",
    "        self.column = column\n",
    "        \n",
    "    def fit(self, X, *args):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, *args):\n",
    "        return X[self.column].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, BaggingClassifier, \\\n",
    "AdaBoostClassifier, AdaBoostRegressor, GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                    stratify=y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'estimator': [DecisionTreeClassifier(), \n",
    "                  DecisionTreeRegressor(),\n",
    "                  LogisticRegression(solver='liblinear'),\n",
    "                  KNeighborsClassifier(),\n",
    "                 ],\n",
    "    \n",
    "    'param_grid': [\n",
    "        {\n",
    "            'max_depth': [1, 2, 3],\n",
    "            'max_features': [None, 1, 2, 3],\n",
    "            'min_samples_split': [2, 3, 4, 20,  30, 50],\n",
    "            'ccp_alpha': [0, 0.001, 0.005, 0.01]\n",
    "        },\n",
    "        {\n",
    "            'max_depth': [1, 2, 3],\n",
    "            'max_features': [None, 1, 2, 3],\n",
    "            'min_samples_split': [2, 3, 4, 20,  30, 50],\n",
    "            'ccp_alpha': [0, 0.001, 0.005, 0.01]\n",
    "        },\n",
    "        {\n",
    "            'C': np.logspace(-4, 4, 15), \n",
    "            'penalty': ['l2', 'l1'],\n",
    "        },\n",
    "        {\n",
    "            'n_neighbors': range(3, 100, 2),\n",
    "            'weights': ['uniform', 'distance']\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estimators_scores(X_train, y_train, X_test=None, y_test=None, \n",
    "                          models=[], params=[], verbose=1, n_jobs=2, cv=5):\n",
    "    best_estimators = []\n",
    "    for i, _ in enumerate(models):\n",
    "        p = {'param_grid': params[i]}\n",
    "        gs = GridSearchCV(estimator=models[i], \n",
    "                          verbose = verbose,\n",
    "                          n_jobs = n_jobs,\n",
    "                          cv = cv,\n",
    "                          **p)\n",
    "        gs.fit(X_train, y_train)\n",
    "        model_name = type(gs.estimator).__name__\n",
    "        print('{model} - Accuracy score: {score}'.format(model=model_name,\n",
    "                                                         score=gs.score(X_train, y_train).round(4)))\n",
    "        print('{model} - CV training score: {score}'.format(model=model_name, \n",
    "                                                            score=gs.best_score_.round(4)))\n",
    "        \n",
    "        if isinstance(X_test, pd.DataFrame) and isinstance(y_test, pd.Series):\n",
    "            print('{model} - CV test score: {score}'.format(model=model_name,\n",
    "                                                            score=gs.score(X_test, y_test).round(4)))\n",
    "        print()\n",
    "        best_estimators.append(gs)\n",
    "    return best_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = get_estimators_scores(X_train, y_train, X_test, y_test,\n",
    "                                  models = model_params['estimator'], \n",
    "                                  params = model_params['param_grid'],\n",
    "                                  cv = skf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DecisionTreeClassifier/ DecisionTreeRegressor:\n",
    "- Feature Importance is computed as the total reduction of the criterion brought by that feature. Because the default setting for that classifier was used, it refers to the gini score. The state California has the highest effect to the classification in the given case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DecisionTreeClassifier\n",
    "pd.DataFrame(list(zip(X_train.columns, estimators[0].best_estimator_.feature_importances_)), \n",
    "             columns=['Feature', 'Importance']\n",
    "            ).sort_values(by='Importance', ascending=False)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DecisionTreeRegressor\n",
    "pd.DataFrame(list(zip(X_train.columns, estimators[1].best_estimator_.feature_importances_)), \n",
    "             columns=['Feature', 'Importance']\n",
    "            ).sort_values(by='Importance', ascending=False)[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LogisticRegression:\n",
    "- If following link function is used:\n",
    "$$P(y=1|X)=\\frac{1}{1+e^{-z}}$$,\n",
    "then is the probability to predict class 1 by 50% if the sum of all coefficients multiplied by their features is 0. Otherwise if z increases the probability for predicting class 1 increases as well and vice versa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LogisticRegression\n",
    "pd.DataFrame({'Feature': X_train.columns, \n",
    "              'Coefficient': estimators[2].best_estimator_.coef_[0], \n",
    "              'Abs_Coefficient': abs(estimators[2].best_estimator_.coef_[0])}\n",
    "            ).sort_values(by='Abs_Coefficient', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = jobs.copy()\n",
    "X.drop(['salary'], axis=1, inplace=True)\n",
    "y = X.pop('high_salary')\n",
    "#X.reset_index(inplace=True)\n",
    "X.drop(columns=['location', 'summary'], inplace=True)\n",
    "X = pd.get_dummies(X, columns=['company', 'city', 'state'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                    stratify=y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvec = TfidfVectorizer(stop_words='english',\n",
    "                            #sublinear_tf=True,\n",
    "                            max_df=0.3,\n",
    "                            max_features=1000)\n",
    "\n",
    "tvec.fit(X_train['job_title']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_job_title = tvec.transform(X_train['job_title'])\n",
    "X_test_job_title = tvec.transform(X_test['job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(X_train_job_title.toarray(),\n",
    "                  columns=tvec.get_feature_names())\n",
    "\n",
    "df_train.transpose().sort_values(0, ascending=False).transpose().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame(X_test_job_title.toarray(),\n",
    "                  columns=tvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.drop(columns=['job_title'], axis=1, inplace=True)\n",
    "X_test.drop(columns=['job_title'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.reset_index(inplace=True)\n",
    "X_test.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat([X_train, df_train], axis=1)\n",
    "X_test = pd.concat([X_test, df_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X= pd.concat([X.loc[:, ~X.columns.isin(['job_title'])], df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jobs_prep = DropDummyfierPreprocessor(columns_to_drop=['location', 'summary'], \n",
    "#                               columns_to_dummify=['company', 'city', 'state'])\n",
    "\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# pipe = Pipeline(steps=[('jobs_prep', jobs_prep),\n",
    "#                        ('scaler', scaler)])\n",
    "\n",
    "# p = pipe.fit(X, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = get_estimators_scores(X_train, y_train, X_test, y_test,\n",
    "                                  models = model_params['estimator'], \n",
    "                                  params = model_params['param_grid'],\n",
    "                                  cv = skf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "# confusion matrix\n",
    "def docm(y_true, y_pred, labels=None):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    if labels is not None:\n",
    "        cols = ['pred_' + c for c in labels]\n",
    "        df = pd.DataFrame(cm, index=labels, columns=cols)\n",
    "    else:\n",
    "        cols = ['pred_'+str(i) for i in range(len(cm))]\n",
    "        df = pd.DataFrame(cm, columns=cols)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DecisionTreeClassifier\n",
    "docm(y_test, estimators[0].best_estimator_.predict(X_test), labels=['high_salary', 'low_salary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LogisticRegression\n",
    "docm(y_test, estimators[2].best_estimator_.predict(X_test), labels=['high_salary', 'low_salary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(gamma='scale')\n",
    "rfc = RandomForestClassifier(n_estimators=100)\n",
    "etc = ExtraTreesClassifier(n_estimators=100)\n",
    "rc = RidgeClassifier(tol=1e-2, solver=\"sag\")\n",
    "per = Perceptron(max_iter=1000, tol=1e-3)\n",
    "pac = PassiveAggressiveClassifier(max_iter=1000, tol=1e-3)\n",
    "lsvc = LinearSVC(loss='squared_hinge', \n",
    "                 penalty='l2',\n",
    "                 max_iter=100000,\n",
    "                 #tol=0.01, \n",
    "                 dual=False)\n",
    "sgdc = SGDClassifier(alpha=.0001,\n",
    "                     penalty=\"elasticnet\",\n",
    "                     max_iter=1000,\n",
    "                     tol=1e-3)\n",
    "knn = estimators[3].best_estimator_\n",
    "bg_knn = BaggingClassifier(base_estimator=knn, \n",
    "                           n_estimators=100, \n",
    "                           max_samples=0.8,\n",
    "                           max_features=0.8)\n",
    "dtc = estimators[0].best_estimator_\n",
    "bg_dtc = BaggingClassifier(base_estimator=dtc, \n",
    "                           n_estimators=100, \n",
    "                           max_samples=0.8,\n",
    "                           max_features=0.8)\n",
    "adaboost_dtc = AdaBoostClassifier(base_estimator=dtc,\n",
    "                                  n_estimators=10,\n",
    "                                  algorithm='SAMME')\n",
    "gradientboost = GradientBoostingClassifier(n_estimators=100,\n",
    "                                           criterion='mse',\n",
    "                                           loss='exponential',\n",
    "                                           max_depth=3,\n",
    "                                           learning_rate=1.0)\n",
    "\n",
    "\n",
    "for model in [svc, rfc, etc, rc, per, pac, lsvc, sgdc, bg_knn, bg_dtc, adaboost_dtc, gradientboost]:\n",
    "    model.fit(X_train, y_train)\n",
    "    model_name = type(model).__name__\n",
    "    print('{model} - Accuracy score: {score}' \\\n",
    "              .format(model=model_name,\n",
    "                      score=model.score(X_train, y_train).round(4)))\n",
    "    print('{model} - CV training score: {score}' \\\n",
    "              .format(model=model_name,\n",
    "                      score=cross_val_score(model, X_train, y_train, cv=skf).mean().round(4)))\n",
    "    print('{model} - CV testing score: {score}' \\\n",
    "              .format(model=model_name, \n",
    "                      score=cross_val_score(model, X_test, y_test, cv=skf).mean().round(4)))    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tuning of hyperparameters with gridsearch\n",
    "model_params = {\n",
    "    'models': [RandomForestClassifier(n_estimators=200), \n",
    "               ExtraTreesClassifier(n_estimators=200), \n",
    "               LinearSVC(dual=False)],\n",
    "    'gs_params': [\n",
    "        {\n",
    "            'max_depth': [1, 2, 3, None],\n",
    "            'max_features': [None, 1, 2, 3],\n",
    "            'min_samples_split': [2, 3, 4, 20,  30, 50],\n",
    "            'ccp_alpha': [0, 0.001, 0.005, 0.01, 0.1]\n",
    "        },\n",
    "        {\n",
    "            'max_depth': [1, 2, 3, None],\n",
    "            'max_features': [None, 1, 2, 3],\n",
    "            'min_samples_split': [2, 3, 4, 20,  30, 50],\n",
    "            'ccp_alpha': [0, 0.001, 0.005, 0.01, 0.1]\n",
    "        },\n",
    "        {\n",
    "            'penalty': ['l2', 'l1'],\n",
    "            'loss': ['hinge', 'squared_hinge'],\n",
    "            'C': [1, 10, 100, 1000],\n",
    "            'fit_intercept': [True, False]\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = get_estimators_scores(X_train, y_train, X_test, y_test,\n",
    "                                  models = model_params['models'], \n",
    "                                  params = model_params['gs_params'],\n",
    "                                  cv = skf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "4fb29de2-5b98-474c-a4ad-5170b72b9aea"
   },
   "source": [
    "### Model evaluation:\n",
    "\n",
    "Your boss would rather tell a client incorrectly that they would get a lower salary job than tell a client incorrectly that they would get a high salary job. Adjust one of your models to ease his mind, and explain what it is doing and any tradeoffs.\n",
    "\n",
    "\n",
    "- Use cross-validation to evaluate your models.\n",
    "- Evaluate the accuracy, AUC, precision and recall of the models.\n",
    "- Plot the ROC and precision-recall curves for at least one of your models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, plot_confusion_matrix, \\\n",
    "plot_roc_curve, recall_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#estimators[0].best_estimator_.predict(X_train, y_train)\n",
    "rfc = RandomForestClassifier(n_estimators=100, n_jobs=2, class_weight={0:.6, 1:.4})\n",
    "\n",
    "params = {\n",
    "    'max_depth': [1, 3, None],\n",
    "    'max_features': [None, 1, 3],\n",
    "    'min_samples_split': [2, 3, 50],\n",
    "    'ccp_alpha': [0, 0.001, 0.1]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(rfc, # scoring='precision', \n",
    "                  param_grid=params, \n",
    "                  verbose=1, \n",
    "                  n_jobs=2,\n",
    "                  cv=5)\n",
    "gs.fit(X_train, y_train)\n",
    "print('Accuracy score:', gs.score(X_train, y_train))\n",
    "print('CV score:', gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rather to predict false negative than false positive, that's why class 0 was weighted higher \n",
    "# (and the gridsearch was layed out for precision scoring)\n",
    "plot_confusion_matrix(gs, X_train, y_train, cmap='Blues', labels=[1, 0], values_format='.0f')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(gs, X_test, y_test, cmap='Blues', labels=[1, 0], values_format='.0f')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_at_threshold(x, threshold=0.5):\n",
    "    if x >= threshold:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_train = gs.predict(X_train)\n",
    "predictions_test = gs.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase the threshold for the probability of class 1 to have a higher precision on prediction of class 0\n",
    "Y_prediction = pd.DataFrame(gs.predict_proba(X_train), columns=['high_salary', 'low_salary'])\n",
    "Y_prediction['predict_with_thres'] = Y_prediction['high_salary'].apply(predict_at_threshold, threshold=0.6)\n",
    "Y_prediction['true_value'] = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_prediction.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training accuracy:', accuracy_score(y_train, predictions_train))\n",
    "print('Testing accuracy:', accuracy_score(y_test, predictions_test))\n",
    "\n",
    "print('Training precision:', precision_score(y_train, predictions_train))\n",
    "print('Testing precision:', precision_score(y_test, predictions_test))\n",
    "\n",
    "print('Training recall:', recall_score(y_train, predictions_train))\n",
    "print('Testing recall:', recall_score(y_test, predictions_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score, plot_roc_curve, plot_precision_recall_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the ROC curve you want to gain a really strong slope in the beginning with as less false positive rate as possible. The best score would be having a area under the curve of 1 to have a perfect prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "plot_roc_curve(gs.best_estimator_, X_train, y_train, ax=ax)\n",
    "ax.plot([0, 1], [0, 1], 'k--', linewidth=4)  # baseline\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The precision shows the ratio of how often the positive labelled class was predicted correclty over \n",
    "all predictions which were done for that class. Whereas the recall depicts the ratio of that class over it occurence\n",
    "in the dataset. That means that you can predict less, but more precise to have a high precision score, but then chance\n",
    "increases that the class occurence more often then you actually predict it and then the recall decreases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.fig, ax = plt.subplots(figsize=(6, 6))\n",
    "plot_precision_recall_curve(gs, X_train, y_train, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "4fb29de2-5b98-474c-a4ad-5170b72b9aea"
   },
   "source": [
    "<img src=\"http://imgur.com/xDpSobf.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "### Bonus:\n",
    "\n",
    "- Answer the salary discussion by using your model to explain the tradeoffs between detecting high vs low salary positions. \n",
    "- Discuss the differences and explain when you want a high-recall or a high-precision model in this scenario.\n",
    "- Obtain the ROC/precision-recall curves for the different models you studied (at least the tuned model of each category) and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "focus": false,
    "id": "068dc1cf-7fd7-4f27-a1f1-7f0a5a221d29"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize your results in an executive summary written for a non-technical audience.\n",
    "   \n",
    "- Writeups should be at least 500-1000 words, defining any technical terms, explaining your approach, as well as any risks and limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR TEXT HERE IN MARKDOWN FORMAT "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/xDpSobf.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "### BONUS\n",
    "\n",
    "Convert your executive summary into a public blog post of at least 500 words, in which you document your approach in a tutorial for other aspiring data scientists. Link to this in your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR LINK HERE IN MARKDOWN FORMAT "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
